{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Book to work with data scraped from ESPN for college baserball 2016-23 Seasons\n",
    "\n",
    "# Dependent Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pyproj\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import transform\n",
    "from shapely.affinity import rotate\n",
    "\n",
    "\n",
    "from geopy.distance import great_circle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "## 2016-22 game data - Games with HR Data - 2016-2022\n",
    "\n",
    "file = 'TEMP/collegebaseball_scrape_2016-2022_v1.csv'\n",
    "\n",
    "df = pd.read_csv(file)\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Value Counts\n",
    "\n",
    "# # locations\n",
    "# print(df['location'].value_counts())\n",
    "\n",
    "# # teams\n",
    "# print(df['team_1'].value_counts())\n",
    "\n",
    "# print(df['team_2'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2023 Data - from full season\n",
    "\n",
    "file = 'TEMP/NEW_HR_Scrape_with full_team_names.csv'\n",
    "\n",
    "df_2023 = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charles Schwab Field                 78\n",
      "Baum-Walker Stadium                  67\n",
      "Bryson Field at Boshamer Stadium     60\n",
      "Hawkins Field                        57\n",
      "UFCU Disch-Falk Field                57\n",
      "                                     ..\n",
      "Kauffman Stadium                      1\n",
      "FedExPark Avron Fogelman Field        1\n",
      "Cleveland S. Harley Baseball Park     1\n",
      "Dickey-Stephens Park                  1\n",
      "Tomlinson Stadiumâ€“Kell Field          1\n",
      "Name: location, Length: 102, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2023    1190\n",
       "2022     142\n",
       "2021     139\n",
       "2018     132\n",
       "2019     131\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2023.head()\n",
    "# df_2023.info()\n",
    "# add year column\n",
    "# df_2023['year'] = 2023\n",
    "\n",
    "# # locations\n",
    "print(df_2023['location'].value_counts())\n",
    "\n",
    "# # teams\n",
    "# print(df_2023['team_1'].value_counts())\n",
    "\n",
    "# Look at data format\n",
    "df_2023.head()\n",
    "\n",
    "## year value counts\n",
    "df_2023['year'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a list with all of the team abbreviations used in the team_1 and team_2 columns\n",
    "teams = list(df_2023['team_1'].unique())\n",
    "teams.extend(list(df_2023['team_2'].unique()))\n",
    "teams = list(set(teams))\n",
    "\n",
    "## How many teams are there?\n",
    "len(teams)\n",
    "\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a list of all of the locations in the location column\n",
    "locations = list(df_2023['location'].unique())\n",
    "len(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate simple stats for just 2023\n",
    "\n",
    "# add together runs, hits and erros to get total events\n",
    "df_2023['runs_total'] = df_2023['runs_1'] + df_2023['runs_2']\n",
    "df_2023['hits_total'] = df_2023['hits_1'] + df_2023['hits_2']\n",
    "df_2023['errors_total'] = df_2023['errors_1'] + df_2023['errors_2']\n",
    "\n",
    "\n",
    "## avg runs, hits, errors, HRs,per game for each team, grouped by location\n",
    "df_2023_stats = df_2023.groupby(['location']).mean().reset_index()\n",
    "\n",
    "# Add a column for total games at each location\n",
    "df_2023_stats['total_games'] = df_2023.groupby(['location']).count().reset_index()['team_1']\n",
    "\n",
    "df_2023_stats.head()\n",
    "\n",
    "# rank by HR per game\n",
    "df_2023_stats.sort_values(by=['home_runs'], ascending=False, inplace=True)\n",
    "\n",
    "# Filter to only locations with 20 or more games\n",
    "# df_2023_stats = df_2023_stats[df_2023_stats['total_games'] >= 20]\n",
    "\n",
    "\n",
    "\n",
    "df_2023_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show distrobution of games played at each location\n",
    "# sns.histplot(data=df_2023_stats, x='total_games', bins=10)\n",
    "\n",
    "# filter to only locations with 20 or more games\n",
    "df_2023_stats = df_2023_stats[df_2023_stats['total_games'] >= 25]\n",
    "\n",
    "## Distribution of HRs per game at each location\n",
    "sns.histplot(data=df_2023_stats, x='home_runs', bins=10)\n",
    "# add title and labels\n",
    "plt.title('2023 - Distribution of HRs per game')\n",
    "\n",
    "# add note about total games in data set and only locations with > 20 games\n",
    "note = 'Total Games:  ' + str(df_2023_stats['total_games'].sum()) + '\\n' + '* Locations with > 20 games'\n",
    "plt.gca().annotate(note, xy=(1, 1), xytext=(-15, -15), fontsize=10,\n",
    "                   xycoords='axes fraction', textcoords='offset points',\n",
    "                   bbox=dict(facecolor='white', alpha=0.8),\n",
    "                   horizontalalignment='right', verticalalignment='top')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Runs and hits per game histograms\n",
    "\n",
    "# Runs\n",
    "sns.histplot(data=df_2023_stats, x='runs_total', bins=10)\n",
    "# add title and labels\n",
    "plt.title('2023 - Distribution of Runs per game')\n",
    "note = 'Total Games:  ' + str(df_2023_stats['total_games'].sum()) + '\\n' + '* Locations with > 20 games'\n",
    "plt.gca().annotate(note, xy=(1, 1), xytext=(-15, -15), fontsize=10,\n",
    "                   xycoords='axes fraction', textcoords='offset points',\n",
    "                   bbox=dict(facecolor='white', alpha=0.8),\n",
    "                   horizontalalignment='right', verticalalignment='top')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New plot for hits\n",
    "sns.histplot(data=df_2023_stats, x='hits_total', bins=10)\n",
    "# add title and labels\n",
    "plt.title('2023 - Distribution of Hits per game')\n",
    "note = 'Total Games:  ' + str(df_2023_stats['total_games'].sum()) + '\\n' + '* Locations with > 20 games'\n",
    "plt.gca().annotate(note, xy=(1, 1), xytext=(-15, -15), fontsize=10,\n",
    "                   xycoords='axes fraction', textcoords='offset points',\n",
    "                   bbox=dict(facecolor='white', alpha=0.8),\n",
    "                   horizontalalignment='right', verticalalignment='top')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get same summary and plots for 2016-22 data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## location value counts\n",
    "# print(df['location'].value_counts())\n",
    "\n",
    "# get total runs, hits and errors\n",
    "df['runs_total'] = df['runs_1'] + df['runs_2']\n",
    "df['hits_total'] = df['hits_1'] + df['hits_2']\n",
    "df['errors_total'] = df['errors_1'] + df['errors_2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a list with all of the team abbreviations used in the team_1 and team_2 columns\n",
    "teams = list(df['team_1'].unique())\n",
    "teams.extend(list(df['team_2'].unique()))\n",
    "teams = list(set(teams))\n",
    "\n",
    "## How many teams are there?\n",
    "len(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### groupby location\n",
    "df_stats = df.groupby(['location']).mean().reset_index()\n",
    "## add a games played column\n",
    "df_stats['total_games'] = df.groupby(['location']).count().reset_index()['team_1']\n",
    "\n",
    "df_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get a list of all of the locations and add them to the existing list\n",
    "locations_2 = list(df['location'].unique())\n",
    "locations.extend(locations_2)\n",
    "\n",
    "locations = list(set(locations))\n",
    "len(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## histogram HRs\n",
    "sns.histplot(data=df_stats, x='home_runs', bins=10)\n",
    "# add title and labels\n",
    "plt.title('2016-2022 - Distribution of HRs per game')\n",
    "plt.gca().annotate(note, xy=(1, 1), xytext=(-15, -15), fontsize=10,\n",
    "                   xycoords='axes fraction', textcoords='offset points',\n",
    "                   bbox=dict(facecolor='white', alpha=0.8),\n",
    "                   horizontalalignment='right', verticalalignment='top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Total Runs\n",
    "sns.histplot(data=df_stats, x='runs_total', bins=10)\n",
    "# add title and labels\n",
    "plt.title('2016-2022 - Distribution of Runs per game')\n",
    "note = 'Total Games:  ' + str(df_stats['total_games'].sum()) + '\\n NCAA Tourney Games'\n",
    "plt.gca().annotate(note, xy=(1, 1), xytext=(-15, -15), fontsize=10,\n",
    "                   xycoords='axes fraction', textcoords='offset points',\n",
    "                   bbox=dict(facecolor='white', alpha=0.8),\n",
    "                   horizontalalignment='right', verticalalignment='top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hits\n",
    "sns.histplot(data=df_stats, x='hits_total', bins=10)\n",
    "# add title and labels\n",
    "plt.title('2016-2022 - Distribution of Hits per game')\n",
    "note = 'Total Games:  ' + str(df_stats['total_games'].sum()) + '\\n NCAA Tourney Games'\n",
    "plt.gca().annotate(note, xy=(1, 1), xytext=(-15, -15), fontsize=10,\n",
    "                   xycoords='axes fraction', textcoords='offset points',\n",
    "                   bbox=dict(facecolor='white', alpha=0.8),\n",
    "                   horizontalalignment='right', verticalalignment='top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple Stats dor 2016-2022 \n",
    "## These are the games that didn't ahve box scores so I just ahve team abbrev and team runs hits errors\n",
    "\n",
    "file = 'TEMP/NCAA_Simple/ncaa_Baseball_simple_2016-2022.csv'\n",
    "\n",
    "# load data\n",
    "df_simple = pd.read_csv(file)\n",
    "\n",
    "df_simple.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many games have game IDs? These were already charted in the previous dataset\n",
    "df_simple['game_id'].notna().sum()\n",
    "\n",
    "## Drop the rows with game ids\n",
    "df_simple = df_simple[df_simple['game_id'].isna()]\n",
    "\n",
    "df_simple.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calc total hits, runs and errors\n",
    "df_simple['runs_total'] = df_simple['away_team_runs'] + df_simple['home_team_runs']\n",
    "df_simple['hits_total'] = df_simple['away_team_hits'] + df_simple['home_team_hits']\n",
    "df_simple['errors_total'] = df_simple['away_team_errors'] + df_simple['home_team_errors']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make histograms of total runs, hits and errors per game\n",
    "\n",
    "## Total Runs\n",
    "sns.histplot(data=df_simple, x='runs_total', bins=25)\n",
    "# add title and labels\n",
    "plt.title('2016-2022 - Distribution of Total Runs per game')\n",
    "note = 'Total Games:  ' + str(df_simple.shape[0])\n",
    "plt.gca().annotate(note, xy=(1, 1), xytext=(-15, -15), fontsize=10,\n",
    "                     xycoords='axes fraction', textcoords='offset points',\n",
    "                        bbox=dict(facecolor='white', alpha=0.8),\n",
    "                        horizontalalignment='right', verticalalignment='top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## home Team Runs\n",
    "sns.histplot(data=df_simple, x='home_team_runs', bins=25)\n",
    "# add title and labels\n",
    "plt.title('2016-2022 - Distribution of Home Team Runs per game')\n",
    "note = 'Total Games:  ' + str(df_simple.shape[0])\n",
    "plt.gca().annotate(note, xy=(1, 1), xytext=(-15, -15), fontsize=10,\n",
    "                        xycoords='axes fraction', textcoords='offset points',\n",
    "                        bbox=dict(facecolor='white', alpha=0.8),\n",
    "                        horizontalalignment='right', verticalalignment='top')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Away Team Runs\n",
    "\n",
    "sns.histplot(data=df_simple, x='away_team_runs', bins=25)\n",
    "# add title and labels\n",
    "plt.title('2016-2022 - Distribution of Away Team Runs per game')\n",
    "note = 'Total Games:  ' + str(df_simple.shape[0])\n",
    "plt.gca().annotate(note, xy=(1, 1), xytext=(-15, -15), fontsize=10,\n",
    "                        xycoords='axes fraction', textcoords='offset points',\n",
    "                        bbox=dict(facecolor='white', alpha=0.8),\n",
    "                        horizontalalignment='right', verticalalignment='top')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Total Hits Per Game\n",
    "sns.histplot(data=df_simple, x='hits_total', bins=25)\n",
    "# add title and labels\n",
    "plt.title('2016-2022 - Distribution of Total Hits per game')\n",
    "note = 'Total Games:  ' + str(df_simple.shape[0])\n",
    "plt.gca().annotate(note, xy=(1, 1), xytext=(-15, -15), fontsize=10,\n",
    "                        xycoords='axes fraction', textcoords='offset points',\n",
    "                        bbox=dict(facecolor='white', alpha=0.8),\n",
    "                        horizontalalignment='right', verticalalignment='top')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Error Per Game\n",
    "\n",
    "sns.histplot(data=df_simple, x='errors_total', bins=12)\n",
    "# add title and labels\n",
    "plt.title('2016-2022 - Distribution of Total Errors per game')\n",
    "note = 'Total Games:  ' + str(df_simple.shape[0])\n",
    "plt.gca().annotate(note, xy=(1, 1), xytext=(-15, -15), fontsize=10,\n",
    "                        xycoords='axes fraction', textcoords='offset points',\n",
    "                        bbox=dict(facecolor='white', alpha=0.8),\n",
    "                        horizontalalignment='right', verticalalignment='top')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(locations))\n",
    "\n",
    "print(locations[0:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get a list of all of the locations in the HR dataset and use google api to get a lat and long for each location\n",
    "\n",
    "import googlemaps\n",
    "\n",
    "# Your API Key goes here\n",
    "gmaps = googlemaps.Client(key='AIzaSyA_BhlTupRdBPBhRptQuR6pYorMVYQnRMA')\n",
    "\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through all locations\n",
    "for location in locations:\n",
    "    # Geocode location\n",
    "    geocode_result = gmaps.geocode(location)\n",
    "    # If a result was returned, append the result as a dictionary to the results list\n",
    "    if geocode_result:\n",
    "        latitude = geocode_result[0]['geometry']['location']['lat']\n",
    "        longitude = geocode_result[0]['geometry']['location']['lng']\n",
    "        results.append({'location': location, 'latitude': latitude, 'longitude': longitude})\n",
    "    else:\n",
    "        print(f\"Could not find coordinates for {location}.\")\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df_locations = pd.DataFrame(results)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_locations)\n",
    "\n",
    "## Save as a csv as backup\n",
    "df_locations.to_csv('TEMP/NCAA_locations_lat_lng.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concat the data with home run stats into a single dataframe\n",
    "df_hr = pd.concat([df, df_2023], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hr.info(){\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"clouds\": 75,\n",
    "            \"dew_point\": 73.29,\n",
    "            \"dt\": 1527969600,\n",
    "            \"feels_like\": 95.31,\n",
    "            \"humidity\": 63,\n",
    "            \"pressure\": 1013,\n",
    "            \"sunrise\": 1527937264,\n",
    "            \"sunset\": 1527989325,\n",
    "            \"temp\": 87.44,\n",
    "            \"visibility\": 10000,\n",
    "            \"weather\": [\n",
    "                {\n",
    "                    \"description\": \"haze\",\n",
    "                    \"icon\": \"50d\",\n",
    "                    \"id\": 721,\n",
    "                    \"main\": \"Haze\"\n",
    "                }\n",
    "            ],\n",
    "            \"wind_deg\": 30,\n",
    "            \"wind_speed\": 9.22\n",
    "        }\n",
    "    ],\n",
    "    \"lat\": 36.0499,\n",
    "    \"lon\": -94.1822,\n",
    "    \"timezone\": \"America/Chicago\",\n",
    "    \"timezone_offset\": -18000\n",
    "}\n",
    "\n",
    "df_locations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge the lat and long into the HR dataset\n",
    "df_hr = pd.merge(df_hr, df_locations, how='left', left_on='location', right_on='location')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_2023.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hr.info()\n",
    "\n",
    "df_hr.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get a list of all of the team abbreviations\n",
    "df_hr['team_1'].unique()\n",
    "df_hr['team_2'].unique()\n",
    "\n",
    "## Add all of the team abbreviations to a list\n",
    "teams = df_hr['team_1'].unique().tolist()\n",
    "teams.extend(df_hr['team_2'].unique().tolist())\n",
    "\n",
    "len(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load NCAA School Data\n",
    "df_schools = pd.read_csv('TEMP/NCAA_School_Dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schools.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try to match the df_schools['school_abrv'] to the teams list\n",
    "df_schools['school_abrv'].isin(teams).sum()\n",
    "\n",
    "# Create new DF of only the schools that are in the teams list\n",
    "df_schools_matched = df_schools[df_schools['school_abrv'].isin(teams)]\n",
    "\n",
    "## DF of schoolt that are not on the teams list\n",
    "df_schools_not_matched = df_schools[~df_schools['school_abrv'].isin(teams)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the other abriviation list\n",
    "df_adbri_2 = pd.read_csv('TEMP/team_abr_dictionary_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adbri_2.info()\n",
    "\n",
    "## Get list of unmatched teams from df_schools_not_matched\n",
    "unmatched_teams = df_schools_not_matched['school_abrv'].unique().tolist()\n",
    "\n",
    "## Match them agaisnt the df_adbri_2['Abbreviation]\n",
    "\n",
    "df_adbri_2['Abbreviation'].isin(unmatched_teams).sum()\n",
    "\n",
    "## Extract those new matches to another DF and add it to the df_schools_matched\n",
    "df_adbri_2_matched = df_adbri_2[df_adbri_2['Abbreviation'].isin(unmatched_teams)]\n",
    "\n",
    "## Change column names to match the df_schools_matched\n",
    "df_adbri_2_matched.rename(columns={'Abbreviation': 'school_abrv', 'Team': 'school_name'}, inplace=True)\n",
    "\n",
    "df_adbri_2_matched.info()\n",
    "\n",
    "df_schools_matched = pd.concat([df_schools_matched, df_adbri_2_matched], ignore_index=True)\n",
    "\n",
    "df_schools_matched.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hr.info()\n",
    "\n",
    "## groupby location and get the mean of the lat and long\n",
    "df_hr_locations = df_hr.groupby('location').mean()\n",
    "\n",
    "df_hr_locations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the school abriviation is in the df_schools_matched['school_abrv'] then replace it with the df_schools_matched['school_name']\n",
    "df_hr['team_1'] = df_hr['team_1'].apply(lambda x: df_schools_matched[df_schools_matched['school_abrv'] == x]['school_name'].values[0] if x in df_schools_matched['school_abrv'].tolist() else x)\n",
    "df_hr['team_2'] = df_hr['team_2'].apply(lambda x: df_schools_matched[df_schools_matched['school_abrv'] == x]['school_name'].values[0] if x in df_schools_matched['school_abrv'].tolist() else x)\n",
    "\n",
    "df_hr.sample(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Value count year\n",
    "df_hr['year'].value_counts()\n",
    "\n",
    "## Make sure 'year' is an integer and not a float\n",
    "# df_hr['year'] = df_hr['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Groupby location "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't need to use the airport method or weather underground. \n",
    "\n",
    "Found an easier way to do it with open weather api\n",
    "- see that code in the new book by that name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the closest airport to each field with a lat/lng coordinate\n",
    "## Can use the airport code to look up historical weather info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopy.distance\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Load the \"OurAirports\" data into a DataFrame\n",
    "airports_df = pd.read_csv('http://ourairports.com/data/airports.csv')\n",
    "\n",
    "# Filter the airports DataFrame to include only US airports\n",
    "airports_df = airports_df[(airports_df['iso_country'] == 'US') & (airports_df['scheduled_service'] == 'yes')]\n",
    "\n",
    "# Create a KDTree from the airport coordinates\n",
    "airport_tree = cKDTree(airports_df[['latitude_deg', 'longitude_deg']].values)\n",
    "\n",
    "# Function to calculate the distance between two points given their (latitude, longitude) coordinates\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    coords_1 = (lat1, lon1)\n",
    "    coords_2 = (lat2, lon2)\n",
    "    return geopy.distance.distance(coords_1, coords_2).km\n",
    "\n",
    "def find_nearest_airport(lat, lon):\n",
    "    if pd.isnull(lat) or pd.isnull(lon):\n",
    "        return '', 0.0  # Return empty values for both airport and distance\n",
    "\n",
    "    # Query the KDTree to find the nearest airport\n",
    "    _, nearest_index = airport_tree.query((lat, lon))\n",
    "\n",
    "    nearest_airport = airports_df.iloc[nearest_index]['iata_code']\n",
    "    airport_distance = calculate_distance(lat, lon, airports_df.iloc[nearest_index]['latitude_deg'], airports_df.iloc[nearest_index]['longitude_deg'])\n",
    "    \n",
    "    return nearest_airport if pd.notnull(nearest_airport) else '', airport_distance\n",
    "\n",
    "# Create new columns for the nearest airport and distance\n",
    "df_hr_locations['Nearest_Airport'], df_hr_locations['Distance_to_Airport'] = zip(*df_hr_locations.apply(\n",
    "    lambda row: find_nearest_airport(row['latitude'], row['longitude']),\n",
    "    axis=1\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hr_locations.info()\n",
    "\n",
    "df_hr_locations.head()\n",
    "\n",
    "# histogram of distance to nearest airport\n",
    "# sns.histplot(data=df_hr_locations, x='distance_to_nearest_airport', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram of distances to the nearest airport with units\n",
    "sns.histplot(data=df_hr_locations, x='Distance_to_Airport', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code is not very efficent because it calculates distance from every field to every airport\n",
    "## If it takes too long to run wiht a few hundred fields, then we will need to find a better way to do this\n",
    "## like using a KDTree for spatial indexing\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import geopy.distance\n",
    "\n",
    "# Load the \"OurAirports\" data into a DataFrame\n",
    "airports_df = pd.read_csv('http://ourairports.com/data/airports.csv')\n",
    "\n",
    "# Filter the airports DataFrame to include only US airports\n",
    "airports_df = airports_df[airports_df['iso_country'] == 'US']\n",
    "airports_df = airports_df[airports_df['scheduled_service'] == 'yes']\n",
    "\n",
    "# Function to calculate the distance between two points given their (latitude, longitude) coordinates\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    # Check if any of the coordinates are NaN or non-finite\n",
    "    if pd.isnull(lat1) or pd.isnull(lon1) or pd.isnull(lat2) or pd.isnull(lon2) or not np.isfinite(lat1) or not np.isfinite(lon1) or not np.isfinite(lat2) or not np.isfinite(lon2):\n",
    "        return 0.0  # Return a default distance value when coordinates are invalid\n",
    "    else:\n",
    "        coords_1 = (lat1, lon1)\n",
    "        coords_2 = (lat2, lon2)\n",
    "        return geopy.distance.distance(coords_1, coords_2).km\n",
    "\n",
    "\n",
    "\n",
    "def find_nearest_airport(lat, lon):\n",
    "    if pd.isnull(lat) or pd.isnull(lon):\n",
    "        return ''\n",
    "\n",
    "    distances = airports_df.apply(\n",
    "        lambda row: calculate_distance(lat, lon, row['latitude_deg'], row['longitude_deg']),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    valid_distances = distances.dropna()  # Drop rows with NaN distances\n",
    "    \n",
    "    if valid_distances.empty:\n",
    "        return ''\n",
    "    \n",
    "    nearest_index = valid_distances.idxmin()\n",
    "    nearest_airport = airports_df.loc[nearest_index, 'iata_code']\n",
    "    \n",
    "    airport_distance = valid_distances.min()\n",
    "    \n",
    "    return nearest_airport if pd.notnull(nearest_airport) else ''\n",
    "    return airport_distance if pd.notnull(airport_distance) else 0.0\n",
    "    \n",
    "\n",
    "df_hr_locations['Nearest_Airport'] = df_hr_locations.progress_apply(\n",
    "    lambda row: find_nearest_airport(row['latitude'], row['longitude']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# # Calculate and store the distance in km from the field to the nearest airport\n",
    "# df_hr_locations['Distance_to_Airport'] = df_hr_locations.progress_apply(\n",
    "#     lambda row: calculate_distance(\n",
    "#         row['latitude'],\n",
    "#         row['longitude'],\n",
    "#         airports_df.loc[row['Nearest_Airport'], 'latitude_deg'] if row['Nearest_Airport'] else None,\n",
    "#         airports_df.loc[row['Nearest_Airport'], 'longitude_deg'] if row['Nearest_Airport'] else None\n",
    "#     ),\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(len(airports_df))\n",
    "\n",
    "# # Apply the function to each row in your DataFrame\n",
    "# from tqdm import tqdm\n",
    "# tqdm.pandas()\n",
    "\n",
    "# # Replace 'df_hr_locations' with the appropriate DataFrame name that contains 'latitude' and 'longitude' columns\n",
    "\n",
    "# df_hr_locations['Nearest_Airport'] = df_hr_locations.progress_apply(lambda row: find_nearest_airport(row['latitude'], row['longitude']), axis=1)\n",
    "\n",
    "# # Calculate and store the distance in km from the field to the nearest airport\n",
    "# df_hr_locations['Distance_to_Airport'] = df_hr_locations.progress_apply(lambda row: calculate_distance(row['latitude'], row['longitude'], airports_df.loc[row['Nearest_Airport'], 'latitude_deg'], airports_df.loc[row['Nearest_Airport'], 'longitude_deg']), axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hr_locations.sample(20)\n",
    "\n",
    "df_hr.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eyrjh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find the Closest field in the College KML file to each field\n",
    "## This will be used to find the closest field to each field in the HR dataset\n",
    "\n",
    "## Load the College KML file\n",
    "file_path = 'data/kml/College - All.kml'\n",
    "\n",
    "## 2A. SET UP NESSISARY DICTIONARIES\n",
    "# Define a dictionary that maps level indicators to levels and size factors\n",
    "level_dict = {\n",
    "    'International': 'international',\n",
    "    'Major Leagues': 'mlb', \n",
    "    'Professional': 'pro', \n",
    "    'College': 'college', \n",
    "    'High School': 'high_school',\n",
    "    'Youth': 'youth',\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the KML File to evaluate\n",
    "with open(file_path) as file:\n",
    "\n",
    "    xml_data = file.read()\n",
    "\n",
    "# Initialize soup variables for parsing file\n",
    "soup = BeautifulSoup(xml_data, 'xml')\n",
    "folders = soup.Document.Folder\n",
    "list = soup.Document.Folder.find_all('Folder')\n",
    "\n",
    "## Create a dataframe to hold the data parsed from xml\n",
    "df = pd.DataFrame(columns=['field', 'foul', 'fop'])\n",
    "\n",
    "failed = []\n",
    "\n",
    "## 3. PARSE THE KML FILE\n",
    "# Create an empty list to store the rows to append to the DataFrame\n",
    "rows = []\n",
    "\n",
    "# Loop through the folders and extract the data\n",
    "for folder in list:\n",
    "    try:\n",
    "        field_name = folder.find('name').text\n",
    "        foul = folder.find_all('coordinates')[0].text\n",
    "        fop = folder.find_all('coordinates')[1].text\n",
    "\n",
    "        row = {\n",
    "            'field': field_name,\n",
    "            'foul': foul,\n",
    "            'fop': fop\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Add name of folder to a list of failed folders\n",
    "        failed.append(folder.find('name').text)\n",
    "        print(f\"Error processing folder: {folder.find('name').text}. Error message: {str(e)}\")\n",
    "\n",
    "# Convert the list of rows to a DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Print a list of failed folders\n",
    "print(f\"Failed to process {len(failed)} folders: {', '.join(failed)}\")\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Remove new line and space characters from coordinates\n",
    "df_cleaned = df_cleaned.replace(r'\\n','', regex=True) \n",
    "df_cleaned = df_cleaned.replace(r'\\t','', regex=True) \n",
    "\n",
    "# Drop any duplicate rows\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['field'], keep='first')\n",
    "\n",
    "# Drop any rows with empty fields\n",
    "df_cleaned = df_cleaned[(df_cleaned != 0).all(1)]\n",
    "\n",
    "# Define the regex patterns for each level\n",
    "re_mlb = re.compile(r'mlb', re.IGNORECASE)\n",
    "re_pro = re.compile(r'pro|semi[-\\s]*pro', re.IGNORECASE)\n",
    "re_college = re.compile(r'college', re.IGNORECASE)\n",
    "re_high_school = re.compile(r'high school|hs', re.IGNORECASE)  # Include the abbreviation 'hs'\n",
    "re_youth = re.compile(r'youth', re.IGNORECASE)\n",
    "re_muni = re.compile(r'muni', re.IGNORECASE)\n",
    "re_international = re.compile(r'international', re.IGNORECASE)\n",
    "\n",
    "# Define a function to classify the fields based on the regex patterns\n",
    "def classify_field(field_name):\n",
    "    if re_mlb.search(field_name):\n",
    "        return 'Major League'\n",
    "    elif re_pro.search(field_name):\n",
    "        return 'Professional'\n",
    "    elif re_college.search(field_name):\n",
    "        return 'College'\n",
    "    elif re_high_school.search(field_name):\n",
    "        return 'High School'\n",
    "    elif re_youth.search(field_name):\n",
    "        return 'Youth'\n",
    "    elif re_muni.search(field_name):\n",
    "        return 'State / County / Municipal'\n",
    "    elif re_international.search(field_name):\n",
    "        return 'International'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Apply the classify_field function to the 'field' column\n",
    "df_cleaned['level'] = df_cleaned['field'].apply(classify_field)\n",
    "\n",
    "# Clean up the 'field' column by removing the level indicator and any trailing '-' characters\n",
    "level_regex = r'\\s*(%s)\\s*' % '|'.join(re.escape(level) for level in level_dict.values())\n",
    "df_cleaned['field'] = df_cleaned['field'].str.replace(level_regex, '', regex=True, flags=re.IGNORECASE)\n",
    "df_cleaned['field'] = df_cleaned['field'].str.replace(r'-\\s*$', '', regex=True)\n",
    "\n",
    "# Rename field column to park_name to avoid confusion down the line\n",
    "df_cleaned = df_cleaned.rename(columns={'field': 'park_name'})\n",
    "\n",
    "##### Clean up polygon data and create a new home_plate column\n",
    "def parse_coordinates(coord_string):\n",
    "    coords = coord_string.split()\n",
    "    parsed_coords = [tuple(map(float, coord.split(',')[:2])) for coord in coords]\n",
    "    return parsed_coords\n",
    "\n",
    "# Create a new column for the home_plate location using the first set of coordinates in the 'fop' column\n",
    "df_cleaned['home_plate'] = df_cleaned['fop'].apply(lambda x: parse_coordinates(x)[0])\n",
    "# Apply the parse_coordinates function to the 'foul' and 'fop' columns\n",
    "df_cleaned['foul'] = df_cleaned['foul'].apply(parse_coordinates)\n",
    "df_cleaned['fop'] = df_cleaned['fop'].apply(parse_coordinates)\n",
    "\n",
    "# 4. PROFORM GEOGRAPHIC CALCULATIONS - DISTANCE, AREA, ETC.\n",
    "def calculate_area(coords):\n",
    "    # Create a Polygon object from the coordinates\n",
    "    polygon = Polygon(coords)\n",
    "\n",
    "    # Calculate the centroid of the polygon\n",
    "    centroid = polygon.centroid\n",
    "\n",
    "    # Create a custom LAEA projection centered on the centroid\n",
    "    custom_projection = f\"+proj=laea +lat_0={centroid.y} +lon_0={centroid.x} +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"\n",
    "\n",
    "    # Create a transformer for converting coordinates to the custom LAEA projection\n",
    "    transformer = pyproj.Transformer.from_crs(\n",
    "        pyproj.CRS(\"EPSG:4326\"),  # WGS 84 (latitude and longitude)\n",
    "        pyproj.CRS(custom_projection),  # Custom LAEA projection\n",
    "        always_xy=True\n",
    "    )\n",
    "\n",
    "    # Define a function to transform coordinates using the transformer\n",
    "    def transform_coordinates(x, y):\n",
    "        return transformer.transform(x, y)\n",
    "\n",
    "    # Convert the coordinates to the custom LAEA projection\n",
    "    polygon_laea = transform(transform_coordinates, polygon)\n",
    "\n",
    "    # Calculate the area in square meters\n",
    "    area_sqm = polygon_laea.area\n",
    "\n",
    "    # Convert the area to square feet (1 square meter = 10.764 square feet)\n",
    "    area_sqft = area_sqm * 10.764\n",
    "\n",
    "    return area_sqft\n",
    "\n",
    "\n",
    "\n",
    "### Call Function and add to dataframe\n",
    "df_cleaned['foul_area_sqft'] = df_cleaned['foul'].apply(calculate_area)\n",
    "df_cleaned['fop_area_sqft'] = df_cleaned['fop'].apply(calculate_area)\n",
    "\n",
    "## Calculate the total area of the field and the ratio of foul area to field area\n",
    "df_cleaned['field_area_sqft'] = df_cleaned['foul_area_sqft'] + df_cleaned['fop_area_sqft']\n",
    "## Percentage foul area\n",
    "df_cleaned['foul_area_per'] = df_cleaned['foul_area_sqft'] / df_cleaned['field_area_sqft']\n",
    "## Fair to Foul Ratio\n",
    "df_cleaned['fair_to_foul'] = df_cleaned['fop_area_sqft'] / df_cleaned['foul_area_sqft']\n",
    "\n",
    "# 4B. Calculate the distance from home plate to the outfield fences\n",
    "def interpolate_points(start, end, length_ratio):\n",
    "    start_np = np.array(start)\n",
    "    end_np = np.array(end)\n",
    "    return tuple(start_np + (end_np - start_np) * length_ratio)\n",
    "\n",
    "def calculate_distances(home_plate, outfield_coords, num_points=540):\n",
    "    def is_same_point(point1, point2, tolerance=1e-6):\n",
    "        return abs(point1[0] - point2[0]) < tolerance and abs(point1[1] - point2[1]) < tolerance\n",
    "\n",
    "    home_plate_lat_lon = (home_plate[1], home_plate[0])\n",
    "    distances = []\n",
    "\n",
    "    # Calculate total line length\n",
    "    total_length = 0\n",
    "    segments = []\n",
    "    for i in range(len(outfield_coords) - 1):\n",
    "        start = outfield_coords[i]\n",
    "        end = outfield_coords[i + 1]\n",
    "        if not is_same_point(home_plate, start) and not is_same_point(home_plate, end):\n",
    "            segment_length = great_circle((start[1], start[0]), (end[1], end[0])).feet\n",
    "            segments.append((start, end, segment_length))\n",
    "            total_length += segment_length\n",
    "\n",
    "    # Calculate the distance between equally spaced points\n",
    "    spacing = total_length / (num_points - 1)\n",
    "\n",
    "    # Interpolate points and calculate distances\n",
    "    current_length = 0\n",
    "    segment_index = 0\n",
    "    for i in range(num_points):\n",
    "        while segment_index < len(segments) - 1 and current_length > segments[segment_index][2]:\n",
    "            current_length -= segments[segment_index][2]\n",
    "            segment_index += 1\n",
    "\n",
    "        start, end, segment_length = segments[segment_index]\n",
    "        length_ratio = current_length / segment_length\n",
    "        point = interpolate_points(start, end, length_ratio)\n",
    "        distance = great_circle(home_plate_lat_lon, (point[1], point[0])).feet\n",
    "        distances.append(distance)\n",
    "\n",
    "        current_length += spacing\n",
    "\n",
    "    return distances\n",
    "\n",
    "# Calculate distances for each row\n",
    "df_cleaned['distances'] = df_cleaned.apply(lambda row: calculate_distances(row['home_plate'], row['fop']), axis=1)\n",
    "\n",
    "# Calculate max, min, and average distances for each row\n",
    "df_cleaned['max_distance'] = df_cleaned['distances'].apply(max)\n",
    "df_cleaned['min_distance'] = df_cleaned['distances'].apply(min)\n",
    "df_cleaned['avg_distance'] = df_cleaned['distances'].apply(lambda distances: sum(distances) / len(distances))\n",
    "# get the median distance\n",
    "df_cleaned['median_distance'] = df_cleaned['distances'].apply(lambda distances: np.median(distances))\n",
    "\n",
    "## Return the dataframe as df\n",
    "df = df_cleaned\n",
    "\n",
    "## Reverse the order of the tuples within the coordinate columns (foul, fop, home_plate)\n",
    "def reverse_tuples(coords):\n",
    "    return [(coord[1], coord[0]) for coord in coords]\n",
    "\n",
    "df['foul'] = df['foul'].apply(reverse_tuples)\n",
    "df['fop'] = df['fop'].apply(reverse_tuples)\n",
    "\n",
    "# Reverse the home plate coordinates single tuple\n",
    "df['home_plate'] = df['home_plate'].apply(lambda coord: (coord[1], coord[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)\n",
    "\n",
    "# create lat and long columns from the home_plate column\n",
    "df['lat'] = df['home_plate'].apply(lambda coord: coord[0])\n",
    "df['long'] = df['home_plate'].apply(lambda coord: coord[1])\n",
    "\n",
    "df_cleaned = df\n",
    "\n",
    "## drop any rows with null lat or long\n",
    "df_cleaned = df_cleaned.dropna(subset=['lat', 'long'])\n",
    "\n",
    "## Drop any null lat long rows from the df_hr_locations dataframe\n",
    "df_hr_locations = df_hr_locations.dropna(subset=['latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the nearest field in the College KML file (df_cleaned) to each field in the HR dataset and create a column witht he distance measurement\n",
    "\n",
    "for index, row in df_hr_locations.iterrows():\n",
    "    lat = row['latitude']\n",
    "    lon = row['longitude']\n",
    "    distances = df_cleaned.apply(\n",
    "        lambda row: calculate_distance(lat, lon, row['home_plate'][0], row['home_plate'][1]), \n",
    "        axis=1)\n",
    "    df_hr_locations.loc[index, 'Nearest_Field'] = df.loc[distances.idxmin(), 'park_name']\n",
    "    ## Add the measurement as a new column\n",
    "    df_hr_locations.loc[index, 'Nearest_Field_Distance'] = distances.min()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# for index, row in df_hr_locations.iterrows():\n",
    "#     lat = row['latitude']\n",
    "#     lon = row['longitude']\n",
    "#     distances_2 = df_cleaned.apply(\n",
    "#         lambda row: calculate_distance(lat, lon, row['home_plate'][0], row['home_plate'][1]), \n",
    "#         axis=1)\n",
    "#     df_hr_locations.loc[index, 'Nearest_Field'] = df.loc[distances_2.idxmin(), 'park_name']\n",
    "#     df_hr_locations.loc[index, 'Nearest_Field_Distance'] = distances_2.min()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hr_locations.sample(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histograms of distance to closest park\n",
    "sns.histplot(data=df_hr_locations, x='Nearest_Field_Distance', bins=10)\n",
    "# add title and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## describe nearest field distance\n",
    "df_hr_locations['Nearest_Field_Distance'].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
