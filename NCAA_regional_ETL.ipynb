{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ETL NOTEBOOK FOR 2023 NCAA D1 REGIONAL DATA - 16 HOSTS + 3 Finals SITES\n",
    "\n",
    "#### Adapted from ETL for JSON\n",
    "\n",
    "## Dependencies and Setup\n",
    "### Dependencies\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "## Start timer\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD BLOCK###\n",
    "#### Load data from kml file exported from Google Earth\n",
    "\n",
    "file_path = ('data/kml/NCAA_regional.kml') # file path to kml file\n",
    "\n",
    "\n",
    "# Read the KML file\n",
    "with open(file_path) as file:\n",
    "    xml_data = file.read()\n",
    "\n",
    "# Initialize soup variables for parsing file\n",
    "soup = BeautifulSoup(xml_data, 'xml')\n",
    "folders = soup.Document\n",
    "list = soup.Document.find_all('Folder')\n",
    "\n",
    "# Create a list to store rows to append to the DataFrame\n",
    "rows = []\n",
    "\n",
    "# Loop through the folders and extract the data\n",
    "for folder in list:\n",
    "    try:\n",
    "        field_name = folder.find('name').text\n",
    "        foul = folder.find_all('coordinates')[0].text\n",
    "        fop = folder.find_all('coordinates')[1].text\n",
    "        notes = None\n",
    "\n",
    "        # Check if there is a description tag, if so, use it for notes\n",
    "        if folder.find('description') is not None:\n",
    "            notes = folder.find('description').text\n",
    "\n",
    "        row = {\n",
    "            'field': field_name,\n",
    "            'foul': foul,\n",
    "            'fop': fop,\n",
    "            'notes': notes\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Add name of folder to a list of failed folders\n",
    "        failed.append(folder.find('name').text)\n",
    "        print(f\"Error processing folder: {folder.find('name').text}. Error message: {str(e)}\")\n",
    "\n",
    "# Convert the list of rows to a DataFrame\n",
    "df = pd.DataFrame(rows, columns=['field', 'foul', 'fop', 'notes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the new dataframe\n",
    "\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Remove new line and space characters from coordinates\n",
    "df_cleaned = df_cleaned.replace(r'\\n','', regex=True) \n",
    "df_cleaned = df_cleaned.replace(r'\\t','', regex=True) \n",
    "\n",
    "# Drop any duplicate rows\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['field'], keep='first')\n",
    "\n",
    "# Drop any rows with empty fields\n",
    "df_cleaned = df_cleaned[(df_cleaned != 0).all(1)]\n",
    "\n",
    "##### Clean up polygon data and create a new home_plate column\n",
    "\n",
    "def parse_coordinates(coord_string):\n",
    "    coords = coord_string.split()\n",
    "    parsed_coords = [tuple(map(float, coord.split(',')[:2])) for coord in coords]\n",
    "    return parsed_coords\n",
    "\n",
    "# Create a new column for the home_plate location using the first set of coordinates in the 'fop' column\n",
    "df_cleaned['home_plate'] = df_cleaned['fop'].apply(lambda x: parse_coordinates(x)[0])\n",
    "\n",
    "# Apply the parse_coordinates function to the 'foul' and 'fop' columns\n",
    "df_cleaned['foul'] = df_cleaned['foul'].apply(parse_coordinates)\n",
    "df_cleaned['fop'] = df_cleaned['fop'].apply(parse_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## AREA CALCULATION ##############\n",
    "\n",
    "\n",
    "import pyproj\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import transform\n",
    "\n",
    "\n",
    "def calculate_area(coords):\n",
    "    # Create a Polygon object from the coordinates\n",
    "    polygon = Polygon(coords)\n",
    "\n",
    "    # Calculate the centroid of the polygon\n",
    "    centroid = polygon.centroid\n",
    "\n",
    "    # Create a custom LAEA projection centered on the centroid\n",
    "    custom_projection = f\"+proj=laea +lat_0={centroid.y} +lon_0={centroid.x} +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"\n",
    "\n",
    "    # Create a transformer for converting coordinates to the custom LAEA projection\n",
    "    transformer = pyproj.Transformer.from_crs(\n",
    "        pyproj.CRS(\"EPSG:4326\"),  # WGS 84 (latitude and longitude)\n",
    "        pyproj.CRS(custom_projection),  # Custom LAEA projection\n",
    "        always_xy=True\n",
    "    )\n",
    "\n",
    "    # Define a function to transform coordinates using the transformer\n",
    "    def transform_coordinates(x, y):\n",
    "        return transformer.transform(x, y)\n",
    "\n",
    "    # Convert the coordinates to the custom LAEA projection\n",
    "    polygon_laea = transform(transform_coordinates, polygon)\n",
    "\n",
    "    # Calculate the area in square meters\n",
    "    area_sqm = polygon_laea.area\n",
    "\n",
    "    # Convert the area to square feet (1 square meter = 10.764 square feet)\n",
    "    area_sqft = area_sqm * 10.764\n",
    "\n",
    "    return area_sqft\n",
    "\n",
    "\n",
    "\n",
    "### Call Function and add to dataframe\n",
    "df_cleaned['foul_area_sqft'] = df_cleaned['foul'].apply(calculate_area)\n",
    "df_cleaned['fop_area_sqft'] = df_cleaned['fop'].apply(calculate_area)\n",
    "\n",
    "## Calculate the total area of the field and the ratio of foul area to field area\n",
    "df_cleaned['field_area_sqft'] = df_cleaned['foul_area_sqft'] + df_cleaned['fop_area_sqft']\n",
    "## Percentage foul area\n",
    "df_cleaned['foul_area_per'] = df_cleaned['foul_area_sqft'] / df_cleaned['field_area_sqft']\n",
    "## Fair to Foul Ratio\n",
    "df_cleaned['fair_to_foul'] = df_cleaned['fop_area_sqft'] / df_cleaned['foul_area_sqft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# FENCE DISTANCE CALCULATION #############\n",
    "\n",
    "from geopy.distance import great_circle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def interpolate_points(start, end, length_ratio):\n",
    "    start_np = np.array(start)\n",
    "    end_np = np.array(end)\n",
    "    return tuple(start_np + (end_np - start_np) * length_ratio)\n",
    "\n",
    "def calculate_distances(home_plate, outfield_coords, num_points=540):\n",
    "    def is_same_point(point1, point2, tolerance=1e-6):\n",
    "        return abs(point1[0] - point2[0]) < tolerance and abs(point1[1] - point2[1]) < tolerance\n",
    "\n",
    "    home_plate_lat_lon = (home_plate[1], home_plate[0])\n",
    "    distances = []\n",
    "\n",
    "    # Calculate total line length\n",
    "    total_length = 0\n",
    "    segments = []\n",
    "    for i in range(len(outfield_coords) - 1):\n",
    "        start = outfield_coords[i]\n",
    "        end = outfield_coords[i + 1]\n",
    "        if not is_same_point(home_plate, start) and not is_same_point(home_plate, end):\n",
    "            segment_length = great_circle((start[1], start[0]), (end[1], end[0])).feet\n",
    "            segments.append((start, end, segment_length))\n",
    "            total_length += segment_length\n",
    "\n",
    "    # Calculate the distance between equally spaced points\n",
    "    spacing = total_length / (num_points - 1)\n",
    "\n",
    "    # Interpolate points and calculate distances\n",
    "    current_length = 0\n",
    "    segment_index = 0\n",
    "    for i in range(num_points):\n",
    "        while segment_index < len(segments) - 1 and current_length > segments[segment_index][2]:\n",
    "            current_length -= segments[segment_index][2]\n",
    "            segment_index += 1\n",
    "\n",
    "        start, end, segment_length = segments[segment_index]\n",
    "        length_ratio = current_length / segment_length\n",
    "        point = interpolate_points(start, end, length_ratio)\n",
    "        distance = great_circle(home_plate_lat_lon, (point[1], point[0])).feet\n",
    "        distances.append(distance)\n",
    "\n",
    "        current_length += spacing\n",
    "\n",
    "    return distances\n",
    "\n",
    "# Calculate distances for each row\n",
    "df_cleaned['distances'] = df_cleaned.apply(lambda row: calculate_distances(row['home_plate'], row['fop']), axis=1)\n",
    "\n",
    "# Calculate max, min, and average distances for each row\n",
    "df_cleaned['max_distance'] = df_cleaned['distances'].apply(max)\n",
    "df_cleaned['min_distance'] = df_cleaned['distances'].apply(min)\n",
    "df_cleaned['avg_distance'] = df_cleaned['distances'].apply(lambda distances: sum(distances) / len(distances))\n",
    "# get the median distance\n",
    "df_cleaned['median_distance'] = df_cleaned['distances'].apply(lambda distances: np.median(distances))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEDD TO DROP FOLDER OBJECT\n",
    "\n",
    "## Drop NCAA_Regional_Map\n",
    "df_cleaned = df_cleaned.drop(df_cleaned.index[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 22 entries, 1 to 22\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   field            22 non-null     object \n",
      " 1   foul             22 non-null     object \n",
      " 2   fop              22 non-null     object \n",
      " 3   notes            3 non-null      object \n",
      " 4   home_plate       22 non-null     object \n",
      " 5   foul_area_sqft   22 non-null     float64\n",
      " 6   fop_area_sqft    22 non-null     float64\n",
      " 7   field_area_sqft  22 non-null     float64\n",
      " 8   foul_area_per    22 non-null     float64\n",
      " 9   fair_to_foul     22 non-null     float64\n",
      " 10  distances        22 non-null     object \n",
      " 11  max_distance     22 non-null     float64\n",
      " 12  min_distance     22 non-null     float64\n",
      " 13  avg_distance     22 non-null     float64\n",
      " 14  median_distance  22 non-null     float64\n",
      "dtypes: float64(9), object(6)\n",
      "memory usage: 2.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.head()\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORNER SHARPNESS - PROBABLY WANT TO REPLACE THIS WITH A OUTFIELD ANGLE CALCULATION\n",
    "\n",
    "def calculate_corner_sharpness(distances):\n",
    "    # Calculate the difference between each distance and its neighbors\n",
    "    differences = np.diff(distances)\n",
    "    \n",
    "    # Take the absolute value to ignore whether the difference is positive or negative\n",
    "    differences = np.abs(differences)\n",
    "    \n",
    "    # Sum up the differences to get a total \"sharpness\" score\n",
    "    sharpness_score = np.sum(differences)\n",
    "    \n",
    "    return sharpness_score\n",
    "\n",
    "# Calculate the sharpemss score for each row\n",
    "df_cleaned['corner_sharpness'] = df_cleaned['distances'].apply(calculate_corner_sharpness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create ranks for each column\n",
    "\n",
    "def rank_fields(df):\n",
    "    # Calculate the rank for each category\n",
    "    df['max_distance_rank'] = df['max_distance'].rank(ascending=False, method='min')\n",
    "    df['min_distance_rank'] = df['min_distance'].rank(ascending=False, method='min')\n",
    "    df['avg_distance_rank'] = df['avg_distance'].rank(ascending=False, method='min')\n",
    "    df['median_distance_rank'] = df['median_distance'].rank(ascending=False, method='min')\n",
    "    df['field_area_rank'] = df['field_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['foul_area_rank'] = df['foul_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['fop_area_per_rank'] = df['fop_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['ratio_rank'] = df['fair_to_foul'].rank(ascending=False, method='min')\n",
    "\n",
    "    return df\n",
    "\n",
    "## Run Function\n",
    "\n",
    "df_cleaned = rank_fields(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Orienting the map to the home plate location ####\n",
    "\n",
    "### Find the center of the field\n",
    "def calculate_centroid(coords):\n",
    "    x_coords = [coord[0] for coord in coords]\n",
    "    y_coords = [coord[1] for coord in coords]\n",
    "    centroid_x = sum(x_coords) / len(coords)\n",
    "    centroid_y = sum(y_coords) / len(coords)\n",
    "    return (centroid_x, centroid_y)\n",
    "\n",
    "\n",
    "## Find the bearing between the home plate and the center of the field\n",
    "import math\n",
    "\n",
    "def calculate_bearing(point1, point2):\n",
    "    lat1, lon1 = math.radians(point1[1]), math.radians(point1[0])\n",
    "    lat2, lon2 = math.radians(point2[1]), math.radians(point2[0])\n",
    "\n",
    "    d_lon = lon2 - lon1\n",
    "\n",
    "    x = math.cos(lat2) * math.sin(d_lon)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(d_lon)\n",
    "\n",
    "    bearing = math.degrees(math.atan2(x, y))\n",
    "    bearing = (bearing + 360) % 360  # Normalize the bearing to the range [0, 360)\n",
    "\n",
    "    return bearing\n",
    "\n",
    "### Function to classify direction in laymans terms North, South, East, West, ect\n",
    "def degrees_to_cardinal_direction(degrees):\n",
    "    directions = ['North', 'Northeast', 'East', 'Southeast', 'South', 'Southwest', 'West', 'Northwest', 'North']\n",
    "    index = round(degrees / 45)\n",
    "    return directions[index]\n",
    "\n",
    "\n",
    "# Calculate the centroid of the outfield fence coordinates for each row\n",
    "df_cleaned['fop_centroid'] = df_cleaned['fop'].apply(lambda coords: calculate_centroid(coords[1:]))\n",
    "\n",
    "# Calculate the bearing between home plate and the centroid for each row\n",
    "df_cleaned['field_orientation'] = df_cleaned.apply(lambda row: calculate_bearing(row['home_plate'], row['fop_centroid']), axis=1)\n",
    "\n",
    "# Convert the bearing to a cardinal direction\n",
    "df_cleaned['field_cardinal_direction'] = df_cleaned['field_orientation'].apply(degrees_to_cardinal_direction)\n",
    "\n",
    "# rename 'field' to 'park_name'\n",
    "df_cleaned.rename(columns={'field': 'park_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            park_name     score\n",
      "11                   Clemson - Doug Kingsmore Stadium  7.369327\n",
      "12                         Coastal Carolina - college  7.196044\n",
      "10                            Auburn - Plainsman Park  5.079798\n",
      "8                     Alabama - Sewell–Thomas Stadium  4.223985\n",
      "1                     TCU - Texas Christian - college  3.615535\n",
      "18                      U of South Carolina - college  2.564203\n",
      "9            Arkansas - Baum-Walker Stadium - college  1.905866\n",
      "21                             Wake Forrest - college  1.773028\n",
      "16    Miami - Alex Rodriguez Park at Mark Light Field  1.491373\n",
      "20               Vanderbilt - Hawkins Field - college  0.833920\n",
      "2                                       Southern Miss  0.267855\n",
      "15                   LSU - Alex Box Stadium - college -0.496179\n",
      "13                            Indiana State - college -0.761681\n",
      "7                                 Florida Univ - TEMP -1.864287\n",
      "19                            U of Virginia - college -2.123384\n",
      "22                   Oklahoma State - O'Brate Stadium -2.411114\n",
      "14                     Kentucky - Kentucky Proud Park -2.762524\n",
      "17                   Stanford - Klein Field - college -3.177174\n",
      "3                         Oregon University - college -3.493877\n",
      "4                Charles Schwab Field Omaha - college -5.440232\n",
      "5   Veterans Memorial Stadium - Cedar Rapids Kerne... -5.547803\n",
      "6       Coleman Field - USA National Baseball Stadium -8.242679\n"
     ]
    }
   ],
   "source": [
    "### THIS BLOCK CREATES THE RANKING OF PITCHER VS HITTER FRIENDLY FIELDS\n",
    "def rank_fields(data):\n",
    "    # Define weights for each parameter\n",
    "    weights = {\n",
    "        'max_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'min_distance': 1,  # positive weight since shorter fences favor hitters\n",
    "        'avg_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'median_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'field_area_sqft': -1,  # negative weight since larger fields favor pitchers\n",
    "        'fair_to_foul': -1,  # negative weight since larger ratio (more foul territory) favors pitchers\n",
    "        'foul_area_sqft': -1, # negative weight since larger foul area favors pitchers\n",
    "        'fop_area_sqft': -1, # negative weight since larger out of play area favors pitchers\n",
    "    }\n",
    "\n",
    "    # Standardize features (subtract mean and divide by standard deviation)\n",
    "    standardized_data = data.copy()\n",
    "    for column in weights.keys():\n",
    "        standardized_data[column] = (standardized_data[column] - standardized_data[column].mean()) / standardized_data[column].std()\n",
    "\n",
    "    # Calculate score for each field\n",
    "    standardized_data['score'] = standardized_data.apply(lambda row: sum(row[param] * weight for param, weight in weights.items()), axis=1)\n",
    "\n",
    "    # Save scores to original dataframe\n",
    "    data['score'] = standardized_data['score']\n",
    "\n",
    "    # Rank fields based on score (higher scores are more hitter-friendly)\n",
    "    ranked_fields = data.sort_values('score', ascending=False)\n",
    "\n",
    "    return ranked_fields\n",
    "\n",
    "# Suppose 'df' is your DataFrame containing the field data\n",
    "df = rank_fields(df_cleaned)\n",
    "print(df[['park_name', 'score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 22 entries, 11 to 6\n",
      "Data columns (total 28 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   park_name                 22 non-null     object \n",
      " 1   foul                      22 non-null     object \n",
      " 2   fop                       22 non-null     object \n",
      " 3   notes                     3 non-null      object \n",
      " 4   home_plate                22 non-null     object \n",
      " 5   foul_area_sqft            22 non-null     float64\n",
      " 6   fop_area_sqft             22 non-null     float64\n",
      " 7   field_area_sqft           22 non-null     float64\n",
      " 8   foul_area_per             22 non-null     float64\n",
      " 9   fair_to_foul              22 non-null     float64\n",
      " 10  distances                 22 non-null     object \n",
      " 11  max_distance              22 non-null     float64\n",
      " 12  min_distance              22 non-null     float64\n",
      " 13  avg_distance              22 non-null     float64\n",
      " 14  median_distance           22 non-null     float64\n",
      " 15  corner_sharpness          22 non-null     float64\n",
      " 16  max_distance_rank         22 non-null     float64\n",
      " 17  min_distance_rank         22 non-null     float64\n",
      " 18  avg_distance_rank         22 non-null     float64\n",
      " 19  median_distance_rank      22 non-null     float64\n",
      " 20  field_area_rank           22 non-null     float64\n",
      " 21  foul_area_rank            22 non-null     float64\n",
      " 22  fop_area_per_rank         22 non-null     float64\n",
      " 23  ratio_rank                22 non-null     float64\n",
      " 24  fop_centroid              22 non-null     object \n",
      " 25  field_orientation         22 non-null     float64\n",
      " 26  field_cardinal_direction  22 non-null     object \n",
      " 27  score                     22 non-null     float64\n",
      "dtypes: float64(20), object(8)\n",
      "memory usage: 5.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21 entries, 0 to 20\n",
      "Data columns (total 18 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   park_name       21 non-null     object\n",
      " 1   plotted?        21 non-null     object\n",
      " 2   host_school     21 non-null     object\n",
      " 3   nickname        21 non-null     object\n",
      " 4   logo_file       21 non-null     object\n",
      " 5   display_name    21 non-null     object\n",
      " 6   display_name2   8 non-null      object\n",
      " 7   color1          21 non-null     object\n",
      " 8   color2          21 non-null     object\n",
      " 9   color3          4 non-null      object\n",
      " 10  found_date      21 non-null     int64 \n",
      " 11  capacity        21 non-null     object\n",
      " 12  former_names    13 non-null     object\n",
      " 13  renovation      15 non-null     object\n",
      " 14  st_address      16 non-null     object\n",
      " 15  coords_string1  14 non-null     object\n",
      " 16  surface         21 non-null     object\n",
      " 17  description     18 non-null     object\n",
      "dtypes: int64(1), object(17)\n",
      "memory usage: 3.1+ KB\n"
     ]
    }
   ],
   "source": [
    "### Get the Field data to merge in\n",
    "## Load the data from the csv file\n",
    "\n",
    "## Open the CSV file with the conference names\n",
    "conf_df = pd.read_csv('data/NCAA_D1/NCAA_regional_sites.csv')\n",
    "\n",
    "df.info()\n",
    "conf_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 22 entries, 0 to 21\n",
      "Data columns (total 45 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   park_name                 22 non-null     object \n",
      " 1   foul                      22 non-null     object \n",
      " 2   fop                       22 non-null     object \n",
      " 3   notes                     3 non-null      object \n",
      " 4   home_plate                22 non-null     object \n",
      " 5   foul_area_sqft            22 non-null     float64\n",
      " 6   fop_area_sqft             22 non-null     float64\n",
      " 7   field_area_sqft           22 non-null     float64\n",
      " 8   foul_area_per             22 non-null     float64\n",
      " 9   fair_to_foul              22 non-null     float64\n",
      " 10  distances                 22 non-null     object \n",
      " 11  max_distance              22 non-null     float64\n",
      " 12  min_distance              22 non-null     float64\n",
      " 13  avg_distance              22 non-null     float64\n",
      " 14  median_distance           22 non-null     float64\n",
      " 15  corner_sharpness          22 non-null     float64\n",
      " 16  max_distance_rank         22 non-null     float64\n",
      " 17  min_distance_rank         22 non-null     float64\n",
      " 18  avg_distance_rank         22 non-null     float64\n",
      " 19  median_distance_rank      22 non-null     float64\n",
      " 20  field_area_rank           22 non-null     float64\n",
      " 21  foul_area_rank            22 non-null     float64\n",
      " 22  fop_area_per_rank         22 non-null     float64\n",
      " 23  ratio_rank                22 non-null     float64\n",
      " 24  fop_centroid              22 non-null     object \n",
      " 25  field_orientation         22 non-null     float64\n",
      " 26  field_cardinal_direction  22 non-null     object \n",
      " 27  score                     22 non-null     float64\n",
      " 28  plotted?                  21 non-null     object \n",
      " 29  host_school               21 non-null     object \n",
      " 30  nickname                  21 non-null     object \n",
      " 31  logo_file                 21 non-null     object \n",
      " 32  display_name              21 non-null     object \n",
      " 33  display_name2             8 non-null      object \n",
      " 34  color1                    21 non-null     object \n",
      " 35  color2                    21 non-null     object \n",
      " 36  color3                    4 non-null      object \n",
      " 37  found_date                21 non-null     float64\n",
      " 38  capacity                  21 non-null     object \n",
      " 39  former_names              13 non-null     object \n",
      " 40  renovation                15 non-null     object \n",
      " 41  st_address                16 non-null     object \n",
      " 42  coords_string1            14 non-null     object \n",
      " 43  surface                   21 non-null     object \n",
      " 44  description               18 non-null     object \n",
      "dtypes: float64(21), object(24)\n",
      "memory usage: 7.9+ KB\n"
     ]
    }
   ],
   "source": [
    "## Merge the two dataframes\n",
    "\n",
    "df = pd.merge(df, conf_df, on='park_name', how='outer')\n",
    "\n",
    "df.info()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rename logo column to work with existing javascript\n",
    "df.rename(columns={'logo_file': 'filename'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:46<00:00,  2.09s/it]\n"
     ]
    }
   ],
   "source": [
    "## Get Altitudes of the ballparks\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Set your Google Maps API key here\n",
    "api_key = 'AIzaSyA_BhlTupRdBPBhRptQuR6pYorMVYQnRMA'\n",
    "\n",
    "# Get the altitude of a location from its latitude and longitude\n",
    "def get_altitude(lat, lon):\n",
    "    query = f'https://maps.googleapis.com/maps/api/elevation/json?locations={lat},{lon}&key={api_key}'\n",
    "    r = requests.get(query).json()\n",
    "    elevation = r['results'][0]['elevation']\n",
    "    return elevation\n",
    "\n",
    "# Get the city and state of a location from its latitude and longitude\n",
    "def get_city_state(lat, lon):\n",
    "    query = f'https://maps.googleapis.com/maps/api/geocode/json?latlng={lat},{lon}&key={api_key}'\n",
    "    r = requests.get(query).json()\n",
    "    results = r['results'][0]['address_components']\n",
    "    city = next((item['long_name'] for item in results if 'locality' in item['types']), '')\n",
    "    state = next((item['long_name'] for item in results if 'administrative_area_level_1' in item['types']), '')\n",
    "    return city, state\n",
    "\n",
    "# Initialize empty lists for the new columns\n",
    "altitudes = []\n",
    "cities = []\n",
    "states = []\n",
    "\n",
    "# Loop through each row in the dataframe\n",
    "for coords in tqdm(df['home_plate']):\n",
    "    # Get altitude and add to list\n",
    "    altitude = get_altitude(coords[1], coords[0])\n",
    "    altitudes.append(altitude)\n",
    "\n",
    "    # Get city and state and add to lists\n",
    "    city, state = get_city_state(coords[1], coords[0])\n",
    "    cities.append(city)\n",
    "    states.append(state)\n",
    "\n",
    "    # Sleep for a bit to avoid hitting rate limits\n",
    "    time.sleep(1)  # Adjust this value as needed\n",
    "\n",
    "# Add the new columns to the dataframe\n",
    "df['altitude'] = altitudes\n",
    "df['city'] = cities\n",
    "df['state'] = states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def rate_fields(df, weights):\n",
    "    # Copy original values to new columns\n",
    "    df[['original_foul_area_sqft', 'original_fop_area_sqft', 'original_fair_to_foul', 'original_max_distance', 'original_min_distance', 'original_avg_distance', 'original_median_distance', 'original_corner_sharpness', 'original_field_orientation', 'original_altitude']] = df[['foul_area_sqft', 'fop_area_sqft', 'fair_to_foul', 'max_distance', 'min_distance', 'avg_distance', 'median_distance', 'corner_sharpness', 'field_orientation', 'altitude']]\n",
    "\n",
    "    # Normalize the data\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    df[['foul_area_sqft', 'fop_area_sqft', 'fair_to_foul', 'max_distance', 'min_distance', 'avg_distance', 'median_distance', 'corner_sharpness', 'field_orientation', 'altitude']] = min_max_scaler.fit_transform(df[['foul_area_sqft', 'fop_area_sqft', 'fair_to_foul', 'max_distance', 'min_distance', 'avg_distance', 'median_distance', 'corner_sharpness', 'field_orientation', 'altitude']])\n",
    "    \n",
    "\n",
    "    # Weighted score for home run friendliness\n",
    "    df['hr_friendliness'] = weights['foul_area_sqft']*df['foul_area_sqft'] + weights['fair_to_foul']*df['fair_to_foul'] - weights['max_distance']*df['max_distance'] - weights['avg_distance']*df['avg_distance'] - weights['median_distance']*df['median_distance'] + weights['altitude']*df['altitude']\n",
    "\n",
    "    # Weighted score for old-school friendliness\n",
    "    df['old_school_friendliness'] = weights['fop_area_sqft']*df['fop_area_sqft'] + weights['max_distance']*df['max_distance'] + weights['avg_distance']*df['avg_distance'] + weights['median_distance']*df['median_distance'] + weights['corner_sharpness']*df['corner_sharpness'] - weights['altitude']*df['altitude']\n",
    "\n",
    "    # Field uniqueness score based on variance from mean values\n",
    "    mean_values = df[['foul_area_sqft', 'fop_area_sqft', 'fair_to_foul', 'max_distance', 'min_distance', 'avg_distance', 'median_distance', 'corner_sharpness', 'field_orientation', 'altitude']].mean()\n",
    "    df['uniqueness_score'] = df[['foul_area_sqft', 'fop_area_sqft', 'fair_to_foul', 'max_distance', 'min_distance', 'avg_distance', 'median_distance', 'corner_sharpness', 'field_orientation', 'altitude']].apply(lambda row: sum(abs(row - mean_values)), axis=1)\n",
    "\n",
    "      # Remove the normalized columns, restoring the original values\n",
    "    df[['foul_area_sqft', 'fop_area_sqft', 'fair_to_foul', 'max_distance', 'min_distance', 'avg_distance', 'median_distance', 'corner_sharpness', 'field_orientation', 'altitude']] = df[['original_foul_area_sqft', 'original_fop_area_sqft', 'original_fair_to_foul', 'original_max_distance', 'original_min_distance', 'original_avg_distance', 'original_median_distance', 'original_corner_sharpness', 'original_field_orientation', 'original_altitude']]\n",
    "    \n",
    "    # Remove the temporary columns storing the original values\n",
    "    df = df.drop(columns=['original_foul_area_sqft', 'original_fop_area_sqft', 'original_fair_to_foul', 'original_max_distance', 'original_min_distance', 'original_avg_distance', 'original_median_distance', 'original_corner_sharpness', 'original_field_orientation', 'original_altitude'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "weights = {'foul_area_sqft': 1, 'fop_area_sqft': 1, 'fair_to_foul': 1, 'max_distance': 1, 'min_distance': 1.25, 'avg_distance': 1.25, 'median_distance': 1, 'corner_sharpness': 1, 'field_orientation': 1, 'altitude': 1.5}\n",
    "\n",
    "df = rate_fields(df, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Updated from MHSAAA to create Standard Div +/- Lines\n",
    "\n",
    "## create the min max and mean fence distance rows\n",
    "# Transpose the dataframe to get the \n",
    "transposed_df = pd.DataFrame(df['distances'].to_list()).transpose()\n",
    "\n",
    "# Calculate min, max, mean, median, Q1 and Q3 for each row\n",
    "min_fence_distances = transposed_df.min(axis=1)\n",
    "max_fence_distances = transposed_df.max(axis=1)\n",
    "mean_fence_distances = transposed_df.mean(axis=1)\n",
    "median_fence_distances = transposed_df.median(axis=1)\n",
    "## create profiles for standard deviation\n",
    "std_fence_distances = transposed_df.std(axis=1)\n",
    "first_fence_distances = mean_fence_distances + std_fence_distances\n",
    "third_fence_distances = mean_fence_distances - std_fence_distances\n",
    "\n",
    "# Create a new DataFrame to store these values\n",
    "new_df = pd.DataFrame({\n",
    "    'park_name': ['Min', 'Max', 'Mean', 'Median', 'Q1', 'Q3'],\n",
    "    'distances': [\n",
    "        min_fence_distances.tolist(), \n",
    "        max_fence_distances.tolist(),\n",
    "        mean_fence_distances.tolist(),\n",
    "        median_fence_distances.tolist(), # Add a comma here\n",
    "        first_fence_distances.tolist(),\n",
    "        third_fence_distances.tolist()\n",
    "    ]\n",
    "})\n",
    "\n",
    "# For all other columns in the original DataFrame, add a column of NaN values in the new DataFrame\n",
    "for column in df.columns:\n",
    "    if column not in new_df.columns:\n",
    "        new_df[column] = np.nan\n",
    "\n",
    "# Concatenate the new DataFrame with the original one\n",
    "df = pd.concat([df, new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28 entries, 0 to 27\n",
      "Data columns (total 51 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   park_name                 28 non-null     object \n",
      " 1   foul                      22 non-null     object \n",
      " 2   fop                       22 non-null     object \n",
      " 3   notes                     3 non-null      object \n",
      " 4   home_plate                22 non-null     object \n",
      " 5   foul_area_sqft            22 non-null     float64\n",
      " 6   fop_area_sqft             22 non-null     float64\n",
      " 7   field_area_sqft           22 non-null     float64\n",
      " 8   foul_area_per             22 non-null     float64\n",
      " 9   fair_to_foul              22 non-null     float64\n",
      " 10  distances                 28 non-null     object \n",
      " 11  max_distance              22 non-null     float64\n",
      " 12  min_distance              22 non-null     float64\n",
      " 13  avg_distance              22 non-null     float64\n",
      " 14  median_distance           22 non-null     float64\n",
      " 15  corner_sharpness          22 non-null     float64\n",
      " 16  max_distance_rank         22 non-null     float64\n",
      " 17  min_distance_rank         22 non-null     float64\n",
      " 18  avg_distance_rank         22 non-null     float64\n",
      " 19  median_distance_rank      22 non-null     float64\n",
      " 20  field_area_rank           22 non-null     float64\n",
      " 21  foul_area_rank            22 non-null     float64\n",
      " 22  fop_area_per_rank         22 non-null     float64\n",
      " 23  ratio_rank                22 non-null     float64\n",
      " 24  fop_centroid              22 non-null     object \n",
      " 25  field_orientation         22 non-null     float64\n",
      " 26  field_cardinal_direction  22 non-null     object \n",
      " 27  score                     22 non-null     float64\n",
      " 28  plotted?                  21 non-null     object \n",
      " 29  host_school               21 non-null     object \n",
      " 30  nickname                  21 non-null     object \n",
      " 31  filename                  21 non-null     object \n",
      " 32  display_name              21 non-null     object \n",
      " 33  display_name2             8 non-null      object \n",
      " 34  color1                    21 non-null     object \n",
      " 35  color2                    21 non-null     object \n",
      " 36  color3                    4 non-null      object \n",
      " 37  found_date                21 non-null     float64\n",
      " 38  capacity                  21 non-null     object \n",
      " 39  former_names              13 non-null     object \n",
      " 40  renovation                15 non-null     object \n",
      " 41  st_address                16 non-null     object \n",
      " 42  coords_string1            14 non-null     object \n",
      " 43  surface                   21 non-null     object \n",
      " 44  description               18 non-null     object \n",
      " 45  altitude                  22 non-null     float64\n",
      " 46  city                      22 non-null     object \n",
      " 47  state                     22 non-null     object \n",
      " 48  hr_friendliness           22 non-null     float64\n",
      " 49  old_school_friendliness   22 non-null     float64\n",
      " 50  uniqueness_score          22 non-null     float64\n",
      "dtypes: float64(25), object(26)\n",
      "memory usage: 11.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the output directory for the Outfield Fence Plots\n",
    "\n",
    "output_dir = 'data/NCAA_D1/assests/graphs/'\n",
    "\n",
    "def plot_distances(df, row_index):\n",
    "    # Get rows with 'Min', 'Max', 'Mean', 'Q1', 'Q3' in 'park_name'\n",
    "    rows_to_plot = df[df['park_name'].isin(['Min', 'Max', 'Mean', 'Q1', 'Q3'])]\n",
    "    \n",
    "    # Get the row to be highlighted\n",
    "    highlighted_row = df.loc[row_index]\n",
    "    \n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    # Loop over these rows and plot a line graph for each\n",
    "    for index, row in rows_to_plot.iterrows():\n",
    "        if row['park_name'] in ['Q1', 'Q3']: # If Q1 or Q3, plot thinner, dotted line\n",
    "            plt.plot(row['distances'], linestyle='dotted', alpha=0.3, color='grey', label=row['park_name'])\n",
    "        else:\n",
    "            plt.plot(row['distances'], linestyle='dashed', alpha=0.5, label=row['park_name'])\n",
    "\n",
    "        # Add text labels for Min, Max and Mean lines\n",
    "        if row['park_name'] in ['Min', 'Max', 'Mean']:\n",
    "            plt.text(len(row['distances'])-1, row['distances'][-1], row['park_name'], color='blue', va='center')\n",
    "\n",
    "        # Check if the current row is 'Min', if so, add shading\n",
    "        if row['park_name'] == 'Min':\n",
    "            plt.fill_between(range(len(row['distances'])), row['distances'], color='green', alpha=0.4)\n",
    "\n",
    "        # Check if the current row is 'Max', if so, add shading\n",
    "        if row['park_name'] == 'Max':\n",
    "            plt.fill_between(range(len(row['distances'])), row['distances'], color='yellow', alpha=0.2)\n",
    "\n",
    "        # Check if the current row is 'Max', if so, add shading above\n",
    "        if row['park_name'] == 'Max':\n",
    "            plt.fill_between(range(len(row['distances'])), plt.ylim()[1], row['distances'], color='red', alpha=0.3)\n",
    "            \n",
    "    # Plot the highlighted row with a thicker line\n",
    "    plt.plot(highlighted_row['distances'], linewidth=2, label=highlighted_row['park_name'])\n",
    "    \n",
    "    # Set the minimum and maximum values of y-axis\n",
    "    plt.ylim([300, 420])\n",
    "\n",
    "    # Change y-axis labels and tick marks to be white\n",
    "    plt.ylabel('Distance (feet)', color='white')\n",
    "    plt.tick_params(axis='y', colors='white')\n",
    "\n",
    "    # Hide x axis ticks\n",
    "    plt.xticks([])\n",
    "\n",
    "    # Move the title to the inside the plot, centered, just above the x axis\n",
    "    plt.text(len(highlighted_row['distances'])/2, 270, highlighted_row['display_name'], ha='center', va='bottom', fontsize=16)\n",
    "    \n",
    "    # Add a title with the field name and host school\n",
    "    # plt.title(f\"{highlighted_row['display_name']} ({highlighted_row['host_school']})\", color='white', fontsize=16)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Reverse the x-axis\n",
    "    plt.gca().invert_xaxis()\n",
    "\n",
    "    # Generate the file path\n",
    "    file_path = os.path.join(output_dir, f\"plot_{row_index}.png\")\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(file_path)\n",
    "\n",
    "    # Close the figure to free up memory\n",
    "    plt.close()\n",
    "\n",
    "    # Return the file path\n",
    "    return file_path\n",
    "\n",
    "# Add a new column 'file_path' to the DataFrame to store the file paths\n",
    "df['file_path'] = [plot_distances(df, i) for i in df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the function for each row in the dataframe, save the figure\n",
    "# ## and store the filename in a new column\n",
    "\n",
    "# df['graph'] = ''  # add a new column\n",
    "\n",
    "# for idx, row in df.iterrows():\n",
    "#     plot_distances(df, idx)\n",
    "#     filename = f'lineplot_{idx}.png'  # create a unique filename using the index\n",
    "#     df.loc[idx, 'graph'] = filename  # save the filename to the dataframe\n",
    "#     plt.savefig(filename)  # save the figure\n",
    "#     plt.close()  # close the figure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop any rows with empty home_plate column (they are breaking the javascript)\n",
    "\n",
    "df = df.dropna(subset=['home_plate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output the Files as a JSON and as a csv for review\n",
    "\n",
    "df.to_csv('data/NCAA_D1/regional_tourn_map.csv', index=False)\n",
    "\n",
    "df.to_json('data/NCAA_D1/regional_tourn_map.json', orient='records')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New function to create plots with titles and logos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
