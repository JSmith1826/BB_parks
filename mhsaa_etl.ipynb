{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ETL NOTEBOOK FOR 2023 MHSAA TOURNEY SPECIFIC MAP\n",
    "\n",
    "#### Adapted from ETL for JSON\n",
    "\n",
    "## Dependencies and Setup\n",
    "### Dependencies\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "## Start timer\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD BLOCK###\n",
    "#### Load data from kml file exported from Google Earth\n",
    "\n",
    "file_path = ('data/kml/MHSAA_2023.kml') # file path to kml file\n",
    "\n",
    "\n",
    "# Read the KML file\n",
    "with open(file_path) as file:\n",
    "    xml_data = file.read()\n",
    "\n",
    "# Initialize soup variables for parsing file\n",
    "soup = BeautifulSoup(xml_data, 'xml')\n",
    "folders = soup.Document.Folder\n",
    "list = soup.Document.Folder.find_all('Folder')\n",
    "\n",
    "# Create a list to store rows to append to the DataFrame\n",
    "rows = []\n",
    "\n",
    "# Loop through the folders and extract the data\n",
    "for folder in list:\n",
    "    try:\n",
    "        field_name = folder.find('name').text\n",
    "        foul = folder.find_all('coordinates')[0].text\n",
    "        fop = folder.find_all('coordinates')[1].text\n",
    "        notes = None\n",
    "\n",
    "        # Check if there is a description tag, if so, use it for notes\n",
    "        if folder.find('description') is not None:\n",
    "            notes = folder.find('description').text\n",
    "\n",
    "        row = {\n",
    "            'field': field_name,\n",
    "            'foul': foul,\n",
    "            'fop': fop,\n",
    "            'notes': notes\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Add name of folder to a list of failed folders\n",
    "        failed.append(folder.find('name').text)\n",
    "        print(f\"Error processing folder: {folder.find('name').text}. Error message: {str(e)}\")\n",
    "\n",
    "# Convert the list of rows to a DataFrame\n",
    "df = pd.DataFrame(rows, columns=['field', 'foul', 'fop', 'notes'])\n",
    "\n",
    "# print('Failed to parse:', failed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indian River Inland Lakes HS</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.6358958,45.3838713,0 -84...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.6358958,45.3838713,0 -84...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Holton HS</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-86.0881713,43.4149801,0 -86...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-86.0881713,43.4149801,0 -86...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jackson Lumen Christi HS</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.4602665,42.2219816,0 -84...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.4602665,42.2219816,0 -84...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cornerstone Baseball Field - college</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-85.5975511,42.9811225,0 -85...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-85.5975511,42.9811225,0 -85...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adrian College</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.0697145,41.901861,0 -84....</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.0697145,41.901861,0 -84....</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  field  \\\n",
       "0          Indian River Inland Lakes HS   \n",
       "1                             Holton HS   \n",
       "2              Jackson Lumen Christi HS   \n",
       "3  Cornerstone Baseball Field - college   \n",
       "4                        Adrian College   \n",
       "\n",
       "                                                foul  \\\n",
       "0  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.6358958,45.3838713,0 -84...   \n",
       "1  \\n\\t\\t\\t\\t\\t\\t\\t\\t-86.0881713,43.4149801,0 -86...   \n",
       "2  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.4602665,42.2219816,0 -84...   \n",
       "3  \\n\\t\\t\\t\\t\\t\\t\\t\\t-85.5975511,42.9811225,0 -85...   \n",
       "4  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.0697145,41.901861,0 -84....   \n",
       "\n",
       "                                                 fop notes  \n",
       "0  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.6358958,45.3838713,0 -84...  None  \n",
       "1  \\n\\t\\t\\t\\t\\t\\t\\t\\t-86.0881713,43.4149801,0 -86...  None  \n",
       "2  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.4602665,42.2219816,0 -84...  None  \n",
       "3  \\n\\t\\t\\t\\t\\t\\t\\t\\t-85.5975511,42.9811225,0 -85...  None  \n",
       "4  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.0697145,41.901861,0 -84....  None  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the new dataframe\n",
    "\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Remove new line and space characters from coordinates\n",
    "df_cleaned = df_cleaned.replace(r'\\n','', regex=True) \n",
    "df_cleaned = df_cleaned.replace(r'\\t','', regex=True) \n",
    "\n",
    "# Drop any duplicate rows\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['field'], keep='first')\n",
    "\n",
    "# Drop any rows with empty fields\n",
    "df_cleaned = df_cleaned[(df_cleaned != 0).all(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 142 entries, 0 to 145\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   field   142 non-null    object\n",
      " 1   foul    142 non-null    object\n",
      " 2   fop     142 non-null    object\n",
      " 3   notes   7 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 5.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indian River Inland Lakes HS</td>\n",
       "      <td>-84.6358958,45.3838713,0 -84.6346834,45.383949...</td>\n",
       "      <td>-84.6358958,45.3838713,0 -84.63578579999999,45...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Holton HS</td>\n",
       "      <td>-86.0881713,43.4149801,0 -86.0887185,43.415773...</td>\n",
       "      <td>-86.0881713,43.4149801,0 -86.0870568,43.415367...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jackson Lumen Christi HS</td>\n",
       "      <td>-84.4602665,42.2219816,0 -84.4603248,42.222842...</td>\n",
       "      <td>-84.4602665,42.2219816,0 -84.4591654,42.222017...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cornerstone Baseball Field - college</td>\n",
       "      <td>-85.5975511,42.9811225,0 -85.5980533,42.981930...</td>\n",
       "      <td>-85.5975511,42.9811225,0 -85.59644400000001,42...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adrian College</td>\n",
       "      <td>-84.0697145,41.901861,0 -84.0703958,41.9026485...</td>\n",
       "      <td>-84.0697145,41.901861,0 -84.0687248,41.9023461...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  field  \\\n",
       "0          Indian River Inland Lakes HS   \n",
       "1                             Holton HS   \n",
       "2              Jackson Lumen Christi HS   \n",
       "3  Cornerstone Baseball Field - college   \n",
       "4                        Adrian College   \n",
       "\n",
       "                                                foul  \\\n",
       "0  -84.6358958,45.3838713,0 -84.6346834,45.383949...   \n",
       "1  -86.0881713,43.4149801,0 -86.0887185,43.415773...   \n",
       "2  -84.4602665,42.2219816,0 -84.4603248,42.222842...   \n",
       "3  -85.5975511,42.9811225,0 -85.5980533,42.981930...   \n",
       "4  -84.0697145,41.901861,0 -84.0703958,41.9026485...   \n",
       "\n",
       "                                                 fop notes  \n",
       "0  -84.6358958,45.3838713,0 -84.63578579999999,45...  None  \n",
       "1  -86.0881713,43.4149801,0 -86.0870568,43.415367...  None  \n",
       "2  -84.4602665,42.2219816,0 -84.4591654,42.222017...  None  \n",
       "3  -85.5975511,42.9811225,0 -85.59644400000001,42...  None  \n",
       "4  -84.0697145,41.901861,0 -84.0687248,41.9023461...  None  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.info()\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Clean up polygon data and create a new home_plate column\n",
    "\n",
    "def parse_coordinates(coord_string):\n",
    "    coords = coord_string.split()\n",
    "    parsed_coords = [tuple(map(float, coord.split(',')[:2])) for coord in coords]\n",
    "    return parsed_coords\n",
    "\n",
    "# Create a new column for the home_plate location using the first set of coordinates in the 'fop' column\n",
    "df_cleaned['home_plate'] = df_cleaned['fop'].apply(lambda x: parse_coordinates(x)[0])\n",
    "\n",
    "# Apply the parse_coordinates function to the 'foul' and 'fop' columns\n",
    "df_cleaned['foul'] = df_cleaned['foul'].apply(parse_coordinates)\n",
    "df_cleaned['fop'] = df_cleaned['fop'].apply(parse_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## AREA CALCULATION ##############\n",
    "\n",
    "\n",
    "import pyproj\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import transform\n",
    "\n",
    "\n",
    "def calculate_area(coords):\n",
    "    # Create a Polygon object from the coordinates\n",
    "    polygon = Polygon(coords)\n",
    "\n",
    "    # Calculate the centroid of the polygon\n",
    "    centroid = polygon.centroid\n",
    "\n",
    "    # Create a custom LAEA projection centered on the centroid\n",
    "    custom_projection = f\"+proj=laea +lat_0={centroid.y} +lon_0={centroid.x} +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"\n",
    "\n",
    "    # Create a transformer for converting coordinates to the custom LAEA projection\n",
    "    transformer = pyproj.Transformer.from_crs(\n",
    "        pyproj.CRS(\"EPSG:4326\"),  # WGS 84 (latitude and longitude)\n",
    "        pyproj.CRS(custom_projection),  # Custom LAEA projection\n",
    "        always_xy=True\n",
    "    )\n",
    "\n",
    "    # Define a function to transform coordinates using the transformer\n",
    "    def transform_coordinates(x, y):\n",
    "        return transformer.transform(x, y)\n",
    "\n",
    "    # Convert the coordinates to the custom LAEA projection\n",
    "    polygon_laea = transform(transform_coordinates, polygon)\n",
    "\n",
    "    # Calculate the area in square meters\n",
    "    area_sqm = polygon_laea.area\n",
    "\n",
    "    # Convert the area to square feet (1 square meter = 10.764 square feet)\n",
    "    area_sqft = area_sqm * 10.764\n",
    "\n",
    "    return area_sqft\n",
    "\n",
    "\n",
    "\n",
    "### Call Function and add to dataframe\n",
    "df_cleaned['foul_area_sqft'] = df_cleaned['foul'].apply(calculate_area)\n",
    "df_cleaned['fop_area_sqft'] = df_cleaned['fop'].apply(calculate_area)\n",
    "\n",
    "## Calculate the total area of the field and the ratio of foul area to field area\n",
    "df_cleaned['field_area_sqft'] = df_cleaned['foul_area_sqft'] + df_cleaned['fop_area_sqft']\n",
    "## Percentage foul area\n",
    "df_cleaned['foul_area_per'] = df_cleaned['foul_area_sqft'] / df_cleaned['field_area_sqft']\n",
    "## Fair to Foul Ratio\n",
    "df_cleaned['fair_to_foul'] = df_cleaned['fop_area_sqft'] / df_cleaned['foul_area_sqft']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# FENCE DISTANCE CALCULATION #############\n",
    "\n",
    "from geopy.distance import great_circle\n",
    "import numpy as np\n",
    "\n",
    "def interpolate_points(start, end, length_ratio):\n",
    "    start_np = np.array(start)\n",
    "    end_np = np.array(end)\n",
    "    return tuple(start_np + (end_np - start_np) * length_ratio)\n",
    "\n",
    "def calculate_distances(home_plate, outfield_coords, num_points=90):\n",
    "    def is_same_point(point1, point2, tolerance=1e-6):\n",
    "        return abs(point1[0] - point2[0]) < tolerance and abs(point1[1] - point2[1]) < tolerance\n",
    "\n",
    "    home_plate_lat_lon = (home_plate[1], home_plate[0])\n",
    "    distances = []\n",
    "\n",
    "    # Calculate total line length\n",
    "    total_length = 0\n",
    "    segments = []\n",
    "    for i in range(len(outfield_coords) - 1):\n",
    "        start = outfield_coords[i]\n",
    "        end = outfield_coords[i + 1]\n",
    "        if not is_same_point(home_plate, start) and not is_same_point(home_plate, end):\n",
    "            segment_length = great_circle((start[1], start[0]), (end[1], end[0])).feet\n",
    "            segments.append((start, end, segment_length))\n",
    "            total_length += segment_length\n",
    "\n",
    "    # Calculate the distance between equally spaced points\n",
    "    spacing = total_length / (num_points - 1)\n",
    "\n",
    "    # Interpolate points and calculate distances\n",
    "    current_length = 0\n",
    "    segment_index = 0\n",
    "    for i in range(num_points):\n",
    "        while segment_index < len(segments) - 1 and current_length > segments[segment_index][2]:\n",
    "            current_length -= segments[segment_index][2]\n",
    "            segment_index += 1\n",
    "\n",
    "        start, end, segment_length = segments[segment_index]\n",
    "        length_ratio = current_length / segment_length\n",
    "        point = interpolate_points(start, end, length_ratio)\n",
    "        distance = round(great_circle(home_plate_lat_lon, (point[1], point[0])).feet)\n",
    "        distances.append(distance)\n",
    "\n",
    "        current_length += spacing\n",
    "\n",
    "    return distances\n",
    "\n",
    "# Calculate distances for each row\n",
    "df_cleaned['distances'] = df_cleaned.apply(lambda row: calculate_distances(row['home_plate'], row['fop']), axis=1)\n",
    "\n",
    "# Calculate max, min, and average distances for each row\n",
    "df_cleaned['max_distance'] = df_cleaned['distances'].apply(max)\n",
    "df_cleaned['min_distance'] = df_cleaned['distances'].apply(min)\n",
    "df_cleaned['avg_distance'] = df_cleaned['distances'].apply(lambda distances: sum(distances) / len(distances))\n",
    "# get the median distance\n",
    "df_cleaned['median_distance'] = df_cleaned['distances'].apply(lambda distances: np.median(distances))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90    142\n",
       "Name: num_distances, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## CHECK BLOCK ########\n",
    "\n",
    "## Check how long the distance list is for each row\n",
    "df_cleaned['num_distances'] = df_cleaned['distances'].apply(len)\n",
    "\n",
    "## Print the value counts for the 'num_distances' column\n",
    "df_cleaned['num_distances'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### NOT NECESSARY FOR THIS PROJECT ##########\n",
    "\n",
    "# ### Get Geolocation of each field based on home plate coordinates and return state and country\n",
    "# ### This block takes a long time to run - will need to revisit\n",
    "# ## up to ten minutes\n",
    "\n",
    "# from geopy.geocoders import Nominatim\n",
    "# from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# geolocator = Nominatim(user_agent=\"baseball_field_locator\")\n",
    "\n",
    "# import time\n",
    "\n",
    "# def get_location_info(lng, lat):\n",
    "#     try:\n",
    "#         time.sleep(1)  # Delay for 1 second\n",
    "#         location = geolocator.reverse((lat, lng), timeout=10)\n",
    "#         city = location.raw['address'].get('city', None)\n",
    "#         state = location.raw['address'].get('state', None)\n",
    "#         return city, state\n",
    "#     except GeocoderTimedOut:\n",
    "#         print(f\"GeocoderTimedOut error for coordinates: ({lng}, {lat})\")\n",
    "#         return None, None\n",
    "#     except GeocoderServiceError:\n",
    "#         print(f\"GeocoderServiceError for coordinates: ({lng}, {lat})\")\n",
    "#         return None, None\n",
    "\n",
    "\n",
    "# # Extract the first coordinate for each field\n",
    "# df_cleaned['lng'], df_cleaned['lat'] = zip(*df_cleaned['home_plate'].apply(lambda x: x))\n",
    "\n",
    "# # Wrap the DataFrame apply function with tqdm for progress indication\n",
    "# tqdm.pandas(desc=\"Processing coordinates\")\n",
    "\n",
    "# # Get state and country information for each field\n",
    "# df_cleaned[['city', 'state']] = df_cleaned.progress_apply(lambda row: get_location_info(row['lng'], row['lat']), axis=1, result_type='expand')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create ranks for each column\n",
    "\n",
    "def rank_fields(df):\n",
    "    # Calculate the rank for each category\n",
    "    df['max_distance_rank'] = df['max_distance'].rank(ascending=False, method='min')\n",
    "    df['min_distance_rank'] = df['min_distance'].rank(ascending=False, method='min')\n",
    "    df['avg_distance_rank'] = df['avg_distance'].rank(ascending=False, method='min')\n",
    "    df['median_distance_rank'] = df['median_distance'].rank(ascending=False, method='min')\n",
    "    df['field_area_rank'] = df['field_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['foul_area_rank'] = df['foul_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['fop_area_per_rank'] = df['fop_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['ratio_rank'] = df['fair_to_foul'].rank(ascending=False, method='min')\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Function\n",
    "\n",
    "df_cleaned = rank_fields(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Orienting the map to the home plate location ####\n",
    "\n",
    "### Find the center of the field\n",
    "def calculate_centroid(coords):\n",
    "    x_coords = [coord[0] for coord in coords]\n",
    "    y_coords = [coord[1] for coord in coords]\n",
    "    centroid_x = sum(x_coords) / len(coords)\n",
    "    centroid_y = sum(y_coords) / len(coords)\n",
    "    return (centroid_x, centroid_y)\n",
    "\n",
    "\n",
    "## Find the bearing between the home plate and the center of the field\n",
    "import math\n",
    "\n",
    "def calculate_bearing(point1, point2):\n",
    "    lat1, lon1 = math.radians(point1[1]), math.radians(point1[0])\n",
    "    lat2, lon2 = math.radians(point2[1]), math.radians(point2[0])\n",
    "\n",
    "    d_lon = lon2 - lon1\n",
    "\n",
    "    x = math.cos(lat2) * math.sin(d_lon)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(d_lon)\n",
    "\n",
    "    bearing = math.degrees(math.atan2(x, y))\n",
    "    bearing = (bearing + 360) % 360  # Normalize the bearing to the range [0, 360)\n",
    "\n",
    "    return bearing\n",
    "\n",
    "### Function to classify direction in laymans terms North, South, East, West, ect\n",
    "def degrees_to_cardinal_direction(degrees):\n",
    "    directions = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW', 'N']\n",
    "    index = round(degrees / 45)\n",
    "    return directions[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the centroid of the outfield fence coordinates for each row\n",
    "df_cleaned['fop_centroid'] = df_cleaned['fop'].apply(lambda coords: calculate_centroid(coords[1:]))\n",
    "\n",
    "# Calculate the bearing between home plate and the centroid for each row\n",
    "df_cleaned['field_orientation'] = df_cleaned.apply(lambda row: calculate_bearing(row['home_plate'], row['fop_centroid']), axis=1)\n",
    "\n",
    "# Convert the bearing to a cardinal direction\n",
    "df_cleaned['field_cardinal_direction'] = df_cleaned['field_orientation'].apply(degrees_to_cardinal_direction)\n",
    "\n",
    "# rename 'field' to 'park_name'\n",
    "df_cleaned.rename(columns={'field': 'park_name'}, inplace=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the geo transformation should take place above this\n",
    "\n",
    "## starting the process of matching in data from other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output to csv \n",
    "df_cleaned.to_csv('data/fields_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned.info()\n",
    "\n",
    "# # Load the host team info with nickname and team colors\n",
    "path = 'data/2023_MHSAA_sites.csv'\n",
    "df_hosts = pd.read_csv(path)\n",
    "\n",
    "df_parks = df_cleaned\n",
    "\n",
    "df_hosts.head()\n",
    "\n",
    "park_df = df_parks\n",
    "host_df = df_hosts\n",
    "# df_hosts.info()\n",
    "\n",
    "# # Merge the host team info with the field info\n",
    "# df_cleaned = df_cleaned.merge(df_hosts, on='host_team', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 146 entries, 0 to 145\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   park_name             145 non-null    object \n",
      " 1   host_team             144 non-null    object \n",
      " 2   division              142 non-null    float64\n",
      " 3   district              128 non-null    float64\n",
      " 4   region_semi_number    64 non-null     float64\n",
      " 5   regional_div          78 non-null     float64\n",
      " 6   region_final_quarter  16 non-null     float64\n",
      " 7   finals                1 non-null      float64\n",
      " 8   nickname              143 non-null    object \n",
      " 9   color1                143 non-null    object \n",
      " 10  color2                143 non-null    object \n",
      " 11  color3                14 non-null     object \n",
      "dtypes: float64(6), object(6)\n",
      "memory usage: 13.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 142 entries, 0 to 145\n",
      "Data columns (total 27 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   park_name                 142 non-null    object \n",
      " 1   foul                      142 non-null    object \n",
      " 2   fop                       142 non-null    object \n",
      " 3   notes                     7 non-null      object \n",
      " 4   home_plate                142 non-null    object \n",
      " 5   foul_area_sqft            142 non-null    float64\n",
      " 6   fop_area_sqft             142 non-null    float64\n",
      " 7   field_area_sqft           142 non-null    float64\n",
      " 8   foul_area_per             142 non-null    float64\n",
      " 9   fair_to_foul              142 non-null    float64\n",
      " 10  distances                 142 non-null    object \n",
      " 11  max_distance              142 non-null    int64  \n",
      " 12  min_distance              142 non-null    int64  \n",
      " 13  avg_distance              142 non-null    float64\n",
      " 14  median_distance           142 non-null    float64\n",
      " 15  num_distances             142 non-null    int64  \n",
      " 16  max_distance_rank         142 non-null    float64\n",
      " 17  min_distance_rank         142 non-null    float64\n",
      " 18  avg_distance_rank         142 non-null    float64\n",
      " 19  median_distance_rank      142 non-null    float64\n",
      " 20  field_area_rank           142 non-null    float64\n",
      " 21  foul_area_rank            142 non-null    float64\n",
      " 22  fop_area_per_rank         142 non-null    float64\n",
      " 23  ratio_rank                142 non-null    float64\n",
      " 24  fop_centroid              142 non-null    object \n",
      " 25  field_orientation         142 non-null    float64\n",
      " 26  field_cardinal_direction  142 non-null    object \n",
      "dtypes: float64(16), int64(3), object(8)\n",
      "memory usage: 31.1+ KB\n"
     ]
    }
   ],
   "source": [
    "host_df.info()\n",
    "park_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple Merge, should work because the park_name columns should match exactly\n",
    "## Do not detroy any data\n",
    "\n",
    "df_merged = host_df.merge(park_df, on='park_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 146 entries, 0 to 145\n",
      "Data columns (total 38 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   park_name                 145 non-null    object \n",
      " 1   host_team                 144 non-null    object \n",
      " 2   division                  142 non-null    float64\n",
      " 3   district                  128 non-null    float64\n",
      " 4   region_semi_number        64 non-null     float64\n",
      " 5   regional_div              78 non-null     float64\n",
      " 6   region_final_quarter      16 non-null     float64\n",
      " 7   finals                    1 non-null      float64\n",
      " 8   nickname                  143 non-null    object \n",
      " 9   color1                    143 non-null    object \n",
      " 10  color2                    143 non-null    object \n",
      " 11  color3                    14 non-null     object \n",
      " 12  foul                      142 non-null    object \n",
      " 13  fop                       142 non-null    object \n",
      " 14  notes                     7 non-null      object \n",
      " 15  home_plate                142 non-null    object \n",
      " 16  foul_area_sqft            142 non-null    float64\n",
      " 17  fop_area_sqft             142 non-null    float64\n",
      " 18  field_area_sqft           142 non-null    float64\n",
      " 19  foul_area_per             142 non-null    float64\n",
      " 20  fair_to_foul              142 non-null    float64\n",
      " 21  distances                 142 non-null    object \n",
      " 22  max_distance              142 non-null    float64\n",
      " 23  min_distance              142 non-null    float64\n",
      " 24  avg_distance              142 non-null    float64\n",
      " 25  median_distance           142 non-null    float64\n",
      " 26  num_distances             142 non-null    float64\n",
      " 27  max_distance_rank         142 non-null    float64\n",
      " 28  min_distance_rank         142 non-null    float64\n",
      " 29  avg_distance_rank         142 non-null    float64\n",
      " 30  median_distance_rank      142 non-null    float64\n",
      " 31  field_area_rank           142 non-null    float64\n",
      " 32  foul_area_rank            142 non-null    float64\n",
      " 33  fop_area_per_rank         142 non-null    float64\n",
      " 34  ratio_rank                142 non-null    float64\n",
      " 35  fop_centroid              142 non-null    object \n",
      " 36  field_orientation         142 non-null    float64\n",
      " 37  field_cardinal_direction  142 non-null    object \n",
      "dtypes: float64(25), object(13)\n",
      "memory usage: 44.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### FUZZY MATCH FUNCTION - NO LONGER NEEDED #########\n",
    "\n",
    "# import pandas as pd\n",
    "# from fuzzywuzzy import fuzz\n",
    "# from fuzzywuzzy import process\n",
    "\n",
    "# # Assuming you have two dataframes named 'park_df' and 'host_df'\n",
    "\n",
    "# # Function to perform fuzzy matching\n",
    "# def fuzzy_match(row):\n",
    "#     team = row['team']\n",
    "#     if pd.isnull(team):\n",
    "#         return None\n",
    "#     park_names = park_df['park_name']\n",
    "#     best_match = process.extractOne(team, park_names, scorer=fuzz.ratio)\n",
    "#     if best_match[1] >= 80:  # Specify a threshold for matching accuracy\n",
    "#         return best_match[0]\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# # Add a new column to 'host_df' with the best fuzzy matched park_name\n",
    "# host_df['fuzzy_matched_park_name'] = host_df.apply(fuzzy_match, axis=1)\n",
    "\n",
    "# # Merge the dataframes based on the fuzzy matched park_name and team columns\n",
    "# merged_df = park_df.merge(host_df, left_on='park_name', right_on='fuzzy_matched_park_name', how='left')\n",
    "\n",
    "# # Print the merged dataframe\n",
    "# print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 146 entries, 0 to 145\n",
      "Data columns (total 38 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   park_name                 145 non-null    object \n",
      " 1   host_team                 144 non-null    object \n",
      " 2   division                  142 non-null    float64\n",
      " 3   district                  128 non-null    float64\n",
      " 4   region_semi_number        64 non-null     float64\n",
      " 5   regional_div              78 non-null     float64\n",
      " 6   region_final_quarter      16 non-null     float64\n",
      " 7   finals                    1 non-null      float64\n",
      " 8   nickname                  143 non-null    object \n",
      " 9   color1                    143 non-null    object \n",
      " 10  color2                    143 non-null    object \n",
      " 11  color3                    14 non-null     object \n",
      " 12  foul                      142 non-null    object \n",
      " 13  fop                       142 non-null    object \n",
      " 14  notes                     7 non-null      object \n",
      " 15  home_plate                142 non-null    object \n",
      " 16  foul_area_sqft            142 non-null    float64\n",
      " 17  fop_area_sqft             142 non-null    float64\n",
      " 18  field_area_sqft           142 non-null    float64\n",
      " 19  foul_area_per             142 non-null    float64\n",
      " 20  fair_to_foul              142 non-null    float64\n",
      " 21  distances                 142 non-null    object \n",
      " 22  max_distance              142 non-null    float64\n",
      " 23  min_distance              142 non-null    float64\n",
      " 24  avg_distance              142 non-null    float64\n",
      " 25  median_distance           142 non-null    float64\n",
      " 26  num_distances             142 non-null    float64\n",
      " 27  max_distance_rank         142 non-null    float64\n",
      " 28  min_distance_rank         142 non-null    float64\n",
      " 29  avg_distance_rank         142 non-null    float64\n",
      " 30  median_distance_rank      142 non-null    float64\n",
      " 31  field_area_rank           142 non-null    float64\n",
      " 32  foul_area_rank            142 non-null    float64\n",
      " 33  fop_area_per_rank         142 non-null    float64\n",
      " 34  ratio_rank                142 non-null    float64\n",
      " 35  fop_centroid              142 non-null    object \n",
      " 36  field_orientation         142 non-null    float64\n",
      " 37  field_cardinal_direction  142 non-null    object \n",
      "dtypes: float64(25), object(13)\n",
      "memory usage: 44.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_merged.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop the rows with null values in the 'fop', 'foul' or 'home_plate' columns\n",
    "df_merged.dropna(subset=['fop', 'foul', 'home_plate'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 142 entries, 0 to 145\n",
      "Data columns (total 38 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   park_name                 142 non-null    object \n",
      " 1   host_team                 140 non-null    object \n",
      " 2   division                  138 non-null    float64\n",
      " 3   district                  125 non-null    float64\n",
      " 4   region_semi_number        63 non-null     float64\n",
      " 5   regional_div              76 non-null     float64\n",
      " 6   region_final_quarter      15 non-null     float64\n",
      " 7   finals                    1 non-null      float64\n",
      " 8   nickname                  139 non-null    object \n",
      " 9   color1                    139 non-null    object \n",
      " 10  color2                    139 non-null    object \n",
      " 11  color3                    13 non-null     object \n",
      " 12  foul                      142 non-null    object \n",
      " 13  fop                       142 non-null    object \n",
      " 14  notes                     7 non-null      object \n",
      " 15  home_plate                142 non-null    object \n",
      " 16  foul_area_sqft            142 non-null    float64\n",
      " 17  fop_area_sqft             142 non-null    float64\n",
      " 18  field_area_sqft           142 non-null    float64\n",
      " 19  foul_area_per             142 non-null    float64\n",
      " 20  fair_to_foul              142 non-null    float64\n",
      " 21  distances                 142 non-null    object \n",
      " 22  max_distance              142 non-null    float64\n",
      " 23  min_distance              142 non-null    float64\n",
      " 24  avg_distance              142 non-null    float64\n",
      " 25  median_distance           142 non-null    float64\n",
      " 26  num_distances             142 non-null    float64\n",
      " 27  max_distance_rank         142 non-null    float64\n",
      " 28  min_distance_rank         142 non-null    float64\n",
      " 29  avg_distance_rank         142 non-null    float64\n",
      " 30  median_distance_rank      142 non-null    float64\n",
      " 31  field_area_rank           142 non-null    float64\n",
      " 32  foul_area_rank            142 non-null    float64\n",
      " 33  fop_area_per_rank         142 non-null    float64\n",
      " 34  ratio_rank                142 non-null    float64\n",
      " 35  fop_centroid              142 non-null    object \n",
      " 36  field_orientation         142 non-null    float64\n",
      " 37  field_cardinal_direction  142 non-null    object \n",
      "dtypes: float64(25), object(13)\n",
      "memory usage: 43.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_merged.info()\n",
    "df = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              park_name      score\n",
      "115       Central Lake HS - high_school   8.235868\n",
      "84                              Hart HS   8.185211\n",
      "136                         Kingston HS   7.787572\n",
      "129                           Martin HS   7.206732\n",
      "132            East Jackson High School   6.653831\n",
      "..                                  ...        ...\n",
      "5           Kalamazoo College - college  -8.547677\n",
      "142  Michigan State - Old College Field  -8.573670\n",
      "106                   Detroit Osborn HS -13.339288\n",
      "107                  Laura F. Osborn HS -13.339288\n",
      "21     Lowell High School - high school -16.047450\n",
      "\n",
      "[142 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "### THIS BLOCK CREATES THE RANKING OF PITCHER VS HITTER FRIENDLY FIELDS\n",
    "def rank_fields(data):\n",
    "    # Define weights for each parameter\n",
    "    weights = {\n",
    "        'max_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'min_distance': 1,  # positive weight since shorter fences favor hitters\n",
    "        'avg_distance': -.75, # negative weight since longer fences favor pitchers\n",
    "        'median_distance': -.75, # negative weight since longer fences favor pitchers\n",
    "        'field_area_sqft': -1,  # negative weight since larger fields favor pitchers\n",
    "        'fair_to_foul': -1,  # negative weight since larger ratio (more foul territory) favors pitchers\n",
    "        'foul_area_sqft': -1, # negative weight since larger foul area favors pitchers\n",
    "        'fop_area_sqft': -1, # negative weight since larger out of play area favors pitchers\n",
    "    }\n",
    "\n",
    "    # Standardize features (subtract mean and divide by standard deviation)\n",
    "    standardized_data = data.copy()\n",
    "    for column in weights.keys():\n",
    "        standardized_data[column] = (standardized_data[column] - standardized_data[column].mean()) / standardized_data[column].std()\n",
    "\n",
    "    # Calculate score for each field\n",
    "    standardized_data['score'] = standardized_data.apply(lambda row: sum(row[param] * weight for param, weight in weights.items()), axis=1)\n",
    "\n",
    "    # Save scores to original dataframe\n",
    "    data['score'] = standardized_data['score']\n",
    "\n",
    "    # Rank fields based on score (higher scores are more hitter-friendly)\n",
    "    ranked_fields = data.sort_values('score', ascending=False)\n",
    "\n",
    "    return ranked_fields\n",
    "\n",
    "# Suppose 'df' is your DataFrame containing the field data\n",
    "ranked_fields = rank_fields(df)\n",
    "print(ranked_fields[['park_name', 'score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 142 entries, 115 to 21\n",
      "Data columns (total 39 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   park_name                 142 non-null    object \n",
      " 1   host_team                 140 non-null    object \n",
      " 2   division                  138 non-null    float64\n",
      " 3   district                  125 non-null    float64\n",
      " 4   region_semi_number        63 non-null     float64\n",
      " 5   regional_div              76 non-null     float64\n",
      " 6   region_final_quarter      15 non-null     float64\n",
      " 7   finals                    1 non-null      float64\n",
      " 8   nickname                  139 non-null    object \n",
      " 9   color1                    139 non-null    object \n",
      " 10  color2                    139 non-null    object \n",
      " 11  color3                    13 non-null     object \n",
      " 12  foul                      142 non-null    object \n",
      " 13  fop                       142 non-null    object \n",
      " 14  notes                     7 non-null      object \n",
      " 15  home_plate                142 non-null    object \n",
      " 16  foul_area_sqft            142 non-null    float64\n",
      " 17  fop_area_sqft             142 non-null    float64\n",
      " 18  field_area_sqft           142 non-null    float64\n",
      " 19  foul_area_per             142 non-null    float64\n",
      " 20  fair_to_foul              142 non-null    float64\n",
      " 21  distances                 142 non-null    object \n",
      " 22  max_distance              142 non-null    float64\n",
      " 23  min_distance              142 non-null    float64\n",
      " 24  avg_distance              142 non-null    float64\n",
      " 25  median_distance           142 non-null    float64\n",
      " 26  num_distances             142 non-null    float64\n",
      " 27  max_distance_rank         142 non-null    float64\n",
      " 28  min_distance_rank         142 non-null    float64\n",
      " 29  avg_distance_rank         142 non-null    float64\n",
      " 30  median_distance_rank      142 non-null    float64\n",
      " 31  field_area_rank           142 non-null    float64\n",
      " 32  foul_area_rank            142 non-null    float64\n",
      " 33  fop_area_per_rank         142 non-null    float64\n",
      " 34  ratio_rank                142 non-null    float64\n",
      " 35  fop_centroid              142 non-null    object \n",
      " 36  field_orientation         142 non-null    float64\n",
      " 37  field_cardinal_direction  142 non-null    object \n",
      " 38  score                     142 non-null    float64\n",
      "dtypes: float64(26), object(13)\n",
      "memory usage: 44.4+ KB\n"
     ]
    }
   ],
   "source": [
    "ranked_fields.info()\n",
    "merged_df = ranked_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### outpus csv to check\n",
    "df_merged.to_csv('data/2023_merged_test.csv', index=False)\n",
    "\n",
    "### OUTPUT JSON TO USE IN MAP\n",
    "df_merged.to_json('data/html/mhsaa/data/map.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 142 entries, 0 to 145\n",
      "Data columns (total 39 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   park_name                 142 non-null    object \n",
      " 1   host_team                 140 non-null    object \n",
      " 2   division                  138 non-null    float64\n",
      " 3   district                  125 non-null    float64\n",
      " 4   region_semi_number        63 non-null     float64\n",
      " 5   regional_div              76 non-null     float64\n",
      " 6   region_final_quarter      15 non-null     float64\n",
      " 7   finals                    1 non-null      float64\n",
      " 8   nickname                  139 non-null    object \n",
      " 9   color1                    139 non-null    object \n",
      " 10  color2                    139 non-null    object \n",
      " 11  color3                    13 non-null     object \n",
      " 12  foul                      142 non-null    object \n",
      " 13  fop                       142 non-null    object \n",
      " 14  notes                     7 non-null      object \n",
      " 15  home_plate                142 non-null    object \n",
      " 16  foul_area_sqft            142 non-null    float64\n",
      " 17  fop_area_sqft             142 non-null    float64\n",
      " 18  field_area_sqft           142 non-null    float64\n",
      " 19  foul_area_per             142 non-null    float64\n",
      " 20  fair_to_foul              142 non-null    float64\n",
      " 21  distances                 142 non-null    object \n",
      " 22  max_distance              142 non-null    float64\n",
      " 23  min_distance              142 non-null    float64\n",
      " 24  avg_distance              142 non-null    float64\n",
      " 25  median_distance           142 non-null    float64\n",
      " 26  num_distances             142 non-null    float64\n",
      " 27  max_distance_rank         142 non-null    float64\n",
      " 28  min_distance_rank         142 non-null    float64\n",
      " 29  avg_distance_rank         142 non-null    float64\n",
      " 30  median_distance_rank      142 non-null    float64\n",
      " 31  field_area_rank           142 non-null    float64\n",
      " 32  foul_area_rank            142 non-null    float64\n",
      " 33  fop_area_per_rank         142 non-null    float64\n",
      " 34  ratio_rank                142 non-null    float64\n",
      " 35  fop_centroid              142 non-null    object \n",
      " 36  field_orientation         142 non-null    float64\n",
      " 37  field_cardinal_direction  142 non-null    object \n",
      " 38  score                     142 non-null    float64\n",
      "dtypes: float64(26), object(13)\n",
      "memory usage: 48.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parks.shape\n",
    "\n",
    "df_parks.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END BLOCK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Do a FUZZY MATCH OF DF_HOSTS AND DF_PARKS\n",
    "# # Debugging step to check for non-string values in host_teams\n",
    "# for team in host_teams:\n",
    "#     if not isinstance(team, str):\n",
    "#         print(f\"Non-string value found in host_teams: {team}\")\n",
    "\n",
    "# # Debugging step to check for non-string values in park_names\n",
    "# for park in park_names:\n",
    "#     if not isinstance(park, str):\n",
    "#         print(f\"Non-string value found in park_names: {park}\")\n",
    "\n",
    "# # Continue with fuzzy matching if no non-string values are found\n",
    "# matches = [(team, process.extractOne(team, park_names)) for team in host_teams]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## List the values from the team column\n",
    "\n",
    "# print(len(df_hosts['team'].unique()))\n",
    "# df_hosts['team'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parks = df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_df = df_cleaned.copy()\n",
    "\n",
    "parks_df.info()\n",
    "host_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(parks_df, host_df, min_score=90):\n",
    "    dict_list = []\n",
    "    unmatched_rows = []\n",
    "    # Drop NaN values in 'team' and 'park_name' before matching\n",
    "    host_names = host_df.team.dropna().unique()\n",
    "    for name in parks_df.park_name.dropna():  # ignore NaN values\n",
    "        match = match_team(name, host_names, min_score)\n",
    "        \n",
    "        # If no match found, add to unmatched_rows and continue to next iteration\n",
    "        if match[0] == \"\":\n",
    "            unmatched_rows.append(name)\n",
    "            continue\n",
    "\n",
    "        dict_ = {}\n",
    "        dict_.update({\"park_name_parks\" : name})\n",
    "        dict_.update({\"match_name_host\" : match[0]})\n",
    "        dict_.update({\"score\" : match[1]})\n",
    "        dict_list.append(dict_)\n",
    "\n",
    "    merge_table = pd.DataFrame(dict_list)\n",
    "    \n",
    "    # Remove duplicates in merge_table, keeping only the row with the highest score\n",
    "    merge_table = merge_table.sort_values('score', ascending=False).drop_duplicates(['park_name_parks'], keep='first')\n",
    "\n",
    "    if 'match_name_host' in merge_table.columns:\n",
    "        merged_df = pd.merge(parks_df, merge_table, left_on='park_name', right_on='park_name_parks', how='left')\n",
    "        merged_df = pd.merge(merged_df, host_df, left_on='match_name_host', right_on='team', how='left')\n",
    "    else:\n",
    "        print(\"No matches found.\")\n",
    "        merged_df = None\n",
    "\n",
    "    return merged_df, unmatched_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = merged_df.sort_values('score', ascending=False).drop_duplicates(subset=['park_name', 'team'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df, unmatched_rows = merge_dataframes(parks_df, host_df, min_score=90)\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we first match each team name in host_df with the park names in parks_df. If a match with a similarity score greater than the threshold (85 in your case) is found, we record the match in merge_table. If no match is found, we record the team name in unmatched_rows. After going through all team names, we merge host_df and parks_df based on the matches in merge_table.\n",
    "\n",
    "The function merge_dataframes returns two objects. The first object, merged_df, is a DataFrame that contains the merged data. The second object, unmatched_rows, is a list of team names in host_df for which no match in parks_df could be found. You can inspect unmatched_rows to see which rows couldn't be matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(parks_df, host_df, min_score=90):\n",
    "    dict_list = []\n",
    "    unmatched_rows = []\n",
    "    # Drop NaN values in 'team' and 'park_name' before matching\n",
    "    host_names = host_df.team.dropna().unique()\n",
    "    for name in parks_df.park_name.dropna():  # ignore NaN values\n",
    "        match = match_team(name, host_names, min_score)\n",
    "        \n",
    "        # If no match found, add to unmatched_rows and continue to next iteration\n",
    "        if match[0] == \"\":\n",
    "            unmatched_rows.append(name)\n",
    "            continue\n",
    "\n",
    "        dict_ = {}\n",
    "        dict_.update({\"park_name_parks\" : name})\n",
    "        dict_.update({\"match_name_host\" : match[0]})\n",
    "        dict_.update({\"score\" : match[1]})\n",
    "        dict_list.append(dict_)\n",
    "\n",
    "    merge_table = pd.DataFrame(dict_list)\n",
    "    \n",
    "    # Remove duplicates in merge_table, keeping only the row with the highest score\n",
    "    merge_table = merge_table.sort_values('score', ascending=False).drop_duplicates(['park_name_parks'], keep='first')\n",
    "\n",
    "    if 'match_name_host' in merge_table.columns:\n",
    "        merged_df = pd.merge(parks_df, merge_table, left_on='park_name', right_on='park_name_parks', how='left')\n",
    "        merged_df = pd.merge(merged_df, host_df, left_on='match_name_host', right_on='team', how='left')\n",
    "    else:\n",
    "        print(\"No matches found.\")\n",
    "        merged_df = None\n",
    "\n",
    "    return merged_df, unmatched_rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORKING HERE DOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matching Function to compair host names to park names \n",
    "\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "def find_host_matches(host_df, parks_df):\n",
    "    # Initialize empty lists to store the matches and unmatched park names\n",
    "    matches = []\n",
    "    unmatched_park_names = []\n",
    "\n",
    "    # Iterate over each host and district in the host_df\n",
    "    for host, district in zip(host_df['team'], host_df['district']):\n",
    "        # Use fuzzy matching to find potential matches in park names\n",
    "        potential_matches = process.extractBests(host, parks_df['park_name'], scorer=fuzz.token_set_ratio, score_cutoff=80)\n",
    "\n",
    "        # Store the host, district, and potential matches with their scores\n",
    "        matches.append({'host': host, 'district': district, 'potential_matches': potential_matches})\n",
    "\n",
    "        # Check if any strong matches were found\n",
    "        if len(potential_matches) > 0:\n",
    "            max_score = max(potential_matches, key=lambda x: x[1])[1]\n",
    "            if max_score >= 80:\n",
    "                continue\n",
    "\n",
    "        # If no strong matches were found, add the park name to unmatched list\n",
    "        unmatched_park_names.append((host, district))\n",
    "\n",
    "    # Create a dataframe from the matches list\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "\n",
    "    # Count the number of strong matches and unmatched park names\n",
    "    strong_matches_count = matches_df['potential_matches'].apply(lambda x: sum(match[1] >= 80 for match in x)).sum()\n",
    "    unmatched_count = len(unmatched_park_names)\n",
    "\n",
    "    print(\"Number of strong matches:\", strong_matches_count)\n",
    "    print(\"Number of unmatched park names:\", unmatched_count)\n",
    "    print(\"Unmatched park names:\", unmatched_park_names)\n",
    "\n",
    "    return matches_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_df.info()\n",
    "parks_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to find host matches\n",
    "result_df = find_host_matches(host_df, parks_df)\n",
    "\n",
    "# # Merge the result_df to messe_df on district number\n",
    "# merged_df = result_df.merge(messe_df, left_on='district', right_on='district number')\n",
    "\n",
    "# # Select the desired column'region_semi_numbers\n",
    "# merged_df = merged_df[['host', 'division', 'district', 'region_semi_number', 'region_final_quarter', 'finals', 'potential_matches', 'Plot Note', 'Map Link MHSAA']]\n",
    "# merged_df.head()\n",
    "# # merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Keep only the field name from the potential matches, ignore if the potential match is empty\n",
    "merged_df['potential_matches'] = merged_df['potential_matches'].apply(lambda x: x[0][0] if len(x) > 0 else np.nan)\n",
    "\n",
    "\n",
    "# merged_df['potential_matches'] = merged_df['potential_matches'].apply(lambda x: x[0][0])\n",
    "\n",
    "# Rename the potential_matches column to park_name\n",
    "\n",
    "merged_df.rename(columns={'potential_matches': 'park_name'}, inplace=True)\n",
    "\n",
    "\n",
    "merged_df.head()\n",
    "merged_df.info()\n",
    "# # Merge the merged_df to the parks_df on park_name\n",
    "\n",
    "# parks_df = parks_df.merge(merged_df, on='park_name', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge the merged_df to the parks_df on park_name\n",
    "parks_df = parks_df.merge(merged_df, on='park_name', how='left')\n",
    "\n",
    "parks_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_df.info()\n",
    "parks_df.head()\n",
    "\n",
    "# host_df.info()\n",
    "# host_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here down are simple plots to do spot check of data and hold example of polar chart\n",
    "\n",
    "### FILL IN THE REST OF JSON WITH THE DATA FOR THE 2023 TOURNEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pull the host team information into the parks_df\n",
    "parks_df = parks_df.merge(host_df, on='host', how='left')\n",
    "\n",
    "### Create columns for tournament levels\n",
    "# District is done, Need regional semi, regional final, quarter final, final_four\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parks_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Build the file path using os.path.join\n",
    "file_path = os.path.join('data', 'html', 'mhsaa', 'data', 'tourney_2023.json')\n",
    "\n",
    "# Save the dataframe to JSON using the constructed file path\n",
    "parks_df.to_json(file_path, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the max distance, min distance, average distance, and median distance\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "ax[0, 0].hist(df_cleaned['max_distance'], bins=20)\n",
    "\n",
    "ax[0, 1].hist(df_cleaned['min_distance'], bins=20)\n",
    "\n",
    "ax[1, 0].hist(df_cleaned['avg_distance'], bins=20)\n",
    "\n",
    "ax[1, 1].hist(df_cleaned['median_distance'], bins=20)\n",
    "\n",
    "ax[0, 0].set_title('Max Distance')\n",
    "ax[0, 1].set_title('Min Distance')\n",
    "\n",
    "ax[1, 0].set_title('Average Distance')\n",
    "ax[1, 1].set_title('Median Distance')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile a list of fields that are outliers\n",
    "\n",
    "outlier_fields = df_cleaned[(df_cleaned['max_distance'] > 400) | (df_cleaned['min_distance'] < 200) | (df_cleaned['avg_distance'] > 400) | (df_cleaned['median_distance'] > 400)]\n",
    "\n",
    "len(outlier_fields)\n",
    "\n",
    "print(outlier_fields['park_name'].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of the top and bottom ten from each category\n",
    "\n",
    "top_ten_max = df_cleaned.sort_values(by='max_distance', ascending=False).head(10)\n",
    "top_ten_min = df_cleaned.sort_values(by='min_distance', ascending=True).head(10)\n",
    "\n",
    "top_ten_avg = df_cleaned.sort_values(by='avg_distance', ascending=False).head(10)\n",
    "top_ten_median = df_cleaned.sort_values(by='median_distance', ascending=False).head(10)\n",
    "\n",
    "top_ten_field_area = df_cleaned.sort_values(by='field_area_sqft', ascending=False).head(10)\n",
    "\n",
    "top_ten_foul_area = df_cleaned.sort_values(by='foul_area_sqft', ascending=False).head(10)\n",
    "\n",
    "top_ten_fop_area = df_cleaned.sort_values(by='fop_area_sqft', ascending=False).head(10)\n",
    "\n",
    "top_ten_ratio = df_cleaned.sort_values(by='fair_to_foul', ascending=False).head(10)\n",
    "\n",
    "bottom_ten_ratio = df_cleaned.sort_values(by='fair_to_foul', ascending=True).head(10)\n",
    "\n",
    "bottom_ten_max = df_cleaned.sort_values(by='max_distance', ascending=True).head(10)\n",
    "bottom_ten_min = df_cleaned.sort_values(by='min_distance', ascending=False).head(10)\n",
    "bottom_ten_avg = df_cleaned.sort_values(by='avg_distance', ascending=True).head(10)\n",
    "bottom_ten_median = df_cleaned.sort_values(by='median_distance', ascending=True).head(10)\n",
    "\n",
    "\n",
    "### Create and display a dataframe with columns for the top and bottom ten fields for each category\n",
    "\n",
    "top_bottom_df = pd.DataFrame()\n",
    "\n",
    "top_bottom_df['top_ten_max'] = top_ten_max['park_name'].values\n",
    "top_bottom_df['top_ten_min'] = top_ten_min['park_name'].values\n",
    "top_bottom_df['top_ten_avg'] = top_ten_avg['park_name'].values\n",
    "\n",
    "top_bottom_df['top_ten_median'] = top_ten_median['park_name'].values\n",
    "top_bottom_df['top_ten_field_area'] = top_ten_field_area['park_name'].values\n",
    "top_bottom_df['top_ten_foul_area'] = top_ten_foul_area['park_name'].values\n",
    "top_bottom_df['top_ten_fop_area'] = top_ten_fop_area['park_name'].values\n",
    "top_bottom_df['top_ten_ratio'] = top_ten_ratio['park_name'].values\n",
    "\n",
    "top_bottom_df['bottom_ten_ratio'] = bottom_ten_ratio['park_name'].values\n",
    "top_bottom_df['bottom_ten_max'] = bottom_ten_max['park_name'].values\n",
    "top_bottom_df['bottom_ten_min'] = bottom_ten_min['park_name'].values\n",
    "top_bottom_df['bottom_ten_avg'] = bottom_ten_avg['park_name'].values\n",
    "top_bottom_df['bottom_ten_median'] = bottom_ten_median['park_name'].values\n",
    "\n",
    "\n",
    "top_bottom_df.head(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW WITH AUTO SCALING\n",
    "\n",
    "def calculate_max_y(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    return max(bin_counts)\n",
    "\n",
    "\n",
    "def create_polar_chart(data, num_bins=36, level_filter=None, y_min=-20, background_color='#2b2b2b', color_map=plt.cm.viridis, bar_alpha=0.8):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 2 * np.pi / num_bins\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "\n",
    "    ax.set_facecolor('#808080')\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "    # Set dark background\n",
    "    ax.set_facecolor(background_color)\n",
    "    plt.gca().set_rlabel_position(22.5)\n",
    "    y_max = calculate_max_y(data, num_bins=num_bins, level_filter=level_filter) + 5\n",
    "    ax.set_ylim(y_min, y_max)  # Adjust based on max count\n",
    "\n",
    "    # Add bars for negative values\n",
    "    zero_height_bars = ax.bar(bin_edges[:-1], np.abs(ax.get_ylim()[0]), width=bin_width, bottom=0.0, color='k', alpha=0.3)\n",
    "\n",
    "    bars = ax.bar(bin_edges[:-1], bin_counts, width=bin_width, bottom=0)\n",
    "    \n",
    "    # Use custom colors and opacity\n",
    "    for r, bar in zip(bin_counts, bars):\n",
    "        bar.set_facecolor(color_map(r / max(bin_counts)))\n",
    "        bar.set_alpha(bar_alpha)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CALL AUTO ADJUSTING CHART #####\n",
    "\n",
    "\n",
    "## NEW PERAMS\n",
    "\n",
    "\n",
    "# Call your function\n",
    "create_polar_chart(\n",
    "    data, \n",
    "    num_bins=30, \n",
    "    # level_filter=\"level1\", \n",
    "    y_min=0, \n",
    "    background_color='#2b2b2b', \n",
    "    color_map=plt.cm.viridis, \n",
    "    bar_alpha=0.8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW CHAT GPT CODE\n",
    "\n",
    "def create_polar_chart(data, num_bins=36, level_filter=None, y_min=-20, y_max=130, background_color='#2b2b2b', color_map=plt.cm.viridis, bar_alpha=0.8):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 2 * np.pi / num_bins\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "\n",
    "    ax.set_facecolor('#808080')\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "    # Set dark background\n",
    "    ax.set_facecolor(background_color)\n",
    "    plt.gca().set_rlabel_position(22.5)\n",
    "    ax.set_ylim(y_min, y_max)  # Adjust based on max count\n",
    "\n",
    "    # Add bars for negative values\n",
    "    zero_height_bars = ax.bar(bin_edges[:-1], np.abs(ax.get_ylim()[0]), width=bin_width, bottom=0.0, color='k', alpha=0.3)\n",
    "\n",
    "    bars = ax.bar(bin_edges[:-1], bin_counts, width=bin_width, bottom=0)\n",
    "    \n",
    "    # Use custom colors and opacity\n",
    "    for r, bar in zip(bin_counts, bars):\n",
    "        bar.set_facecolor(color_map(r / max(bin_counts)))\n",
    "        bar.set_alpha(bar_alpha)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a polar chart showing the direction of all the tournment fields\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a function to process the data, counting the orientations and filtering by level.\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_data(data, level_filter=None):\n",
    "    count_by_orientation = defaultdict(int)\n",
    "    \n",
    "    for record in data:\n",
    "        if level_filter is None or record['level'] == level_filter:\n",
    "            orientation = round(record['field_orientation'])\n",
    "            count_by_orientation[orientation] += 1\n",
    "\n",
    "    return count_by_orientation\n",
    "\n",
    "def create_polar_chart(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 2 * np.pi / num_bins\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "\n",
    "    ax.set_facecolor('#808080')\n",
    "    ###\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "    \n",
    "    # # Set dark background\n",
    "    ax.set_facecolor('#2b2b2b')\n",
    "    plt.gca().set_rlabel_position(22.5)\n",
    "    ax.set_ylim(-20, 130)  # Adjust based on max count\n",
    "\n",
    "    # Add bars for negative values\n",
    "    zero_height_bars = ax.bar(bin_edges[:-1], np.abs(ax.get_ylim()[0]), width=bin_width, bottom=0.0, color='k', alpha=0.3)\n",
    "\n",
    "    bars = ax.bar(bin_edges[:-1], bin_counts, width=bin_width, bottom=0)\n",
    "    \n",
    "    # Use custom colors and opacity\n",
    "    for r, bar in zip(bin_counts, bars):\n",
    "        bar.set_facecolor(plt.cm.viridis(r / max(bin_counts)))\n",
    "        # bar.set_facecolor(plt.cm.plasma(r / max(bin_counts)))\n",
    "        bar.set_alpha(0.8)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_polar_chart(data, num_bins=50, level_filter=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
