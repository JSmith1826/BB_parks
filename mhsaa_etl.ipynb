{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ETL NOTEBOOK FOR 2023 MHSAA TOURNEY SPECIFIC MAP\n",
    "\n",
    "#### Adapted from ETL for JSON\n",
    "\n",
    "## Dependencies and Setup\n",
    "### Dependencies\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Start timer\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD BLOCK###\n",
    "#### Load data from kml file exported from Google Earth\n",
    "\n",
    "file_path = ('data/kml/MHSAA_2023.kml') # file path to kml file\n",
    "\n",
    "\n",
    "# Read the KML file\n",
    "with open(file_path) as file:\n",
    "    xml_data = file.read()\n",
    "\n",
    "# Initialize soup variables for parsing file\n",
    "soup = BeautifulSoup(xml_data, 'xml')\n",
    "folders = soup.Document.Folder\n",
    "list = soup.Document.Folder.find_all('Folder')\n",
    "\n",
    "# Create a list to store rows to append to the DataFrame\n",
    "rows = []\n",
    "\n",
    "# Loop through the folders and extract the data\n",
    "for folder in list:\n",
    "    try:\n",
    "        field_name = folder.find('name').text\n",
    "        foul = folder.find_all('coordinates')[0].text\n",
    "        fop = folder.find_all('coordinates')[1].text\n",
    "        notes = None\n",
    "\n",
    "        # Check if there is a description tag, if so, use it for notes\n",
    "        if folder.find('description') is not None:\n",
    "            notes = folder.find('description').text\n",
    "\n",
    "        row = {\n",
    "            'field': field_name,\n",
    "            'foul': foul,\n",
    "            'fop': fop,\n",
    "            'notes': notes\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Add name of folder to a list of failed folders\n",
    "        failed.append(folder.find('name').text)\n",
    "        print(f\"Error processing folder: {folder.find('name').text}. Error message: {str(e)}\")\n",
    "\n",
    "# Convert the list of rows to a DataFrame\n",
    "df = pd.DataFrame(rows, columns=['field', 'foul', 'fop', 'notes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams Butzel Complex</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-83.1678186,42.3966942,0 -83...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-83.1678186,42.3966942,0 -83...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adrian HS</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.0416584,41.9091676,0 -84...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.0416584,41.9091676,0 -84...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alcona HS</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-83.4068606,44.6597432,0 -83...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-83.4068606,44.6597432,0 -83...</td>\n",
       "      <td>tough treeline in center and left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algonac High School</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-82.58239759999999,42.628620...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-82.58239759999999,42.628620...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allen Park High School</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-83.2273711,42.2455509,0 -83...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-83.2273711,42.2455509,0 -83...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    field                                               foul  \\\n",
       "0    Adams Butzel Complex  \\n\\t\\t\\t\\t\\t\\t\\t\\t-83.1678186,42.3966942,0 -83...   \n",
       "1               Adrian HS  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.0416584,41.9091676,0 -84...   \n",
       "2               Alcona HS  \\n\\t\\t\\t\\t\\t\\t\\t\\t-83.4068606,44.6597432,0 -83...   \n",
       "3     Algonac High School  \\n\\t\\t\\t\\t\\t\\t\\t\\t-82.58239759999999,42.628620...   \n",
       "4  Allen Park High School  \\n\\t\\t\\t\\t\\t\\t\\t\\t-83.2273711,42.2455509,0 -83...   \n",
       "\n",
       "                                                 fop  \\\n",
       "0  \\n\\t\\t\\t\\t\\t\\t\\t\\t-83.1678186,42.3966942,0 -83...   \n",
       "1  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.0416584,41.9091676,0 -84...   \n",
       "2  \\n\\t\\t\\t\\t\\t\\t\\t\\t-83.4068606,44.6597432,0 -83...   \n",
       "3  \\n\\t\\t\\t\\t\\t\\t\\t\\t-82.58239759999999,42.628620...   \n",
       "4  \\n\\t\\t\\t\\t\\t\\t\\t\\t-83.2273711,42.2455509,0 -83...   \n",
       "\n",
       "                               notes  \n",
       "0                               None  \n",
       "1                               None  \n",
       "2  tough treeline in center and left  \n",
       "3                               None  \n",
       "4                               None  "
      ]
     },
     "execution_count": 1114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the new dataframe\n",
    "\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Remove new line and space characters from coordinates\n",
    "df_cleaned = df_cleaned.replace(r'\\n','', regex=True) \n",
    "df_cleaned = df_cleaned.replace(r'\\t','', regex=True) \n",
    "\n",
    "# Drop any duplicate rows\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['field'], keep='first')\n",
    "\n",
    "# Drop any rows with empty fields\n",
    "df_cleaned = df_cleaned[(df_cleaned != 0).all(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 140 entries, 0 to 143\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   field   140 non-null    object\n",
      " 1   foul    140 non-null    object\n",
      " 2   fop     140 non-null    object\n",
      " 3   notes   7 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 5.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams Butzel Complex</td>\n",
       "      <td>-83.1678186,42.3966942,0 -83.1678776,42.397648...</td>\n",
       "      <td>-83.1678186,42.3966942,0 -83.1665385,42.396724...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adrian HS</td>\n",
       "      <td>-84.0416584,41.9091676,0 -84.04166909999999,41...</td>\n",
       "      <td>-84.0416584,41.9091676,0 -84.0405493,41.909184...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alcona HS</td>\n",
       "      <td>-83.4068606,44.6597432,0 -83.40803409999999,44...</td>\n",
       "      <td>-83.4068606,44.6597432,0 -83.40680159999999,44...</td>\n",
       "      <td>tough treeline in center and left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algonac High School</td>\n",
       "      <td>-82.58239759999999,42.6286202,0 -82.5813153999...</td>\n",
       "      <td>-82.58239759999999,42.6286202,0 -82.5826256,42...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allen Park High School</td>\n",
       "      <td>-83.2273711,42.2455509,0 -83.2285244,42.245525...</td>\n",
       "      <td>-83.2273711,42.2455509,0 -83.22739919999999,42...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    field                                               foul  \\\n",
       "0    Adams Butzel Complex  -83.1678186,42.3966942,0 -83.1678776,42.397648...   \n",
       "1               Adrian HS  -84.0416584,41.9091676,0 -84.04166909999999,41...   \n",
       "2               Alcona HS  -83.4068606,44.6597432,0 -83.40803409999999,44...   \n",
       "3     Algonac High School  -82.58239759999999,42.6286202,0 -82.5813153999...   \n",
       "4  Allen Park High School  -83.2273711,42.2455509,0 -83.2285244,42.245525...   \n",
       "\n",
       "                                                 fop  \\\n",
       "0  -83.1678186,42.3966942,0 -83.1665385,42.396724...   \n",
       "1  -84.0416584,41.9091676,0 -84.0405493,41.909184...   \n",
       "2  -83.4068606,44.6597432,0 -83.40680159999999,44...   \n",
       "3  -82.58239759999999,42.6286202,0 -82.5826256,42...   \n",
       "4  -83.2273711,42.2455509,0 -83.22739919999999,42...   \n",
       "\n",
       "                               notes  \n",
       "0                               None  \n",
       "1                               None  \n",
       "2  tough treeline in center and left  \n",
       "3                               None  \n",
       "4                               None  "
      ]
     },
     "execution_count": 1116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.info()\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Clean up polygon data and create a new home_plate column\n",
    "\n",
    "def parse_coordinates(coord_string):\n",
    "    coords = coord_string.split()\n",
    "    parsed_coords = [tuple(map(float, coord.split(',')[:2])) for coord in coords]\n",
    "    return parsed_coords\n",
    "\n",
    "# Create a new column for the home_plate location using the first set of coordinates in the 'fop' column\n",
    "df_cleaned['home_plate'] = df_cleaned['fop'].apply(lambda x: parse_coordinates(x)[0])\n",
    "\n",
    "# Apply the parse_coordinates function to the 'foul' and 'fop' columns\n",
    "df_cleaned['foul'] = df_cleaned['foul'].apply(parse_coordinates)\n",
    "df_cleaned['fop'] = df_cleaned['fop'].apply(parse_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## AREA CALCULATION ##############\n",
    "\n",
    "\n",
    "import pyproj\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import transform\n",
    "\n",
    "\n",
    "def calculate_area(coords):\n",
    "    # Create a Polygon object from the coordinates\n",
    "    polygon = Polygon(coords)\n",
    "\n",
    "    # Calculate the centroid of the polygon\n",
    "    centroid = polygon.centroid\n",
    "\n",
    "    # Create a custom LAEA projection centered on the centroid\n",
    "    custom_projection = f\"+proj=laea +lat_0={centroid.y} +lon_0={centroid.x} +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"\n",
    "\n",
    "    # Create a transformer for converting coordinates to the custom LAEA projection\n",
    "    transformer = pyproj.Transformer.from_crs(\n",
    "        pyproj.CRS(\"EPSG:4326\"),  # WGS 84 (latitude and longitude)\n",
    "        pyproj.CRS(custom_projection),  # Custom LAEA projection\n",
    "        always_xy=True\n",
    "    )\n",
    "\n",
    "    # Define a function to transform coordinates using the transformer\n",
    "    def transform_coordinates(x, y):\n",
    "        return transformer.transform(x, y)\n",
    "\n",
    "    # Convert the coordinates to the custom LAEA projection\n",
    "    polygon_laea = transform(transform_coordinates, polygon)\n",
    "\n",
    "    # Calculate the area in square meters\n",
    "    area_sqm = polygon_laea.area\n",
    "\n",
    "    # Convert the area to square feet (1 square meter = 10.764 square feet)\n",
    "    area_sqft = area_sqm * 10.764\n",
    "\n",
    "    return area_sqft\n",
    "\n",
    "\n",
    "\n",
    "### Call Function and add to dataframe\n",
    "df_cleaned['foul_area_sqft'] = df_cleaned['foul'].apply(calculate_area)\n",
    "df_cleaned['fop_area_sqft'] = df_cleaned['fop'].apply(calculate_area)\n",
    "\n",
    "## Calculate the total area of the field and the ratio of foul area to field area\n",
    "df_cleaned['field_area_sqft'] = df_cleaned['foul_area_sqft'] + df_cleaned['fop_area_sqft']\n",
    "## Percentage foul area\n",
    "df_cleaned['foul_area_per'] = df_cleaned['foul_area_sqft'] / df_cleaned['field_area_sqft']\n",
    "## Fair to Foul Ratio\n",
    "df_cleaned['fair_to_foul'] = df_cleaned['fop_area_sqft'] / df_cleaned['foul_area_sqft']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# FENCE DISTANCE CALCULATION #############\n",
    "\n",
    "from geopy.distance import great_circle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def interpolate_points(start, end, length_ratio):\n",
    "    start_np = np.array(start)\n",
    "    end_np = np.array(end)\n",
    "    return tuple(start_np + (end_np - start_np) * length_ratio)\n",
    "\n",
    "def calculate_distances(home_plate, outfield_coords, num_points=540):\n",
    "    def is_same_point(point1, point2, tolerance=1e-6):\n",
    "        return abs(point1[0] - point2[0]) < tolerance and abs(point1[1] - point2[1]) < tolerance\n",
    "\n",
    "    home_plate_lat_lon = (home_plate[1], home_plate[0])\n",
    "    distances = []\n",
    "\n",
    "    # Calculate total line length\n",
    "    total_length = 0\n",
    "    segments = []\n",
    "    for i in range(len(outfield_coords) - 1):\n",
    "        start = outfield_coords[i]\n",
    "        end = outfield_coords[i + 1]\n",
    "        if not is_same_point(home_plate, start) and not is_same_point(home_plate, end):\n",
    "            segment_length = great_circle((start[1], start[0]), (end[1], end[0])).feet\n",
    "            segments.append((start, end, segment_length))\n",
    "            total_length += segment_length\n",
    "\n",
    "    # Calculate the distance between equally spaced points\n",
    "    spacing = total_length / (num_points - 1)\n",
    "\n",
    "    # Interpolate points and calculate distances\n",
    "    current_length = 0\n",
    "    segment_index = 0\n",
    "    for i in range(num_points):\n",
    "        while segment_index < len(segments) - 1 and current_length > segments[segment_index][2]:\n",
    "            current_length -= segments[segment_index][2]\n",
    "            segment_index += 1\n",
    "\n",
    "        start, end, segment_length = segments[segment_index]\n",
    "        length_ratio = current_length / segment_length\n",
    "        point = interpolate_points(start, end, length_ratio)\n",
    "        distance = great_circle(home_plate_lat_lon, (point[1], point[0])).feet\n",
    "        distances.append(distance)\n",
    "\n",
    "        current_length += spacing\n",
    "\n",
    "    return distances\n",
    "\n",
    "# Calculate distances for each row\n",
    "df_cleaned['distances'] = df_cleaned.apply(lambda row: calculate_distances(row['home_plate'], row['fop']), axis=1)\n",
    "\n",
    "# Calculate max, min, and average distances for each row\n",
    "df_cleaned['max_distance'] = df_cleaned['distances'].apply(max)\n",
    "df_cleaned['min_distance'] = df_cleaned['distances'].apply(min)\n",
    "df_cleaned['avg_distance'] = df_cleaned['distances'].apply(lambda distances: sum(distances) / len(distances))\n",
    "# get the median distance\n",
    "df_cleaned['median_distance'] = df_cleaned['distances'].apply(lambda distances: np.median(distances))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540    140\n",
       "Name: num_distances, dtype: int64"
      ]
     },
     "execution_count": 1120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## CHECK BLOCK ########\n",
    "\n",
    "## Check how long the distance list is for each row\n",
    "df_cleaned['num_distances'] = df_cleaned['distances'].apply(len)\n",
    "\n",
    "## Print the value counts for the 'num_distances' column\n",
    "df_cleaned['num_distances'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### NOT NECESSARY FOR THIS PROJECT ##########\n",
    "\n",
    "# ### Get Geolocation of each field based on home plate coordinates and return state and country\n",
    "# ### This block takes a long time to run - will need to revisit\n",
    "# ## up to ten minutes\n",
    "\n",
    "# from geopy.geocoders import Nominatim\n",
    "# from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# geolocator = Nominatim(user_agent=\"baseball_field_locator\")\n",
    "\n",
    "# import time\n",
    "\n",
    "# def get_location_info(lng, lat):\n",
    "#     try:\n",
    "#         time.sleep(1)  # Delay for 1 second\n",
    "#         location = geolocator.reverse((lat, lng), timeout=10)\n",
    "#         city = location.raw['address'].get('city', None)\n",
    "#         state = location.raw['address'].get('state', None)\n",
    "#         return city, state\n",
    "#     except GeocoderTimedOut:\n",
    "#         print(f\"GeocoderTimedOut error for coordinates: ({lng}, {lat})\")\n",
    "#         return None, None\n",
    "#     except GeocoderServiceError:\n",
    "#         print(f\"GeocoderServiceError for coordinates: ({lng}, {lat})\")\n",
    "#         return None, None\n",
    "\n",
    "\n",
    "# # Extract the first coordinate for each field\n",
    "# df_cleaned['lng'], df_cleaned['lat'] = zip(*df_cleaned['home_plate'].apply(lambda x: x))\n",
    "\n",
    "# # Wrap the DataFrame apply function with tqdm for progress indication\n",
    "# tqdm.pandas(desc=\"Processing coordinates\")\n",
    "\n",
    "# # Get state and country information for each field\n",
    "# df_cleaned[['city', 'state']] = df_cleaned.progress_apply(lambda row: get_location_info(row['lng'], row['lat']), axis=1, result_type='expand')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create ranks for each column\n",
    "\n",
    "def rank_fields(df):\n",
    "    # Calculate the rank for each category\n",
    "    df['max_distance_rank'] = df['max_distance'].rank(ascending=False, method='min')\n",
    "    df['min_distance_rank'] = df['min_distance'].rank(ascending=False, method='min')\n",
    "    df['avg_distance_rank'] = df['avg_distance'].rank(ascending=False, method='min')\n",
    "    df['median_distance_rank'] = df['median_distance'].rank(ascending=False, method='min')\n",
    "    df['field_area_rank'] = df['field_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['foul_area_rank'] = df['foul_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['fop_area_per_rank'] = df['fop_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['ratio_rank'] = df['fair_to_foul'].rank(ascending=False, method='min')\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Function\n",
    "\n",
    "df_cleaned = rank_fields(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Orienting the map to the home plate location ####\n",
    "\n",
    "### Find the center of the field\n",
    "def calculate_centroid(coords):\n",
    "    x_coords = [coord[0] for coord in coords]\n",
    "    y_coords = [coord[1] for coord in coords]\n",
    "    centroid_x = sum(x_coords) / len(coords)\n",
    "    centroid_y = sum(y_coords) / len(coords)\n",
    "    return (centroid_x, centroid_y)\n",
    "\n",
    "\n",
    "## Find the bearing between the home plate and the center of the field\n",
    "import math\n",
    "\n",
    "def calculate_bearing(point1, point2):\n",
    "    lat1, lon1 = math.radians(point1[1]), math.radians(point1[0])\n",
    "    lat2, lon2 = math.radians(point2[1]), math.radians(point2[0])\n",
    "\n",
    "    d_lon = lon2 - lon1\n",
    "\n",
    "    x = math.cos(lat2) * math.sin(d_lon)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(d_lon)\n",
    "\n",
    "    bearing = math.degrees(math.atan2(x, y))\n",
    "    bearing = (bearing + 360) % 360  # Normalize the bearing to the range [0, 360)\n",
    "\n",
    "    return bearing\n",
    "\n",
    "### Function to classify direction in laymans terms North, South, East, West, ect\n",
    "def degrees_to_cardinal_direction(degrees):\n",
    "    directions = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW', 'N']\n",
    "    index = round(degrees / 45)\n",
    "    return directions[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the centroid of the outfield fence coordinates for each row\n",
    "df_cleaned['fop_centroid'] = df_cleaned['fop'].apply(lambda coords: calculate_centroid(coords[1:]))\n",
    "\n",
    "# Calculate the bearing between home plate and the centroid for each row\n",
    "df_cleaned['field_orientation'] = df_cleaned.apply(lambda row: calculate_bearing(row['home_plate'], row['fop_centroid']), axis=1)\n",
    "\n",
    "# Convert the bearing to a cardinal direction\n",
    "df_cleaned['field_cardinal_direction'] = df_cleaned['field_orientation'].apply(degrees_to_cardinal_direction)\n",
    "\n",
    "# rename 'field' to 'park_name'\n",
    "df_cleaned.rename(columns={'field': 'park_name'}, inplace=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the geo transformation should take place above this\n",
    "\n",
    "## starting the process of matching in data from other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## output to csv \n",
    "# df_cleaned.to_csv('data/fields_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned.info()\n",
    "\n",
    "# # Load the host team info with nickname and team colors\n",
    "path = 'data/MHSAA/2023_MHSAA_sites.csv'\n",
    "df_hosts = pd.read_csv(path)\n",
    "\n",
    "df_parks = df_cleaned\n",
    "\n",
    "df_hosts.head()\n",
    "\n",
    "park_df = df_parks\n",
    "host_df = df_hosts\n",
    "# df_hosts.info()\n",
    "\n",
    "# # Merge the host team info with the field info\n",
    "# df_cleaned = df_cleaned.merge(df_hosts, on='host_team', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park_name</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>notes</th>\n",
       "      <th>home_plate</th>\n",
       "      <th>foul_area_sqft</th>\n",
       "      <th>fop_area_sqft</th>\n",
       "      <th>field_area_sqft</th>\n",
       "      <th>foul_area_per</th>\n",
       "      <th>fair_to_foul</th>\n",
       "      <th>...</th>\n",
       "      <th>min_distance_rank</th>\n",
       "      <th>avg_distance_rank</th>\n",
       "      <th>median_distance_rank</th>\n",
       "      <th>field_area_rank</th>\n",
       "      <th>foul_area_rank</th>\n",
       "      <th>fop_area_per_rank</th>\n",
       "      <th>ratio_rank</th>\n",
       "      <th>fop_centroid</th>\n",
       "      <th>field_orientation</th>\n",
       "      <th>field_cardinal_direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Lowell High School - high school</td>\n",
       "      <td>[(-85.3763871, 42.9571353), (-85.3768716, 42.9...</td>\n",
       "      <td>[(-85.3763871, 42.9571353), (-85.3752914, 42.9...</td>\n",
       "      <td>guestimate on the fenceline based on the mow p...</td>\n",
       "      <td>(-85.3763871, 42.9571353)</td>\n",
       "      <td>29097.855748</td>\n",
       "      <td>99166.362021</td>\n",
       "      <td>128264.217769</td>\n",
       "      <td>0.226859</td>\n",
       "      <td>3.40803</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>(-85.3761548818182, 42.95801321363636)</td>\n",
       "      <td>10.956002</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           park_name  \\\n",
       "79  Lowell High School - high school   \n",
       "\n",
       "                                                 foul  \\\n",
       "79  [(-85.3763871, 42.9571353), (-85.3768716, 42.9...   \n",
       "\n",
       "                                                  fop  \\\n",
       "79  [(-85.3763871, 42.9571353), (-85.3752914, 42.9...   \n",
       "\n",
       "                                                notes  \\\n",
       "79  guestimate on the fenceline based on the mow p...   \n",
       "\n",
       "                   home_plate  foul_area_sqft  fop_area_sqft  field_area_sqft  \\\n",
       "79  (-85.3763871, 42.9571353)    29097.855748   99166.362021    128264.217769   \n",
       "\n",
       "    foul_area_per  fair_to_foul  ... min_distance_rank  avg_distance_rank  \\\n",
       "79       0.226859       3.40803  ...              30.0               19.0   \n",
       "\n",
       "    median_distance_rank  field_area_rank  foul_area_rank  fop_area_per_rank  \\\n",
       "79                  18.0             24.0            55.0               18.0   \n",
       "\n",
       "    ratio_rank                            fop_centroid  field_orientation  \\\n",
       "79        73.0  (-85.3761548818182, 42.95801321363636)          10.956002   \n",
       "\n",
       "    field_cardinal_direction  \n",
       "79                         N  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 1128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# host_df.info()\n",
    "\n",
    "## find the Lowell High School field\n",
    "host_df[host_df['park_name'] == 'Lowell High School - high school']\n",
    "\n",
    "\n",
    "\n",
    "# park_df.info()\n",
    "\n",
    "## find the Lowell High School field\n",
    "park_df[park_df['park_name'] == 'Lowell High School - high school']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple Merge, should work because the park_name columns should match exactly\n",
    "## Do not detroy any data\n",
    "df_merged = park_df.merge(host_df, on='park_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "park_name                           Concordia University - AA Greenhills Site\n",
       "foul                        [(-83.67825654723873, 42.27801177186786), (-83...\n",
       "fop                         [(-83.67906479189217, 42.27737522544219), (-83...\n",
       "notes                                                                    None\n",
       "home_plate                            (-83.67906479189217, 42.27737522544219)\n",
       "foul_area_sqft                                                   27200.017475\n",
       "fop_area_sqft                                                    97387.018799\n",
       "field_area_sqft                                                 124587.036274\n",
       "foul_area_per                                                        0.218321\n",
       "fair_to_foul                                                         3.580403\n",
       "distances                   [13.354349244817307, 14.97801735189316, 16.601...\n",
       "max_distance                                                       453.605194\n",
       "min_distance                                                        13.354349\n",
       "avg_distance                                                       299.404612\n",
       "median_distance                                                    331.405144\n",
       "num_distances                                                             540\n",
       "max_distance_rank                                                         1.0\n",
       "min_distance_rank                                                       140.0\n",
       "avg_distance_rank                                                       140.0\n",
       "median_distance_rank                                                     91.0\n",
       "field_area_rank                                                          31.0\n",
       "foul_area_rank                                                           69.0\n",
       "fop_area_per_rank                                                        23.0\n",
       "ratio_rank                                                               62.0\n",
       "fop_centroid                          (-83.67811467032898, 42.27717113777751)\n",
       "field_orientation                                                  106.188354\n",
       "field_cardinal_direction                                                    E\n",
       "host_team                                                Ann Arbor Greenhills\n",
       "division                                                                  3.0\n",
       "district                                                                 88.0\n",
       "region_semi_number                                                        NaN\n",
       "regional_div                                                              NaN\n",
       "region_final_quarter                                                      NaN\n",
       "finals                                                                    NaN\n",
       "nickname                                                             Gryphons\n",
       "color1                                                                  Navy \n",
       "color2                                                                 Forest\n",
       "color3                                                                    NaN\n",
       "Name: 139, dtype: object"
      ]
     },
     "execution_count": 1133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## show index 30\n",
    "\n",
    "df_merged.iloc[139]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Osbourn - MAX Outlier - plot is correct it is just a strange fenceless field\n",
    "df_merged.drop([28, 139], inplace=True)\n",
    "\n",
    "# Drop AA Greenhills because something is wrong witht the plot\n",
    "# Division is hosted at Concordia University AA - I have that ploted but it is not apearing in the data\n",
    "# df_merged.drop([137], inplace=True)   \n",
    "\n",
    "## Reset index\n",
    "df_merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 138 entries, 0 to 137\n",
      "Data columns (total 38 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   park_name                 138 non-null    object \n",
      " 1   foul                      138 non-null    object \n",
      " 2   fop                       138 non-null    object \n",
      " 3   notes                     7 non-null      object \n",
      " 4   home_plate                138 non-null    object \n",
      " 5   foul_area_sqft            138 non-null    float64\n",
      " 6   fop_area_sqft             138 non-null    float64\n",
      " 7   field_area_sqft           138 non-null    float64\n",
      " 8   foul_area_per             138 non-null    float64\n",
      " 9   fair_to_foul              138 non-null    float64\n",
      " 10  distances                 138 non-null    object \n",
      " 11  max_distance              138 non-null    float64\n",
      " 12  min_distance              138 non-null    float64\n",
      " 13  avg_distance              138 non-null    float64\n",
      " 14  median_distance           138 non-null    float64\n",
      " 15  num_distances             138 non-null    int64  \n",
      " 16  max_distance_rank         138 non-null    float64\n",
      " 17  min_distance_rank         138 non-null    float64\n",
      " 18  avg_distance_rank         138 non-null    float64\n",
      " 19  median_distance_rank      138 non-null    float64\n",
      " 20  field_area_rank           138 non-null    float64\n",
      " 21  foul_area_rank            138 non-null    float64\n",
      " 22  fop_area_per_rank         138 non-null    float64\n",
      " 23  ratio_rank                138 non-null    float64\n",
      " 24  fop_centroid              138 non-null    object \n",
      " 25  field_orientation         138 non-null    float64\n",
      " 26  field_cardinal_direction  138 non-null    object \n",
      " 27  host_team                 135 non-null    object \n",
      " 28  division                  133 non-null    float64\n",
      " 29  district                  121 non-null    float64\n",
      " 30  region_semi_number        62 non-null     float64\n",
      " 31  regional_div              74 non-null     float64\n",
      " 32  region_final_quarter      14 non-null     float64\n",
      " 33  finals                    1 non-null      float64\n",
      " 34  nickname                  134 non-null    object \n",
      " 35  color1                    134 non-null    object \n",
      " 36  color2                    134 non-null    object \n",
      " 37  color3                    11 non-null     object \n",
      "dtypes: float64(24), int64(1), object(13)\n",
      "memory usage: 41.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename to use next block\n",
    "\n",
    "df = df_merged\n",
    "\n",
    "### Updated to create Standard Div +/- Lines\n",
    "\n",
    "## create the min max and mean fence distance rows\n",
    "# Transpose the dataframe to get the \n",
    "transposed_df = pd.DataFrame(df['distances'].to_list()).transpose()\n",
    "\n",
    "# Calculate min, max, mean, median, Q1 and Q3 for each row\n",
    "min_fence_distances = transposed_df.min(axis=1)\n",
    "max_fence_distances = transposed_df.max(axis=1)\n",
    "mean_fence_distances = transposed_df.mean(axis=1)\n",
    "median_fence_distances = transposed_df.median(axis=1)\n",
    "## create profiles for standard deviation\n",
    "std_fence_distances = transposed_df.std(axis=1)\n",
    "first_fence_distances = mean_fence_distances + std_fence_distances\n",
    "third_fence_distances = mean_fence_distances - std_fence_distances\n",
    "\n",
    "# Create a new DataFrame to store these values\n",
    "new_df = pd.DataFrame({\n",
    "    'park_name': ['Min', 'Max', 'Mean', 'Median', 'Q1', 'Q3'],\n",
    "    'distances': [\n",
    "        min_fence_distances.tolist(), \n",
    "        max_fence_distances.tolist(),\n",
    "        mean_fence_distances.tolist(),\n",
    "        median_fence_distances.tolist(), # Add a comma here\n",
    "        first_fence_distances.tolist(),\n",
    "        third_fence_distances.tolist()\n",
    "    ]\n",
    "})\n",
    "\n",
    "# For all other columns in the original DataFrame, add a column of NaN values in the new DataFrame\n",
    "for column in df.columns:\n",
    "    if column not in new_df.columns:\n",
    "        new_df[column] = np.nan\n",
    "\n",
    "# Concatenate the new DataFrame with the original one\n",
    "df = pd.concat([df, new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the output directory for the Outfield Fence Plots\n",
    "\n",
    "output_dir = 'data/MHSAA/assets/plots/'\n",
    "\n",
    "def plot_distances(df, row_index):\n",
    "    # Get rows with 'Min', 'Max', 'Mean', 'Q1', 'Q3' in 'park_name'\n",
    "    rows_to_plot = df[df['park_name'].isin(['Min', 'Max', 'Mean', 'Q1', 'Q3'])]\n",
    "    \n",
    "    # Get the row to be highlighted\n",
    "    highlighted_row = df.loc[row_index]\n",
    "    \n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    # Loop over these rows and plot a line graph for each\n",
    "    for index, row in rows_to_plot.iterrows():\n",
    "        if row['park_name'] in ['Q1', 'Q3']: # If Q1 or Q3, plot thinner, dotted line\n",
    "            plt.plot(row['distances'], linestyle='dotted', alpha=0.3, color='grey', label=row['park_name'])\n",
    "        else:\n",
    "            plt.plot(row['distances'], linestyle='dashed', alpha=0.5, label=row['park_name'])\n",
    "\n",
    "        # Add text labels for Min, Max and Mean lines\n",
    "        if row['park_name'] in ['Min', 'Max', 'Mean']:\n",
    "            plt.text(len(row['distances'])-1, row['distances'][-1], row['park_name'], color='blue', va='center')\n",
    "\n",
    "        # Check if the current row is 'Min', if so, add shading\n",
    "        if row['park_name'] == 'Min':\n",
    "            plt.fill_between(range(len(row['distances'])), row['distances'], color='green', alpha=0.2)\n",
    "\n",
    "        # Check if the current row is 'Max', if so, add shading\n",
    "        if row['park_name'] == 'Max':\n",
    "            plt.fill_between(range(len(row['distances'])), row['distances'], color='yellow', alpha=0.2)\n",
    "\n",
    "        # Check if the current row is 'Max', if so, add shading above\n",
    "        if row['park_name'] == 'Max':\n",
    "            plt.fill_between(range(len(row['distances'])), plt.ylim()[1], row['distances'], color='red', alpha=0.2)\n",
    "            \n",
    "    # Plot the highlighted row with a thicker line\n",
    "    plt.plot(highlighted_row['distances'], linewidth=2, label=highlighted_row['park_name'])\n",
    "    \n",
    "    # Set the minimum and maximum values of y-axis\n",
    "    plt.ylim([270, 420])\n",
    "\n",
    "    # Change y-axis labels and tick marks to be white\n",
    "    plt.ylabel('Distance (feet)', color='white')\n",
    "    plt.tick_params(axis='y', colors='white')\n",
    "\n",
    "    # Hide x axis ticks\n",
    "    plt.xticks([])\n",
    "\n",
    "    # Move the title to the inside the plot, centered, just above the x axis\n",
    "    plt.text(len(highlighted_row['distances'])/2, 270, highlighted_row['park_name'], ha='center', va='bottom', fontsize=16)\n",
    "\n",
    "    # Reverse the x-axis\n",
    "    plt.gca().invert_xaxis()\n",
    "\n",
    "    # Generate the file path\n",
    "    file_path = os.path.join(output_dir, f\"plot_{row_index}.png\")\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(file_path)\n",
    "\n",
    "    # Close the figure to free up memory\n",
    "    plt.close()\n",
    "\n",
    "    # Return the file path\n",
    "    return file_path\n",
    "\n",
    "# Add a new column 'file_path' to the DataFrame to store the file paths\n",
    "df['file_path'] = [plot_distances(df, i) for i in df.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1089,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_18672\\2481228017.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(park_df.iloc[30])\n",
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_18672\\2481228017.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(park_df.iloc[6])\n"
     ]
    }
   ],
   "source": [
    "## Add Osbourn back to the end of the dataframe\n",
    "df = df.append(park_df.iloc[30])\n",
    "df = df.append(park_df.iloc[6])\n",
    "\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the rows with null values in the 'fop', 'foul' or 'home_plate' columns\n",
    "df.dropna(subset=['fop', 'foul', 'home_plate'], inplace=True)\n",
    "\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Division</th>\n",
       "      <th>District</th>\n",
       "      <th>Host</th>\n",
       "      <th>Location</th>\n",
       "      <th>Teams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Marquette</td>\n",
       "      <td>North Marquette Fields</td>\n",
       "      <td>['Alpena', 'Mount Pleasant', 'Traverse City Ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Midland Dow</td>\n",
       "      <td>H H Dow High School - Baseball - Midland</td>\n",
       "      <td>['Bay City Central', 'Bay City Western', 'Midl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Muskegon Mona Shores</td>\n",
       "      <td>Mona Shores Baseball Field (Baseball Field) - ...</td>\n",
       "      <td>['Grand Haven', 'Grand Rapids Kenowa Hills', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Grand Rapids Forest Hills Northern</td>\n",
       "      <td>FHN Stadium - Baseball - Grand Rapids</td>\n",
       "      <td>['Cedar Springs', 'Grand Rapids Northview', 'G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Grandville</td>\n",
       "      <td>Grandville High School - Baseball</td>\n",
       "      <td>['Byron Center', 'Grand Rapids Union', 'Jeniso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Division  District                                Host  \\\n",
       "0         1         1                           Marquette   \n",
       "1         1         2                         Midland Dow   \n",
       "2         1         3                Muskegon Mona Shores   \n",
       "3         1         4  Grand Rapids Forest Hills Northern   \n",
       "4         1         5                          Grandville   \n",
       "\n",
       "                                            Location  \\\n",
       "0                             North Marquette Fields   \n",
       "1          H H Dow High School - Baseball - Midland    \n",
       "2  Mona Shores Baseball Field (Baseball Field) - ...   \n",
       "3              FHN Stadium - Baseball - Grand Rapids   \n",
       "4                  Grandville High School - Baseball   \n",
       "\n",
       "                                               Teams  \n",
       "0  ['Alpena', 'Mount Pleasant', 'Traverse City Ce...  \n",
       "1  ['Bay City Central', 'Bay City Western', 'Midl...  \n",
       "2  ['Grand Haven', 'Grand Rapids Kenowa Hills', '...  \n",
       "3  ['Cedar Springs', 'Grand Rapids Northview', 'G...  \n",
       "4  ['Byron Center', 'Grand Rapids Union', 'Jeniso...  "
      ]
     },
     "execution_count": 1091,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Merge the display names into the dataframe\n",
    "\n",
    "## Load the display names from csv\n",
    "path = 'data/MHSAA/2023_district_teams.csv'\n",
    "places_df = pd.read_csv(path)\n",
    "\n",
    "places_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Merge the display names into the dataframe\n",
    "# ## merged_df column District, places_df column district\n",
    "# df_merged = df_merged.merge(places_df, on='district', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           park_name      score\n",
      "124  Stoepel Park - Detroit Communications and Media  13.110362\n",
      "58                                           Hart HS  10.155267\n",
      "18                     Central Lake HS - high_school   9.802493\n",
      "71                                       Kingston HS   9.583005\n",
      "82                                         Martin HS   8.504315\n",
      "..                                               ...        ...\n",
      "23                                     CMU - college -10.224740\n",
      "115                   Sagniaw Valley State - college -10.258899\n",
      "85                Michigan State - Old College Field -11.226785\n",
      "69                       Kalamazoo College - college -11.612219\n",
      "137        Concordia University - AA Greenhills Site -13.247693\n",
      "\n",
      "[140 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "### THIS BLOCK CREATES THE RANKING OF PITCHER VS HITTER FRIENDLY FIELDS\n",
    "def rank_fields(data):\n",
    "    # Define weights for each parameter\n",
    "    weights = {\n",
    "        'max_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'min_distance': 1,  # positive weight since shorter fences favor hitters\n",
    "        'avg_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'median_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'field_area_sqft': -1,  # negative weight since larger fields favor pitchers\n",
    "        'fair_to_foul': -1,  # negative weight since larger ratio (more foul territory) favors pitchers\n",
    "        'foul_area_sqft': -1, # negative weight since larger foul area favors pitchers\n",
    "        'fop_area_sqft': -1, # negative weight since larger out of play area favors pitchers\n",
    "    }\n",
    "\n",
    "    # Standardize features (subtract mean and divide by standard deviation)\n",
    "    standardized_data = data.copy()\n",
    "    for column in weights.keys():\n",
    "        standardized_data[column] = (standardized_data[column] - standardized_data[column].mean()) / standardized_data[column].std()\n",
    "\n",
    "    # Calculate score for each field\n",
    "    standardized_data['score'] = standardized_data.apply(lambda row: sum(row[param] * weight for param, weight in weights.items()), axis=1)\n",
    "\n",
    "    # Save scores to original dataframe\n",
    "    data['score'] = standardized_data['score']\n",
    "\n",
    "    # Rank fields based on score (higher scores are more hitter-friendly)\n",
    "    ranked_fields = data.sort_values('score', ascending=False)\n",
    "\n",
    "    return ranked_fields\n",
    "\n",
    "# Suppose 'df' is your DataFrame containing the field data\n",
    "ranked_fields = rank_fields(df)\n",
    "print(ranked_fields[['park_name', 'score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 140 entries, 124 to 137\n",
      "Data columns (total 40 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   park_name                 140 non-null    object \n",
      " 1   foul                      140 non-null    object \n",
      " 2   fop                       140 non-null    object \n",
      " 3   notes                     7 non-null      object \n",
      " 4   home_plate                140 non-null    object \n",
      " 5   foul_area_sqft            140 non-null    float64\n",
      " 6   fop_area_sqft             140 non-null    float64\n",
      " 7   field_area_sqft           140 non-null    float64\n",
      " 8   foul_area_per             140 non-null    float64\n",
      " 9   fair_to_foul              140 non-null    float64\n",
      " 10  distances                 140 non-null    object \n",
      " 11  max_distance              140 non-null    float64\n",
      " 12  min_distance              140 non-null    float64\n",
      " 13  avg_distance              140 non-null    float64\n",
      " 14  median_distance           140 non-null    float64\n",
      " 15  num_distances             140 non-null    float64\n",
      " 16  max_distance_rank         140 non-null    float64\n",
      " 17  min_distance_rank         140 non-null    float64\n",
      " 18  avg_distance_rank         140 non-null    float64\n",
      " 19  median_distance_rank      140 non-null    float64\n",
      " 20  field_area_rank           140 non-null    float64\n",
      " 21  foul_area_rank            140 non-null    float64\n",
      " 22  fop_area_per_rank         140 non-null    float64\n",
      " 23  ratio_rank                140 non-null    float64\n",
      " 24  fop_centroid              140 non-null    object \n",
      " 25  field_orientation         140 non-null    float64\n",
      " 26  field_cardinal_direction  140 non-null    object \n",
      " 27  host_team                 135 non-null    object \n",
      " 28  division                  133 non-null    float64\n",
      " 29  district                  121 non-null    float64\n",
      " 30  region_semi_number        62 non-null     float64\n",
      " 31  regional_div              74 non-null     float64\n",
      " 32  region_final_quarter      14 non-null     float64\n",
      " 33  finals                    1 non-null      float64\n",
      " 34  nickname                  134 non-null    object \n",
      " 35  color1                    134 non-null    object \n",
      " 36  color2                    134 non-null    object \n",
      " 37  color3                    11 non-null     object \n",
      " 38  file_path                 138 non-null    object \n",
      " 39  score                     140 non-null    float64\n",
      "dtypes: float64(26), object(14)\n",
      "memory usage: 44.8+ KB\n"
     ]
    }
   ],
   "source": [
    "ranked_fields.info()\n",
    "merged_df = ranked_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import webcolors\n",
    "\n",
    "# Assuming df is your DataFrame and it has columns 'color1' and 'color2'\n",
    "\n",
    "custom_colors = {\n",
    "    'Maize': '#F2C649',\n",
    "    'Columbia Blue': '#C4D8E2',\n",
    "    'Carolina Blue': '#56A0D3',\n",
    "    'Cardinal': '#C41E3A',\n",
    "    'Burgundy': '#800020',\n",
    "    'Forrest Green': '#18453B',\n",
    "    'Forest Green': '#18453B',\n",
    "    'Columbia': '#C4D8E2',\n",
    "    'Royal': '#4169e1',\n",
    "    'Royal Blue': '#4169e1',\n",
    "    'Vegas Gold': '#C5B358',\n",
    "    'Navy Blue': '#000080'\n",
    "}\n",
    "\n",
    "def convert_to_hex(color_name):\n",
    "    if isinstance(color_name, str):  # Check if color_name is a string\n",
    "        try:\n",
    "            return webcolors.name_to_hex(color_name)\n",
    "        except ValueError:\n",
    "            return custom_colors.get(color_name, '#000000')  # default to black if color name not recognized\n",
    "    else:\n",
    "        return '#000000'  # default to black if color_name is not a string\n",
    "\n",
    "# Convert the color columns to string and strip any trailing spaces\n",
    "df['color1'] = df['color1'].astype(str).str.strip()\n",
    "df['color2'] = df['color2'].astype(str).str.strip()\n",
    "\n",
    "# Convert color names to hex values\n",
    "df['color1'] = df['color1'].apply(convert_to_hex)\n",
    "df['color2'] = df['color2'].apply(convert_to_hex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recreate the division_final and level columns\n",
    "\n",
    "## If division column is not null use that value as division_final. if it is null use the value in the regional_division column\n",
    "df['division_final'] = df['division'].fillna(df['regional_div'])\n",
    "\n",
    "## Create a level column based if the field hosts a district the value should be 1\n",
    "## if region_semi_number is present assign level 2 and if region_final_number is present assign level 3\n",
    "## if finals is present assign level 4\n",
    "df['level'] = np.where(df['district'].notnull(), 1, 0)\n",
    "df['level'] = np.where(df['region_semi_number'].notnull(), 2, df['level'])\n",
    "df['level'] = np.where(df['region_final_quarter'].notnull(), 3, df['level'])\n",
    "df['level'] = np.where(df['finals'].notnull(), 4, df['level'])\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_merged = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Get me a list of every color name used in the color1 and color2 columns\n",
    "# color_list = merged_df['color1'].append(merged_df['color2']).unique()\n",
    "# color_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [],
   "source": [
    "### outpus csv to check\n",
    "df_merged.to_csv('data/MHSAA_FINAL_TEST.csv', index=False)\n",
    "\n",
    "### OUTPUT JSON TO USE IN MAP\n",
    "df_merged.to_json('data/html/mhsaa/data/map.json', orient='records')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_merged.iloc[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13.247692833272215\n",
      "13.110361954997687\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park_name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Concordia University - AA Greenhills Site</td>\n",
       "      <td>-13.247693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Kalamazoo College - college</td>\n",
       "      <td>-11.612219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Michigan State - Old College Field</td>\n",
       "      <td>-11.226785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Sagniaw Valley State - college</td>\n",
       "      <td>-10.258899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CMU - college</td>\n",
       "      <td>-10.224740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Flushing HS</td>\n",
       "      <td>-8.201347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Gaylord High School - high school</td>\n",
       "      <td>-8.134088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Cornerstone Baseball Field - college</td>\n",
       "      <td>-7.981875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Newaygo High School - high school</td>\n",
       "      <td>-7.911542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Grass Lake HS - high school</td>\n",
       "      <td>-7.448464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     park_name      score\n",
       "137  Concordia University - AA Greenhills Site -13.247693\n",
       "69                 Kalamazoo College - college -11.612219\n",
       "85          Michigan State - Old College Field -11.226785\n",
       "115             Sagniaw Valley State - college -10.258899\n",
       "23                               CMU - college -10.224740\n",
       "35                                 Flushing HS  -8.201347\n",
       "41           Gaylord High School - high school  -8.134088\n",
       "27        Cornerstone Baseball Field - college  -7.981875\n",
       "96           Newaygo High School - high school  -7.911542\n",
       "51                 Grass Lake HS - high school  -7.448464"
      ]
     },
     "execution_count": 1102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.describe()\n",
    "\n",
    "## Min and max values of score column\n",
    "print(df_merged['score'].min())\n",
    "print(df_merged['score'].max())\n",
    "\n",
    "# lowest 10 scores with field names\n",
    "df_merged.nsmallest(10, 'score')[['park_name', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park_name</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>notes</th>\n",
       "      <th>home_plate</th>\n",
       "      <th>foul_area_sqft</th>\n",
       "      <th>fop_area_sqft</th>\n",
       "      <th>field_area_sqft</th>\n",
       "      <th>foul_area_per</th>\n",
       "      <th>fair_to_foul</th>\n",
       "      <th>...</th>\n",
       "      <th>region_final_quarter</th>\n",
       "      <th>finals</th>\n",
       "      <th>nickname</th>\n",
       "      <th>color1</th>\n",
       "      <th>color2</th>\n",
       "      <th>color3</th>\n",
       "      <th>file_path</th>\n",
       "      <th>score</th>\n",
       "      <th>division_final</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Lowell High School - high school</td>\n",
       "      <td>[(-85.3763871, 42.9571353), (-85.3768716, 42.9...</td>\n",
       "      <td>[(-85.3763871, 42.9571353), (-85.3752914, 42.9...</td>\n",
       "      <td>guestimate on the fenceline based on the mow p...</td>\n",
       "      <td>(-85.3763871, 42.9571353)</td>\n",
       "      <td>29097.855748</td>\n",
       "      <td>99166.362021</td>\n",
       "      <td>128264.217769</td>\n",
       "      <td>0.226859</td>\n",
       "      <td>3.40803</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Red Arrows</td>\n",
       "      <td>#ff0000</td>\n",
       "      <td>#ffffff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data/MHSAA/assets/plots/plot_78.png</td>\n",
       "      <td>-5.695335</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           park_name  \\\n",
       "78  Lowell High School - high school   \n",
       "\n",
       "                                                 foul  \\\n",
       "78  [(-85.3763871, 42.9571353), (-85.3768716, 42.9...   \n",
       "\n",
       "                                                  fop  \\\n",
       "78  [(-85.3763871, 42.9571353), (-85.3752914, 42.9...   \n",
       "\n",
       "                                                notes  \\\n",
       "78  guestimate on the fenceline based on the mow p...   \n",
       "\n",
       "                   home_plate  foul_area_sqft  fop_area_sqft  field_area_sqft  \\\n",
       "78  (-85.3763871, 42.9571353)    29097.855748   99166.362021    128264.217769   \n",
       "\n",
       "    foul_area_per  fair_to_foul  ... region_final_quarter  finals    nickname  \\\n",
       "78       0.226859       3.40803  ...                  NaN     NaN  Red Arrows   \n",
       "\n",
       "     color1   color2  color3                            file_path     score  \\\n",
       "78  #ff0000  #ffffff     NaN  data/MHSAA/assets/plots/plot_78.png -5.695335   \n",
       "\n",
       "    division_final  level  \n",
       "78             1.0      1  \n",
       "\n",
       "[1 rows x 42 columns]"
      ]
     },
     "execution_count": 1103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged[df_merged['park_name'] == 'Lowell High School - high school']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park_name</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>notes</th>\n",
       "      <th>home_plate</th>\n",
       "      <th>foul_area_sqft</th>\n",
       "      <th>fop_area_sqft</th>\n",
       "      <th>field_area_sqft</th>\n",
       "      <th>foul_area_per</th>\n",
       "      <th>fair_to_foul</th>\n",
       "      <th>...</th>\n",
       "      <th>min_distance_rank</th>\n",
       "      <th>avg_distance_rank</th>\n",
       "      <th>median_distance_rank</th>\n",
       "      <th>field_area_rank</th>\n",
       "      <th>foul_area_rank</th>\n",
       "      <th>fop_area_per_rank</th>\n",
       "      <th>ratio_rank</th>\n",
       "      <th>fop_centroid</th>\n",
       "      <th>field_orientation</th>\n",
       "      <th>field_cardinal_direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Lowell High School - high school</td>\n",
       "      <td>[(-85.3763871, 42.9571353), (-85.3768716, 42.9...</td>\n",
       "      <td>[(-85.3763871, 42.9571353), (-85.3752914, 42.9...</td>\n",
       "      <td>guestimate on the fenceline based on the mow p...</td>\n",
       "      <td>(-85.3763871, 42.9571353)</td>\n",
       "      <td>29097.855748</td>\n",
       "      <td>99166.362021</td>\n",
       "      <td>128264.217769</td>\n",
       "      <td>0.226859</td>\n",
       "      <td>3.40803</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>(-85.3761548818182, 42.95801321363636)</td>\n",
       "      <td>10.956002</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           park_name  \\\n",
       "79  Lowell High School - high school   \n",
       "\n",
       "                                                 foul  \\\n",
       "79  [(-85.3763871, 42.9571353), (-85.3768716, 42.9...   \n",
       "\n",
       "                                                  fop  \\\n",
       "79  [(-85.3763871, 42.9571353), (-85.3752914, 42.9...   \n",
       "\n",
       "                                                notes  \\\n",
       "79  guestimate on the fenceline based on the mow p...   \n",
       "\n",
       "                   home_plate  foul_area_sqft  fop_area_sqft  field_area_sqft  \\\n",
       "79  (-85.3763871, 42.9571353)    29097.855748   99166.362021    128264.217769   \n",
       "\n",
       "    foul_area_per  fair_to_foul  ... min_distance_rank  avg_distance_rank  \\\n",
       "79       0.226859       3.40803  ...              30.0               19.0   \n",
       "\n",
       "    median_distance_rank  field_area_rank  foul_area_rank  fop_area_per_rank  \\\n",
       "79                  18.0             24.0            55.0               18.0   \n",
       "\n",
       "    ratio_rank                            fop_centroid  field_orientation  \\\n",
       "79        73.0  (-85.3761548818182, 42.95801321363636)          10.956002   \n",
       "\n",
       "    field_cardinal_direction  \n",
       "79                         N  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 1104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find Lowell High School in final Dataframe\n",
    "\n",
    "df_parks[df_parks['park_name'] == 'Lowell High School - high school']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END BLOCK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Do a FUZZY MATCH OF DF_HOSTS AND DF_PARKS\n",
    "# # Debugging step to check for non-string values in host_teams\n",
    "# for team in host_teams:\n",
    "#     if not isinstance(team, str):\n",
    "#         print(f\"Non-string value found in host_teams: {team}\")\n",
    "\n",
    "# # Debugging step to check for non-string values in park_names\n",
    "# for park in park_names:\n",
    "#     if not isinstance(park, str):\n",
    "#         print(f\"Non-string value found in park_names: {park}\")\n",
    "\n",
    "# # Continue with fuzzy matching if no non-string values are found\n",
    "# matches = [(team, process.extractOne(team, park_names)) for team in host_teams]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## List the values from the team column\n",
    "\n",
    "# print(len(df_hosts['team'].unique()))\n",
    "# df_hosts['team'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parks = df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 140 entries, 0 to 143\n",
      "Data columns (total 27 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   park_name                 140 non-null    object \n",
      " 1   foul                      140 non-null    object \n",
      " 2   fop                       140 non-null    object \n",
      " 3   notes                     7 non-null      object \n",
      " 4   home_plate                140 non-null    object \n",
      " 5   foul_area_sqft            140 non-null    float64\n",
      " 6   fop_area_sqft             140 non-null    float64\n",
      " 7   field_area_sqft           140 non-null    float64\n",
      " 8   foul_area_per             140 non-null    float64\n",
      " 9   fair_to_foul              140 non-null    float64\n",
      " 10  distances                 140 non-null    object \n",
      " 11  max_distance              140 non-null    float64\n",
      " 12  min_distance              140 non-null    float64\n",
      " 13  avg_distance              140 non-null    float64\n",
      " 14  median_distance           140 non-null    float64\n",
      " 15  num_distances             140 non-null    int64  \n",
      " 16  max_distance_rank         140 non-null    float64\n",
      " 17  min_distance_rank         140 non-null    float64\n",
      " 18  avg_distance_rank         140 non-null    float64\n",
      " 19  median_distance_rank      140 non-null    float64\n",
      " 20  field_area_rank           140 non-null    float64\n",
      " 21  foul_area_rank            140 non-null    float64\n",
      " 22  fop_area_per_rank         140 non-null    float64\n",
      " 23  ratio_rank                140 non-null    float64\n",
      " 24  fop_centroid              140 non-null    object \n",
      " 25  field_orientation         140 non-null    float64\n",
      " 26  field_cardinal_direction  140 non-null    object \n",
      "dtypes: float64(18), int64(1), object(8)\n",
      "memory usage: 30.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 146 entries, 0 to 145\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   park_name             145 non-null    object \n",
      " 1   host_team             144 non-null    object \n",
      " 2   division              142 non-null    float64\n",
      " 3   district              128 non-null    float64\n",
      " 4   region_semi_number    64 non-null     float64\n",
      " 5   regional_div          78 non-null     float64\n",
      " 6   region_final_quarter  16 non-null     float64\n",
      " 7   finals                1 non-null      float64\n",
      " 8   nickname              143 non-null    object \n",
      " 9   color1                143 non-null    object \n",
      " 10  color2                143 non-null    object \n",
      " 11  color3                14 non-null     object \n",
      "dtypes: float64(6), object(6)\n",
      "memory usage: 13.8+ KB\n"
     ]
    }
   ],
   "source": [
    "parks_df = df_cleaned.copy()\n",
    "\n",
    "parks_df.info()\n",
    "host_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(parks_df, host_df, min_score=90):\n",
    "    dict_list = []\n",
    "    unmatched_rows = []\n",
    "    # Drop NaN values in 'team' and 'park_name' before matching\n",
    "    host_names = host_df.team.dropna().unique()\n",
    "    for name in parks_df.park_name.dropna():  # ignore NaN values\n",
    "        match = match_team(name, host_names, min_score)\n",
    "        \n",
    "        # If no match found, add to unmatched_rows and continue to next iteration\n",
    "        if match[0] == \"\":\n",
    "            unmatched_rows.append(name)\n",
    "            continue\n",
    "\n",
    "        dict_ = {}\n",
    "        dict_.update({\"park_name_parks\" : name})\n",
    "        dict_.update({\"match_name_host\" : match[0]})\n",
    "        dict_.update({\"score\" : match[1]})\n",
    "        dict_list.append(dict_)\n",
    "\n",
    "    merge_table = pd.DataFrame(dict_list)\n",
    "    \n",
    "    # Remove duplicates in merge_table, keeping only the row with the highest score\n",
    "    merge_table = merge_table.sort_values('score', ascending=False).drop_duplicates(['park_name_parks'], keep='first')\n",
    "\n",
    "    if 'match_name_host' in merge_table.columns:\n",
    "        merged_df = pd.merge(parks_df, merge_table, left_on='park_name', right_on='park_name_parks', how='left')\n",
    "        merged_df = pd.merge(merged_df, host_df, left_on='match_name_host', right_on='team', how='left')\n",
    "    else:\n",
    "        print(\"No matches found.\")\n",
    "        merged_df = None\n",
    "\n",
    "    return merged_df, unmatched_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = merged_df.sort_values('score', ascending=False).drop_duplicates(subset=['park_name', 'team'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'team'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Justin\\Desktop\\Project\\BB_parks\\mhsaa_etl.ipynb Cell 53\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/mhsaa_etl.ipynb#X64sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m merged_df, unmatched_rows \u001b[39m=\u001b[39m merge_dataframes(parks_df, host_df, min_score\u001b[39m=\u001b[39;49m\u001b[39m90\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/mhsaa_etl.ipynb#X64sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m merged_df\u001b[39m.\u001b[39minfo()\n",
      "\u001b[1;32mc:\\Users\\Justin\\Desktop\\Project\\BB_parks\\mhsaa_etl.ipynb Cell 53\u001b[0m in \u001b[0;36mmerge_dataframes\u001b[1;34m(parks_df, host_df, min_score)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/mhsaa_etl.ipynb#X64sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m unmatched_rows \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/mhsaa_etl.ipynb#X64sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Drop NaN values in 'team' and 'park_name' before matching\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/mhsaa_etl.ipynb#X64sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m host_names \u001b[39m=\u001b[39m host_df\u001b[39m.\u001b[39;49mteam\u001b[39m.\u001b[39mdropna()\u001b[39m.\u001b[39munique()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/mhsaa_etl.ipynb#X64sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m parks_df\u001b[39m.\u001b[39mpark_name\u001b[39m.\u001b[39mdropna():  \u001b[39m# ignore NaN values\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/mhsaa_etl.ipynb#X64sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     match \u001b[39m=\u001b[39m match_team(name, host_names, min_score)\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5568\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[0;32m   5574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5575\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'team'"
     ]
    }
   ],
   "source": [
    "merged_df, unmatched_rows = merge_dataframes(parks_df, host_df, min_score=90)\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we first match each team name in host_df with the park names in parks_df. If a match with a similarity score greater than the threshold (85 in your case) is found, we record the match in merge_table. If no match is found, we record the team name in unmatched_rows. After going through all team names, we merge host_df and parks_df based on the matches in merge_table.\n",
    "\n",
    "The function merge_dataframes returns two objects. The first object, merged_df, is a DataFrame that contains the merged data. The second object, unmatched_rows, is a list of team names in host_df for which no match in parks_df could be found. You can inspect unmatched_rows to see which rows couldn't be matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(parks_df, host_df, min_score=90):\n",
    "    dict_list = []\n",
    "    unmatched_rows = []\n",
    "    # Drop NaN values in 'team' and 'park_name' before matching\n",
    "    host_names = host_df.team.dropna().unique()\n",
    "    for name in parks_df.park_name.dropna():  # ignore NaN values\n",
    "        match = match_team(name, host_names, min_score)\n",
    "        \n",
    "        # If no match found, add to unmatched_rows and continue to next iteration\n",
    "        if match[0] == \"\":\n",
    "            unmatched_rows.append(name)\n",
    "            continue\n",
    "\n",
    "        dict_ = {}\n",
    "        dict_.update({\"park_name_parks\" : name})\n",
    "        dict_.update({\"match_name_host\" : match[0]})\n",
    "        dict_.update({\"score\" : match[1]})\n",
    "        dict_list.append(dict_)\n",
    "\n",
    "    merge_table = pd.DataFrame(dict_list)\n",
    "    \n",
    "    # Remove duplicates in merge_table, keeping only the row with the highest score\n",
    "    merge_table = merge_table.sort_values('score', ascending=False).drop_duplicates(['park_name_parks'], keep='first')\n",
    "\n",
    "    if 'match_name_host' in merge_table.columns:\n",
    "        merged_df = pd.merge(parks_df, merge_table, left_on='park_name', right_on='park_name_parks', how='left')\n",
    "        merged_df = pd.merge(merged_df, host_df, left_on='match_name_host', right_on='team', how='left')\n",
    "    else:\n",
    "        print(\"No matches found.\")\n",
    "        merged_df = None\n",
    "\n",
    "    return merged_df, unmatched_rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORKING HERE DOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matching Function to compair host names to park names \n",
    "\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "def find_host_matches(host_df, parks_df):\n",
    "    # Initialize empty lists to store the matches and unmatched park names\n",
    "    matches = []\n",
    "    unmatched_park_names = []\n",
    "\n",
    "    # Iterate over each host and district in the host_df\n",
    "    for host, district in zip(host_df['team'], host_df['district']):\n",
    "        # Use fuzzy matching to find potential matches in park names\n",
    "        potential_matches = process.extractBests(host, parks_df['park_name'], scorer=fuzz.token_set_ratio, score_cutoff=80)\n",
    "\n",
    "        # Store the host, district, and potential matches with their scores\n",
    "        matches.append({'host': host, 'district': district, 'potential_matches': potential_matches})\n",
    "\n",
    "        # Check if any strong matches were found\n",
    "        if len(potential_matches) > 0:\n",
    "            max_score = max(potential_matches, key=lambda x: x[1])[1]\n",
    "            if max_score >= 80:\n",
    "                continue\n",
    "\n",
    "        # If no strong matches were found, add the park name to unmatched list\n",
    "        unmatched_park_names.append((host, district))\n",
    "\n",
    "    # Create a dataframe from the matches list\n",
    "    matches_df = pd.DataFrame(matches)\n",
    "\n",
    "    # Count the number of strong matches and unmatched park names\n",
    "    strong_matches_count = matches_df['potential_matches'].apply(lambda x: sum(match[1] >= 80 for match in x)).sum()\n",
    "    unmatched_count = len(unmatched_park_names)\n",
    "\n",
    "    print(\"Number of strong matches:\", strong_matches_count)\n",
    "    print(\"Number of unmatched park names:\", unmatched_count)\n",
    "    print(\"Unmatched park names:\", unmatched_park_names)\n",
    "\n",
    "    return matches_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_df.info()\n",
    "parks_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to find host matches\n",
    "result_df = find_host_matches(host_df, parks_df)\n",
    "\n",
    "# # Merge the result_df to messe_df on district number\n",
    "# merged_df = result_df.merge(messe_df, left_on='district', right_on='district number')\n",
    "\n",
    "# # Select the desired column'region_semi_numbers\n",
    "# merged_df = merged_df[['host', 'division', 'district', 'region_semi_number', 'region_final_quarter', 'finals', 'potential_matches', 'Plot Note', 'Map Link MHSAA']]\n",
    "# merged_df.head()\n",
    "# # merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Keep only the field name from the potential matches, ignore if the potential match is empty\n",
    "merged_df['potential_matches'] = merged_df['potential_matches'].apply(lambda x: x[0][0] if len(x) > 0 else np.nan)\n",
    "\n",
    "\n",
    "# merged_df['potential_matches'] = merged_df['potential_matches'].apply(lambda x: x[0][0])\n",
    "\n",
    "# Rename the potential_matches column to park_name\n",
    "\n",
    "merged_df.rename(columns={'potential_matches': 'park_name'}, inplace=True)\n",
    "\n",
    "\n",
    "merged_df.head()\n",
    "merged_df.info()\n",
    "# # Merge the merged_df to the parks_df on park_name\n",
    "\n",
    "# parks_df = parks_df.merge(merged_df, on='park_name', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge the merged_df to the parks_df on park_name\n",
    "parks_df = parks_df.merge(merged_df, on='park_name', how='left')\n",
    "\n",
    "parks_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_df.info()\n",
    "parks_df.head()\n",
    "\n",
    "# host_df.info()\n",
    "# host_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here down are simple plots to do spot check of data and hold example of polar chart\n",
    "\n",
    "### FILL IN THE REST OF JSON WITH THE DATA FOR THE 2023 TOURNEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pull the host team information into the parks_df\n",
    "parks_df = parks_df.merge(host_df, on='host', how='left')\n",
    "\n",
    "### Create columns for tournament levels\n",
    "# District is done, Need regional semi, regional final, quarter final, final_four\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parks_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Build the file path using os.path.join\n",
    "file_path = os.path.join('data', 'html', 'mhsaa', 'data', 'tourney_2023.json')\n",
    "\n",
    "# Save the dataframe to JSON using the constructed file path\n",
    "parks_df.to_json(file_path, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the max distance, min distance, average distance, and median distance\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "ax[0, 0].hist(df_cleaned['max_distance'], bins=20)\n",
    "\n",
    "ax[0, 1].hist(df_cleaned['min_distance'], bins=20)\n",
    "\n",
    "ax[1, 0].hist(df_cleaned['avg_distance'], bins=20)\n",
    "\n",
    "ax[1, 1].hist(df_cleaned['median_distance'], bins=20)\n",
    "\n",
    "ax[0, 0].set_title('Max Distance')\n",
    "ax[0, 1].set_title('Min Distance')\n",
    "\n",
    "ax[1, 0].set_title('Average Distance')\n",
    "ax[1, 1].set_title('Median Distance')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile a list of fields that are outliers\n",
    "\n",
    "outlier_fields = df_cleaned[(df_cleaned['max_distance'] > 400) | (df_cleaned['min_distance'] < 200) | (df_cleaned['avg_distance'] > 400) | (df_cleaned['median_distance'] > 400)]\n",
    "\n",
    "len(outlier_fields)\n",
    "\n",
    "print(outlier_fields['park_name'].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of the top and bottom ten from each category\n",
    "\n",
    "top_ten_max = df_cleaned.sort_values(by='max_distance', ascending=False).head(10)\n",
    "top_ten_min = df_cleaned.sort_values(by='min_distance', ascending=True).head(10)\n",
    "\n",
    "top_ten_avg = df_cleaned.sort_values(by='avg_distance', ascending=False).head(10)\n",
    "top_ten_median = df_cleaned.sort_values(by='median_distance', ascending=False).head(10)\n",
    "\n",
    "top_ten_field_area = df_cleaned.sort_values(by='field_area_sqft', ascending=False).head(10)\n",
    "\n",
    "top_ten_foul_area = df_cleaned.sort_values(by='foul_area_sqft', ascending=False).head(10)\n",
    "\n",
    "top_ten_fop_area = df_cleaned.sort_values(by='fop_area_sqft', ascending=False).head(10)\n",
    "\n",
    "top_ten_ratio = df_cleaned.sort_values(by='fair_to_foul', ascending=False).head(10)\n",
    "\n",
    "bottom_ten_ratio = df_cleaned.sort_values(by='fair_to_foul', ascending=True).head(10)\n",
    "\n",
    "bottom_ten_max = df_cleaned.sort_values(by='max_distance', ascending=True).head(10)\n",
    "bottom_ten_min = df_cleaned.sort_values(by='min_distance', ascending=False).head(10)\n",
    "bottom_ten_avg = df_cleaned.sort_values(by='avg_distance', ascending=True).head(10)\n",
    "bottom_ten_median = df_cleaned.sort_values(by='median_distance', ascending=True).head(10)\n",
    "\n",
    "\n",
    "### Create and display a dataframe with columns for the top and bottom ten fields for each category\n",
    "\n",
    "top_bottom_df = pd.DataFrame()\n",
    "\n",
    "top_bottom_df['top_ten_max'] = top_ten_max['park_name'].values\n",
    "top_bottom_df['top_ten_min'] = top_ten_min['park_name'].values\n",
    "top_bottom_df['top_ten_avg'] = top_ten_avg['park_name'].values\n",
    "\n",
    "top_bottom_df['top_ten_median'] = top_ten_median['park_name'].values\n",
    "top_bottom_df['top_ten_field_area'] = top_ten_field_area['park_name'].values\n",
    "top_bottom_df['top_ten_foul_area'] = top_ten_foul_area['park_name'].values\n",
    "top_bottom_df['top_ten_fop_area'] = top_ten_fop_area['park_name'].values\n",
    "top_bottom_df['top_ten_ratio'] = top_ten_ratio['park_name'].values\n",
    "\n",
    "top_bottom_df['bottom_ten_ratio'] = bottom_ten_ratio['park_name'].values\n",
    "top_bottom_df['bottom_ten_max'] = bottom_ten_max['park_name'].values\n",
    "top_bottom_df['bottom_ten_min'] = bottom_ten_min['park_name'].values\n",
    "top_bottom_df['bottom_ten_avg'] = bottom_ten_avg['park_name'].values\n",
    "top_bottom_df['bottom_ten_median'] = bottom_ten_median['park_name'].values\n",
    "\n",
    "\n",
    "top_bottom_df.head(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW WITH AUTO SCALING\n",
    "\n",
    "def calculate_max_y(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    return max(bin_counts)\n",
    "\n",
    "\n",
    "def create_polar_chart(data, num_bins=36, level_filter=None, y_min=-20, background_color='#2b2b2b', color_map=plt.cm.viridis, bar_alpha=0.8):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 2 * np.pi / num_bins\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "\n",
    "    ax.set_facecolor('#808080')\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "    # Set dark background\n",
    "    ax.set_facecolor(background_color)\n",
    "    plt.gca().set_rlabel_position(22.5)\n",
    "    y_max = calculate_max_y(data, num_bins=num_bins, level_filter=level_filter) + 5\n",
    "    ax.set_ylim(y_min, y_max)  # Adjust based on max count\n",
    "\n",
    "    # Add bars for negative values\n",
    "    zero_height_bars = ax.bar(bin_edges[:-1], np.abs(ax.get_ylim()[0]), width=bin_width, bottom=0.0, color='k', alpha=0.3)\n",
    "\n",
    "    bars = ax.bar(bin_edges[:-1], bin_counts, width=bin_width, bottom=0)\n",
    "    \n",
    "    # Use custom colors and opacity\n",
    "    for r, bar in zip(bin_counts, bars):\n",
    "        bar.set_facecolor(color_map(r / max(bin_counts)))\n",
    "        bar.set_alpha(bar_alpha)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CALL AUTO ADJUSTING CHART #####\n",
    "\n",
    "\n",
    "## NEW PERAMS\n",
    "\n",
    "\n",
    "# Call your function\n",
    "create_polar_chart(\n",
    "    data, \n",
    "    num_bins=30, \n",
    "    # level_filter=\"level1\", \n",
    "    y_min=0, \n",
    "    background_color='#2b2b2b', \n",
    "    color_map=plt.cm.viridis, \n",
    "    bar_alpha=0.8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW CHAT GPT CODE\n",
    "\n",
    "def create_polar_chart(data, num_bins=36, level_filter=None, y_min=-20, y_max=130, background_color='#2b2b2b', color_map=plt.cm.viridis, bar_alpha=0.8):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 2 * np.pi / num_bins\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "\n",
    "    ax.set_facecolor('#808080')\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "    # Set dark background\n",
    "    ax.set_facecolor(background_color)\n",
    "    plt.gca().set_rlabel_position(22.5)\n",
    "    ax.set_ylim(y_min, y_max)  # Adjust based on max count\n",
    "\n",
    "    # Add bars for negative values\n",
    "    zero_height_bars = ax.bar(bin_edges[:-1], np.abs(ax.get_ylim()[0]), width=bin_width, bottom=0.0, color='k', alpha=0.3)\n",
    "\n",
    "    bars = ax.bar(bin_edges[:-1], bin_counts, width=bin_width, bottom=0)\n",
    "    \n",
    "    # Use custom colors and opacity\n",
    "    for r, bar in zip(bin_counts, bars):\n",
    "        bar.set_facecolor(color_map(r / max(bin_counts)))\n",
    "        bar.set_alpha(bar_alpha)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a polar chart showing the direction of all the tournment fields\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a function to process the data, counting the orientations and filtering by level.\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_data(data, level_filter=None):\n",
    "    count_by_orientation = defaultdict(int)\n",
    "    \n",
    "    for record in data:\n",
    "        if level_filter is None or record['level'] == level_filter:\n",
    "            orientation = round(record['field_orientation'])\n",
    "            count_by_orientation[orientation] += 1\n",
    "\n",
    "    return count_by_orientation\n",
    "\n",
    "def create_polar_chart(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 2 * np.pi / num_bins\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "\n",
    "    ax.set_facecolor('#808080')\n",
    "    ###\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "    \n",
    "    # # Set dark background\n",
    "    ax.set_facecolor('#2b2b2b')\n",
    "    plt.gca().set_rlabel_position(22.5)\n",
    "    ax.set_ylim(-20, 130)  # Adjust based on max count\n",
    "\n",
    "    # Add bars for negative values\n",
    "    zero_height_bars = ax.bar(bin_edges[:-1], np.abs(ax.get_ylim()[0]), width=bin_width, bottom=0.0, color='k', alpha=0.3)\n",
    "\n",
    "    bars = ax.bar(bin_edges[:-1], bin_counts, width=bin_width, bottom=0)\n",
    "    \n",
    "    # Use custom colors and opacity\n",
    "    for r, bar in zip(bin_counts, bars):\n",
    "        bar.set_facecolor(plt.cm.viridis(r / max(bin_counts)))\n",
    "        # bar.set_facecolor(plt.cm.plasma(r / max(bin_counts)))\n",
    "        bar.set_alpha(0.8)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_polar_chart(data, num_bins=50, level_filter=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
