{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ETL NOTEBOOK FOR 2023 MHSAA TOURNEY SPECIFIC MAP\n",
    "\n",
    "#### Adapted from ETL for JSON\n",
    "\n",
    "## Dependencies and Setup\n",
    "### Dependencies\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Start timer\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD BLOCK###\n",
    "#### Load data from kml file exported from Google Earth\n",
    "\n",
    "file_path = ('TEMP/clean_tables/geo_data/all_high_schools.kml') # file path to kml file\n",
    "\n",
    "\n",
    "# Read the KML file\n",
    "with open(file_path) as file:\n",
    "    xml_data = file.read()\n",
    "\n",
    "# Initialize soup variables for parsing file\n",
    "soup = BeautifulSoup(xml_data, 'xml')\n",
    "folders = soup.Document.Folder\n",
    "list = soup.Document.Folder.find_all('Folder')\n",
    "\n",
    "# Create a list to store rows to append to the DataFrame\n",
    "rows = []\n",
    "\n",
    "# Loop through the folders and extract the data\n",
    "for folder in list:\n",
    "    try:\n",
    "        field_name = folder.find('name').text\n",
    "        foul = folder.find_all('coordinates')[0].text\n",
    "        fop = folder.find_all('coordinates')[1].text\n",
    "        notes = None\n",
    "\n",
    "        # Check if there is a description tag, if so, use it for notes\n",
    "        if folder.find('description') is not None:\n",
    "            notes = folder.find('description').text\n",
    "\n",
    "        row = {\n",
    "            'field': field_name,\n",
    "            'foul': foul,\n",
    "            'fop': fop,\n",
    "            'notes': notes\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Add name of folder to a list of failed folders\n",
    "        failed.append(folder.find('name').text)\n",
    "        print(f\"Error processing folder: {folder.find('name').text}. Error message: {str(e)}\")\n",
    "\n",
    "# Convert the list of rows to a DataFrame\n",
    "df = pd.DataFrame(rows, columns=['field', 'foul', 'fop', 'notes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ann Arbor Greenhills HS? - Practice?</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.01481943219841,41.866940...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.01374704390223,41.867203...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adams Butzel Complex - Detroit Communication M...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-83.1678186,42.3966942,0 -83...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-83.1678186,42.3966942,0 -83...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Addison HS</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.3395166,41.9898236,0 -84...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.3395166,41.9898236,0 -84...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adrian HS</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.0416584,41.9091676,0 -84...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.0416584,41.9091676,0 -84...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adrian Lenawee Christian HS</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.08374689999999,41.908766...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.08374689999999,41.908766...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               field  \\\n",
       "0               Ann Arbor Greenhills HS? - Practice?   \n",
       "1  Adams Butzel Complex - Detroit Communication M...   \n",
       "2                                         Addison HS   \n",
       "3                                          Adrian HS   \n",
       "4                        Adrian Lenawee Christian HS   \n",
       "\n",
       "                                                foul  \\\n",
       "0  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.01481943219841,41.866940...   \n",
       "1  \\n\\t\\t\\t\\t\\t\\t\\t\\t-83.1678186,42.3966942,0 -83...   \n",
       "2  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.3395166,41.9898236,0 -84...   \n",
       "3  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.0416584,41.9091676,0 -84...   \n",
       "4  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.08374689999999,41.908766...   \n",
       "\n",
       "                                                 fop notes  \n",
       "0  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.01374704390223,41.867203...  None  \n",
       "1  \\n\\t\\t\\t\\t\\t\\t\\t\\t-83.1678186,42.3966942,0 -83...  None  \n",
       "2  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.3395166,41.9898236,0 -84...  None  \n",
       "3  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.0416584,41.9091676,0 -84...  None  \n",
       "4  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.08374689999999,41.908766...  None  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the new dataframe\n",
    "\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Remove new line and space characters from coordinates\n",
    "df_cleaned = df_cleaned.replace(r'\\n','', regex=True) \n",
    "df_cleaned = df_cleaned.replace(r'\\t','', regex=True) \n",
    "\n",
    "# Drop any duplicate rows\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['field'], keep='first')\n",
    "\n",
    "# Drop any rows with empty fields\n",
    "df_cleaned = df_cleaned[(df_cleaned != 0).all(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 494 entries, 0 to 499\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   field   494 non-null    object\n",
      " 1   foul    494 non-null    object\n",
      " 2   fop     494 non-null    object\n",
      " 3   notes   23 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 19.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ann Arbor Greenhills HS? - Practice?</td>\n",
       "      <td>-84.01481943219841,41.86694040616026,0 -84.015...</td>\n",
       "      <td>-84.01374704390223,41.86720354412398,0 -84.013...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adams Butzel Complex - Detroit Communication M...</td>\n",
       "      <td>-83.1678186,42.3966942,0 -83.1678776,42.397648...</td>\n",
       "      <td>-83.1678186,42.3966942,0 -83.1665385,42.396724...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Addison HS</td>\n",
       "      <td>-84.3395166,41.9898236,0 -84.3406566,41.989848...</td>\n",
       "      <td>-84.3395166,41.9898236,0 -84.3394898,41.990659...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adrian HS</td>\n",
       "      <td>-84.0416584,41.9091676,0 -84.04166909999999,41...</td>\n",
       "      <td>-84.0416584,41.9091676,0 -84.0405493,41.909184...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adrian Lenawee Christian HS</td>\n",
       "      <td>-84.08374689999999,41.9087669,0 -84.0841116999...</td>\n",
       "      <td>-84.08374689999999,41.9087669,0 -84.0826392,41...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               field  \\\n",
       "0               Ann Arbor Greenhills HS? - Practice?   \n",
       "1  Adams Butzel Complex - Detroit Communication M...   \n",
       "2                                         Addison HS   \n",
       "3                                          Adrian HS   \n",
       "4                        Adrian Lenawee Christian HS   \n",
       "\n",
       "                                                foul  \\\n",
       "0  -84.01481943219841,41.86694040616026,0 -84.015...   \n",
       "1  -83.1678186,42.3966942,0 -83.1678776,42.397648...   \n",
       "2  -84.3395166,41.9898236,0 -84.3406566,41.989848...   \n",
       "3  -84.0416584,41.9091676,0 -84.04166909999999,41...   \n",
       "4  -84.08374689999999,41.9087669,0 -84.0841116999...   \n",
       "\n",
       "                                                 fop notes  \n",
       "0  -84.01374704390223,41.86720354412398,0 -84.013...  None  \n",
       "1  -83.1678186,42.3966942,0 -83.1665385,42.396724...  None  \n",
       "2  -84.3395166,41.9898236,0 -84.3394898,41.990659...  None  \n",
       "3  -84.0416584,41.9091676,0 -84.0405493,41.909184...  None  \n",
       "4  -84.08374689999999,41.9087669,0 -84.0826392,41...  None  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.info()\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Clean up polygon data and create a new home_plate column\n",
    "\n",
    "def parse_coordinates(coord_string):\n",
    "    coords = coord_string.split()\n",
    "    parsed_coords = [tuple(map(float, coord.split(',')[:2])) for coord in coords]\n",
    "    return parsed_coords\n",
    "\n",
    "# Create a new column for the home_plate location using the first set of coordinates in the 'fop' column\n",
    "df_cleaned['home_plate'] = df_cleaned['fop'].apply(lambda x: parse_coordinates(x)[0])\n",
    "\n",
    "# Apply the parse_coordinates function to the 'foul' and 'fop' columns\n",
    "df_cleaned['foul'] = df_cleaned['foul'].apply(parse_coordinates)\n",
    "df_cleaned['fop'] = df_cleaned['fop'].apply(parse_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## AREA CALCULATION ##############\n",
    "\n",
    "\n",
    "import pyproj\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import transform\n",
    "\n",
    "\n",
    "def calculate_area(coords):\n",
    "    # Create a Polygon object from the coordinates\n",
    "    polygon = Polygon(coords)\n",
    "\n",
    "    # Calculate the centroid of the polygon\n",
    "    centroid = polygon.centroid\n",
    "\n",
    "    # Create a custom LAEA projection centered on the centroid\n",
    "    custom_projection = f\"+proj=laea +lat_0={centroid.y} +lon_0={centroid.x} +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"\n",
    "\n",
    "    # Create a transformer for converting coordinates to the custom LAEA projection\n",
    "    transformer = pyproj.Transformer.from_crs(\n",
    "        pyproj.CRS(\"EPSG:4326\"),  # WGS 84 (latitude and longitude)\n",
    "        pyproj.CRS(custom_projection),  # Custom LAEA projection\n",
    "        always_xy=True\n",
    "    )\n",
    "\n",
    "    # Define a function to transform coordinates using the transformer\n",
    "    def transform_coordinates(x, y):\n",
    "        return transformer.transform(x, y)\n",
    "\n",
    "    # Convert the coordinates to the custom LAEA projection\n",
    "    polygon_laea = transform(transform_coordinates, polygon)\n",
    "\n",
    "    # Calculate the area in square meters\n",
    "    area_sqm = polygon_laea.area\n",
    "\n",
    "    # Convert the area to square feet (1 square meter = 10.764 square feet)\n",
    "    area_sqft = area_sqm * 10.764\n",
    "\n",
    "    return area_sqft\n",
    "\n",
    "\n",
    "\n",
    "### Call Function and add to dataframe\n",
    "df_cleaned['foul_area_sqft'] = df_cleaned['foul'].apply(calculate_area)\n",
    "df_cleaned['fop_area_sqft'] = df_cleaned['fop'].apply(calculate_area)\n",
    "\n",
    "## Calculate the total area of the field and the ratio of foul area to field area\n",
    "df_cleaned['field_area_sqft'] = df_cleaned['foul_area_sqft'] + df_cleaned['fop_area_sqft']\n",
    "## Percentage foul area\n",
    "df_cleaned['foul_area_per'] = df_cleaned['foul_area_sqft'] / df_cleaned['field_area_sqft']\n",
    "## Fair to Foul Ratio\n",
    "df_cleaned['fair_to_foul'] = df_cleaned['fop_area_sqft'] / df_cleaned['foul_area_sqft']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# FENCE DISTANCE CALCULATION #############\n",
    "\n",
    "from geopy.distance import great_circle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def interpolate_points(start, end, length_ratio):\n",
    "    start_np = np.array(start)\n",
    "    end_np = np.array(end)\n",
    "    return tuple(start_np + (end_np - start_np) * length_ratio)\n",
    "\n",
    "def calculate_distances(home_plate, outfield_coords, num_points=540):\n",
    "    def is_same_point(point1, point2, tolerance=1e-6):\n",
    "        return abs(point1[0] - point2[0]) < tolerance and abs(point1[1] - point2[1]) < tolerance\n",
    "\n",
    "    home_plate_lat_lon = (home_plate[1], home_plate[0])\n",
    "    distances = []\n",
    "\n",
    "    # Calculate total line length\n",
    "    total_length = 0\n",
    "    segments = []\n",
    "    for i in range(len(outfield_coords) - 1):\n",
    "        start = outfield_coords[i]\n",
    "        end = outfield_coords[i + 1]\n",
    "        if not is_same_point(home_plate, start) and not is_same_point(home_plate, end):\n",
    "            segment_length = great_circle((start[1], start[0]), (end[1], end[0])).feet\n",
    "            segments.append((start, end, segment_length))\n",
    "            total_length += segment_length\n",
    "\n",
    "    # Calculate the distance between equally spaced points\n",
    "    spacing = total_length / (num_points - 1)\n",
    "\n",
    "    # Interpolate points and calculate distances\n",
    "    current_length = 0\n",
    "    segment_index = 0\n",
    "    for i in range(num_points):\n",
    "        while segment_index < len(segments) - 1 and current_length > segments[segment_index][2]:\n",
    "            current_length -= segments[segment_index][2]\n",
    "            segment_index += 1\n",
    "\n",
    "        start, end, segment_length = segments[segment_index]\n",
    "        length_ratio = current_length / segment_length\n",
    "        point = interpolate_points(start, end, length_ratio)\n",
    "        distance = great_circle(home_plate_lat_lon, (point[1], point[0])).feet\n",
    "        distances.append(distance)\n",
    "\n",
    "        current_length += spacing\n",
    "\n",
    "    return distances\n",
    "\n",
    "# Calculate distances for each row\n",
    "df_cleaned['distances'] = df_cleaned.apply(lambda row: calculate_distances(row['home_plate'], row['fop']), axis=1)\n",
    "\n",
    "# Calculate max, min, and average distances for each row\n",
    "df_cleaned['max_distance'] = df_cleaned['distances'].apply(max)\n",
    "df_cleaned['min_distance'] = df_cleaned['distances'].apply(min)\n",
    "df_cleaned['avg_distance'] = df_cleaned['distances'].apply(lambda distances: sum(distances) / len(distances))\n",
    "# get the median distance\n",
    "df_cleaned['median_distance'] = df_cleaned['distances'].apply(lambda distances: np.median(distances))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540    494\n",
       "Name: num_distances, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## CHECK BLOCK ########\n",
    "\n",
    "## Check how long the distance list is for each row\n",
    "df_cleaned['num_distances'] = df_cleaned['distances'].apply(len)\n",
    "\n",
    "## Print the value counts for the 'num_distances' column\n",
    "df_cleaned['num_distances'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create ranks for each column\n",
    "\n",
    "def rank_fields(df):\n",
    "    # Calculate the rank for each category\n",
    "    df['max_distance_rank'] = df['max_distance'].rank(ascending=False, method='min')\n",
    "    df['min_distance_rank'] = df['min_distance'].rank(ascending=False, method='min')\n",
    "    df['avg_distance_rank'] = df['avg_distance'].rank(ascending=False, method='min')\n",
    "    df['median_distance_rank'] = df['median_distance'].rank(ascending=False, method='min')\n",
    "    df['field_area_rank'] = df['field_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['foul_area_rank'] = df['foul_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['fop_area_per_rank'] = df['fop_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['ratio_rank'] = df['fair_to_foul'].rank(ascending=False, method='min')\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Function\n",
    "\n",
    "df_cleaned = rank_fields(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Orienting the map to the home plate location ####\n",
    "\n",
    "### Find the center of the field\n",
    "def calculate_centroid(coords):\n",
    "    x_coords = [coord[0] for coord in coords]\n",
    "    y_coords = [coord[1] for coord in coords]\n",
    "    centroid_x = sum(x_coords) / len(coords)\n",
    "    centroid_y = sum(y_coords) / len(coords)\n",
    "    return (centroid_x, centroid_y)\n",
    "\n",
    "\n",
    "## Find the bearing between the home plate and the center of the field\n",
    "import math\n",
    "\n",
    "def calculate_bearing(point1, point2):\n",
    "    lat1, lon1 = math.radians(point1[1]), math.radians(point1[0])\n",
    "    lat2, lon2 = math.radians(point2[1]), math.radians(point2[0])\n",
    "\n",
    "    d_lon = lon2 - lon1\n",
    "\n",
    "    x = math.cos(lat2) * math.sin(d_lon)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(d_lon)\n",
    "\n",
    "    bearing = math.degrees(math.atan2(x, y))\n",
    "    bearing = (bearing + 360) % 360  # Normalize the bearing to the range [0, 360)\n",
    "\n",
    "    return bearing\n",
    "\n",
    "### Function to classify direction in laymans terms North, South, East, West, ect\n",
    "def degrees_to_cardinal_direction(degrees):\n",
    "    directions = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW', 'N']\n",
    "    index = round(degrees / 45)\n",
    "    return directions[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the centroid of the outfield fence coordinates for each row\n",
    "df_cleaned['fop_centroid'] = df_cleaned['fop'].apply(lambda coords: calculate_centroid(coords[1:]))\n",
    "\n",
    "# Calculate the bearing between home plate and the centroid for each row\n",
    "df_cleaned['field_orientation'] = df_cleaned.apply(lambda row: calculate_bearing(row['home_plate'], row['fop_centroid']), axis=1)\n",
    "\n",
    "# Convert the bearing to a cardinal direction\n",
    "df_cleaned['field_cardinal_direction'] = df_cleaned['field_orientation'].apply(degrees_to_cardinal_direction)\n",
    "\n",
    "# rename 'field' to 'park_name'\n",
    "df_cleaned.rename(columns={'field': 'park_name'}, inplace=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Need to rename dataframe to df for this block \n",
    "\n",
    "# df = df_cleaned\n",
    "\n",
    "# ### Get the Altitiude of each field as well as city and state\n",
    "# ### This block will take a while to run, can process about 2 seconds per record\n",
    "\n",
    "# ## Get Altitudes of the ballparks\n",
    "# ## Get Altitudes of the ballparks\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# api_key = 'AIzaSyA_BhlTupRdBPBhRptQuR6pYorMVYQnRMA'\n",
    "\n",
    "# def get_altitude(lat, lon):\n",
    "#     query = f'https://maps.googleapis.com/maps/api/elevation/json?locations={lat},{lon}&key={api_key}'\n",
    "#     try:\n",
    "#         r = requests.get(query)\n",
    "#         r.raise_for_status()\n",
    "#         data = r.json()\n",
    "#         return data['results'][0]['elevation']\n",
    "#     except requests.exceptions.RequestException as err:\n",
    "#         print(f\"Request error: {err}\")\n",
    "#         return None\n",
    "#     except KeyError:\n",
    "#         print(f\"No results returned for coordinates: {lat}, {lon}\")\n",
    "#         return None\n",
    "\n",
    "# def get_city_state(lat, lon):\n",
    "#     query = f'https://maps.googleapis.com/maps/api/geocode/json?latlng={lat},{lon}&key={api_key}'\n",
    "#     try:\n",
    "#         r = requests.get(query)\n",
    "#         r.raise_for_status()\n",
    "#         data = r.json()\n",
    "#         results = data['results'][0]['address_components']\n",
    "#         city = next((item['long_name'] for item in results if 'locality' in item['types']), '')\n",
    "#         state = next((item['long_name'] for item in results if 'administrative_area_level_1' in item['types']), '')\n",
    "#         return city, state\n",
    "#     except requests.exceptions.RequestException as err:\n",
    "#         print(f\"Request error: {err}\")\n",
    "#         return None, None\n",
    "#     except KeyError:\n",
    "#         print(f\"No results returned for coordinates: {lat}, {lon}\")\n",
    "#         return None, None\n",
    "\n",
    "# altitudes = []\n",
    "# cities = []\n",
    "# states = []\n",
    "# failed_rows = []\n",
    "\n",
    "# for i, coords in tqdm(enumerate(df['home_plate']), total=df['home_plate'].shape[0]):\n",
    "#     altitude = get_altitude(coords[1], coords[0])\n",
    "#     if altitude is None:\n",
    "#         failed_rows.append(i)\n",
    "#     altitudes.append(altitude)\n",
    "    \n",
    "#     city, state = get_city_state(coords[1], coords[0])\n",
    "#     if city is None or state is None:\n",
    "#         failed_rows.append(i)\n",
    "#     cities.append(city)\n",
    "#     states.append(state)\n",
    "    \n",
    "#     time.sleep(1)\n",
    "\n",
    "# df['altitude'] = altitudes\n",
    "# df['city'] = cities\n",
    "# df['state'] = states\n",
    "\n",
    "# print(f\"Failed rows: {list(set(failed_rows))}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the geo transformation should take place above this\n",
    "\n",
    "## starting the process of matching in data from other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sAVE THE CITY STATE AND ALTITUDE TO A CSV SO i CAN REFERENCE IT AND SKIP THE STEP\n",
    "\n",
    "df_cleaned.to_csv('data/2023_mhsaa_POST_LOOKUP2.csv', index=False)\n",
    "\n",
    "## Load the CSV\n",
    "# df = pd.read_csv('data/2023_mhsaa_POST_LOOKUP2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename back to df_cleaned to continue with the following blocks\n",
    "df_cleaned = df\n",
    "\n",
    "# ## output to csv \n",
    "# df_cleaned.to_csv('data/fields_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned.info()\n",
    "\n",
    "# # Load the host team info with nickname and team colors\n",
    "path = 'data/MHSAA/2023_MHSAA_sites.csv'\n",
    "df_hosts = pd.read_csv(path)\n",
    "\n",
    "df_parks = df_cleaned\n",
    "\n",
    "df_hosts.head()\n",
    "\n",
    "park_df = df_parks\n",
    "host_df = df_hosts\n",
    "# df_hosts.info()\n",
    "\n",
    "# # Merge the host team info with the field info\n",
    "# df_cleaned = df_cleaned.merge(df_hosts, on='host_team', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple Merge, should work because the park_name columns should match exactly\n",
    "## Do not detroy any data\n",
    "df_merged = park_df.merge(host_df, on='park_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop index 1 and 137 (Osborn and Concorida) because they screw up the graphs\n",
    "## Do this by dropping the row with the highest min_distance\n",
    "df_merged = df_merged.drop(df_merged['min_distance'].idxmax())\n",
    "\n",
    "## Drop row with lowest max_distance\n",
    "df_merged = df_merged.drop(df_merged['max_distance'].idxmin())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Osbourn - MAX Outlier - plot is correct it is just a strange fenceless field\n",
    "# df_merged.drop([28], inplace=True)\n",
    "\n",
    "# Drop AA Greenhills because something is wrong witht the plot\n",
    "# Division is hosted at Concordia University AA - I have that ploted but it is not apearing in the data\n",
    "# df_merged.drop([139], inplace=True) \n",
    "\n",
    "## Reset index\n",
    "df_merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename to use next block\n",
    "\n",
    "df = df_merged\n",
    "\n",
    "### Updated to create Standard Div +/- Lines\n",
    "\n",
    "## create the min max and mean fence distance rows\n",
    "# Transpose the dataframe to get the \n",
    "transposed_df = pd.DataFrame(df['distances'].to_list()).transpose()\n",
    "\n",
    "# Calculate min, max, mean, median, Q1 and Q3 for each row\n",
    "min_fence_distances = transposed_df.min(axis=1)\n",
    "max_fence_distances = transposed_df.max(axis=1)\n",
    "mean_fence_distances = transposed_df.mean(axis=1)\n",
    "median_fence_distances = transposed_df.median(axis=1)\n",
    "## create profiles for standard deviation\n",
    "std_fence_distances = transposed_df.std(axis=1)\n",
    "first_fence_distances = mean_fence_distances + std_fence_distances\n",
    "third_fence_distances = mean_fence_distances - std_fence_distances\n",
    "\n",
    "# Create a new DataFrame to store these values\n",
    "new_df = pd.DataFrame({\n",
    "    'park_name': ['Min', 'Max', 'Mean', 'Median', 'Q1', 'Q3'],\n",
    "    'distances': [\n",
    "        min_fence_distances.tolist(), \n",
    "        max_fence_distances.tolist(),\n",
    "        mean_fence_distances.tolist(),\n",
    "        median_fence_distances.tolist(), # Add a comma here\n",
    "        first_fence_distances.tolist(),\n",
    "        third_fence_distances.tolist()\n",
    "    ]\n",
    "})\n",
    "\n",
    "# For all other columns in the original DataFrame, add a column of NaN values in the new DataFrame\n",
    "for column in df.columns:\n",
    "    if column not in new_df.columns:\n",
    "        new_df[column] = np.nan\n",
    "\n",
    "# Concatenate the new DataFrame with the original one\n",
    "df = pd.concat([df, new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Add Osbourn back to the end of the dataframe\n",
    "# df = df.append(park_df.iloc[1])\n",
    "# df = df.append(park_df.iloc[137])\n",
    "\n",
    "# # df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Merge the display names into the dataframe\n",
    "\n",
    "# ## Load the display names from csv\n",
    "# path = 'data/MHSAA/2023_district_teams.csv'\n",
    "# places_df = pd.read_csv(path)\n",
    "\n",
    "# places_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS BLOCK CREATES THE RANKING OF PITCHER VS HITTER FRIENDLY FIELDS\n",
    "def rank_fields(data):\n",
    "    # Define weights for each parameter\n",
    "    weights = {\n",
    "        'max_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'min_distance': 1,  # positive weight since shorter fences favor hitters\n",
    "        'avg_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'median_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'field_area_sqft': -1,  # negative weight since larger fields favor pitchers\n",
    "        'fair_to_foul': -1,  # negative weight since larger ratio (more foul territory) favors pitchers\n",
    "        'foul_area_sqft': -1, # negative weight since larger foul area favors pitchers\n",
    "        'fop_area_sqft': -1, # negative weight since larger out of play area favors pitchers\n",
    "    }\n",
    "\n",
    "    # Standardize features (subtract mean and divide by standard deviation)\n",
    "    standardized_data = data.copy()\n",
    "    for column in weights.keys():\n",
    "        standardized_data[column] = (standardized_data[column] - standardized_data[column].mean()) / standardized_data[column].std()\n",
    "\n",
    "    # Calculate score for each field\n",
    "    standardized_data['score'] = standardized_data.apply(lambda row: sum(row[param] * weight for param, weight in weights.items()), axis=1)\n",
    "\n",
    "    # Save scores to original dataframe\n",
    "    data['score'] = standardized_data['score']\n",
    "\n",
    "    # Rank fields based on score (higher scores are more hitter-friendly)\n",
    "    ranked_fields = data.sort_values('score', ascending=False)\n",
    "\n",
    "    return ranked_fields\n",
    "\n",
    "# Suppose 'df' is your DataFrame containing the field data\n",
    "ranked_fields = rank_fields(df)\n",
    "print(ranked_fields[['park_name', 'score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_fields.info()\n",
    "merged_df = ranked_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import webcolors\n",
    "\n",
    "# Assuming df is your DataFrame and it has columns 'color1' and 'color2'\n",
    "\n",
    "custom_colors = {\n",
    "    'Maize': '#F2C649',\n",
    "    'Columbia Blue': '#C4D8E2',\n",
    "    'Carolina Blue': '#56A0D3',\n",
    "    'Cardinal': '#C41E3A',\n",
    "    'Burgundy': '#800020',\n",
    "    'Forrest Green': '#18453B',\n",
    "    'Forest Green': '#18453B',\n",
    "    'Columbia': '#C4D8E2',\n",
    "    'Royal': '#4169e1',\n",
    "    'Royal Blue': '#4169e1',\n",
    "    'Vegas Gold': '#C5B358',\n",
    "    'Navy Blue': '#000080'\n",
    "}\n",
    "\n",
    "def convert_to_hex(color_name):\n",
    "    if isinstance(color_name, str):  # Check if color_name is a string\n",
    "        try:\n",
    "            return webcolors.name_to_hex(color_name)\n",
    "        except ValueError:\n",
    "            return custom_colors.get(color_name, '#00FF00')  # default to green if color name not recognized\n",
    "    else:\n",
    "        return '#000000'  # default to black if color_name is not a string\n",
    "\n",
    "# Convert the color columns to string and strip any trailing spaces\n",
    "df['color1'] = df['color1'].astype(str).str.strip()\n",
    "df['color2'] = df['color2'].astype(str).str.strip()\n",
    "\n",
    "# Convert color names to hex values\n",
    "df['color1'] = df['color1'].apply(convert_to_hex)\n",
    "df['color2'] = df['color2'].apply(convert_to_hex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recreate the division_final and level columns\n",
    "\n",
    "## If division column is not null use that value as division_final. if it is null use the value in the regional_division column\n",
    "df['division_final'] = df['division'].fillna(df['regional_div'])\n",
    "\n",
    "## Create a level column based if the field hosts a district the value should be 1\n",
    "## if region_semi_number is present assign level 2 and if region_final_number is present assign level 3\n",
    "## if finals is present assign level 4\n",
    "df['level'] = np.where(df['district'].notnull(), 1, 0)\n",
    "df['level'] = np.where(df['region_semi_number'].notnull(), 2, df['level'])\n",
    "df['level'] = np.where(df['region_final_quarter'].notnull(), 3, df['level'])\n",
    "df['level'] = np.where(df['finals'].notnull(), 4, df['level'])\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()\n",
    "\n",
    "## load the display_names csv\n",
    "path = 'data/MHSAA/MHSAA_display_names.csv'\n",
    "\n",
    "display_df = pd.read_csv(path)\n",
    "\n",
    "display_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## match the park name column from the display_df to the park_name column in the df dataframe\n",
    "\n",
    "df = df.merge(display_df, on='park_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the output directory for the Outfield Fence Plots\n",
    "\n",
    "output_dir = 'data/MHSAA/assets/plots/'\n",
    "\n",
    "def plot_distances(df, row_index):\n",
    "    # Get rows with 'Min', 'Max', 'Mean', 'Q1', 'Q3' in 'park_name'\n",
    "    rows_to_plot = df[df['park_name'].isin(['Min', 'Max', 'Mean', 'Q1', 'Q3'])]\n",
    "    \n",
    "    # Get the row to be highlighted\n",
    "    highlighted_row = df.loc[row_index]\n",
    "    \n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    # Loop over these rows and plot a line graph for each\n",
    "    for index, row in rows_to_plot.iterrows():\n",
    "        if row['park_name'] in ['Q1', 'Q3']: # If Q1 or Q3, plot thinner, dotted line\n",
    "            plt.plot(row['distances'], linestyle='dotted', alpha=0.3, color='grey', label=row['park_name'])\n",
    "        else:\n",
    "            plt.plot(row['distances'], linestyle='dashed', alpha=0.5, label=row['park_name'])\n",
    "\n",
    "        # Add text labels for Min, Max and Mean lines\n",
    "        if row['park_name'] in ['Min', 'Max', 'Mean']:\n",
    "            plt.text(len(row['distances'])-1, row['distances'][-1], row['park_name'], color='blue', va='center')\n",
    "\n",
    "        # Check if the current row is 'Min', if so, add shading\n",
    "        if row['park_name'] == 'Min':\n",
    "            plt.fill_between(range(len(row['distances'])), row['distances'], color='green', alpha=0.2)\n",
    "\n",
    "        # Check if the current row is 'Max', if so, add shading\n",
    "        if row['park_name'] == 'Max':\n",
    "            plt.fill_between(range(len(row['distances'])), row['distances'], color='yellow', alpha=0.2)\n",
    "\n",
    "        # Check if the current row is 'Max', if so, add shading above\n",
    "        if row['park_name'] == 'Max':\n",
    "            plt.fill_between(range(len(row['distances'])), plt.ylim()[1], row['distances'], color='red', alpha=0.2)\n",
    "            \n",
    "    # Plot the highlighted row with a thicker line\n",
    "    plt.plot(highlighted_row['distances'], linewidth=2, label=highlighted_row['park_name'])\n",
    "    \n",
    "    # Set the minimum and maximum values of y-axis\n",
    "    plt.ylim([270, 420])\n",
    "\n",
    "    # Change y-axis labels and tick marks to be white\n",
    "    plt.ylabel('Distance (feet)', color='white')\n",
    "    plt.tick_params(axis='y', colors='white')\n",
    "\n",
    "    # Hide x axis ticks\n",
    "    plt.xticks([])\n",
    "\n",
    "    # Move the title to the inside the plot, centered, just above the x axis\n",
    "    plt.text(len(highlighted_row['distances'])/2, 270, highlighted_row['display_name'], ha='center', va='bottom', fontsize=16)\n",
    "\n",
    "    # Reverse the x-axis\n",
    "    plt.gca().invert_xaxis()\n",
    "\n",
    "    # Generate the file path\n",
    "    file_path = os.path.join(output_dir, f\"plot_{row_index}.png\")\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(file_path)\n",
    "\n",
    "    # Close the figure to free up memory\n",
    "    plt.close()\n",
    "\n",
    "    # Return the file path\n",
    "    return file_path\n",
    "\n",
    "# Add a new column 'file_path' to the DataFrame to store the file paths\n",
    "df['file_path'] = [plot_distances(df, i) for i in df.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df\n",
    "\n",
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the rows with null values in the 'fop', 'foul' or 'home_plate' columns\n",
    "df.dropna(subset=['fop', 'foul', 'home_plate'], inplace=True)\n",
    "\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show me the fields without a display name\n",
    "\n",
    "df_merged[df_merged['display_name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### outpus csv to check\n",
    "df_merged.to_csv('data/MHSAA_FINAL_TEST.csv', index=False)\n",
    "\n",
    "### OUTPUT JSON TO USE IN MAP\n",
    "df_merged.to_json('data/html/mhsaa/data/map.json', orient='records')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_merged.iloc[]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END BLOCK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## List the values from the team column\n",
    "\n",
    "# print(len(df_hosts['team'].unique()))\n",
    "# df_hosts['team'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here down are simple plots to do spot check of data and hold example of polar chart\n",
    "\n",
    "### FILL IN THE REST OF JSON WITH THE DATA FOR THE 2023 TOURNEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the max distance, min distance, average distance, and median distance\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "ax[0, 0].hist(df_cleaned['max_distance'], bins=20)\n",
    "\n",
    "ax[0, 1].hist(df_cleaned['min_distance'], bins=20)\n",
    "\n",
    "ax[1, 0].hist(df_cleaned['avg_distance'], bins=20)\n",
    "\n",
    "ax[1, 1].hist(df_cleaned['median_distance'], bins=20)\n",
    "\n",
    "ax[0, 0].set_title('Max Distance')\n",
    "ax[0, 1].set_title('Min Distance')\n",
    "\n",
    "ax[1, 0].set_title('Average Distance')\n",
    "ax[1, 1].set_title('Median Distance')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW WITH AUTO SCALING\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_data(data, level_filter=None):\n",
    "    count_by_orientation = defaultdict(int)\n",
    "    \n",
    "    for _, record in data.iterrows():\n",
    "        if level_filter is None or record['level'] == level_filter:\n",
    "            orientation = round(record['field_orientation'])\n",
    "            count_by_orientation[orientation] += 1\n",
    "\n",
    "    return count_by_orientation\n",
    "\n",
    "\n",
    "\n",
    "def calculate_max_y(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    return max(bin_counts)\n",
    "\n",
    "\n",
    "def create_polar_chart(data, num_bins=36, level_filter=None, y_min=-10, background_color='#748667', color_map=plt.cm.viridis, bar_alpha=0.8):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 2 * np.pi / num_bins\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "\n",
    "    ax.set_facecolor('#000000')\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "    # Set dark background\n",
    "    ax.set_facecolor(background_color)\n",
    "    plt.gca().set_rlabel_position(22.5)\n",
    "    y_max = calculate_max_y(data, num_bins=num_bins, level_filter=level_filter) + 5\n",
    "    ax.set_ylim(y_min, y_max)  # Adjust based on max count\n",
    "\n",
    "    # Add bars for negative values\n",
    "    zero_height_bars = ax.bar(bin_edges[:-1], np.abs(ax.get_ylim()[0]), width=bin_width, bottom=0.0, color='k', alpha=0.3)\n",
    "\n",
    "    bars = ax.bar(bin_edges[:-1], bin_counts, width=bin_width, bottom=0)\n",
    "    \n",
    "    # Use custom colors and opacity\n",
    "    for r, bar in zip(bin_counts, bars):\n",
    "        bar.set_facecolor(color_map(r / max(bin_counts)))\n",
    "        bar.set_alpha(bar_alpha)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setuops to check\n",
    "bin_nums = [30, 40, 50, 60, 120, 180]\n",
    "\n",
    "for i in bin_nums:\n",
    "    create_polar_chart(df_merged, num_bins=i, level_filter=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
