{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ETL NOTEBOOK FOR 2023 MHSAA TOURNEY SPECIFIC MAP\n",
    "\n",
    "#### Adapted from ETL for JSON\n",
    "\n",
    "## Dependencies and Setup\n",
    "### Dependencies\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Start timer\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD BLOCK###\n",
    "#### Load data from kml file exported from Google Earth\n",
    "\n",
    "file_path = ('data/kml/MHSAA_2023.kml') # file path to kml file\n",
    "\n",
    "\n",
    "# Read the KML file\n",
    "with open(file_path) as file:\n",
    "    xml_data = file.read()\n",
    "\n",
    "# Initialize soup variables for parsing file\n",
    "soup = BeautifulSoup(xml_data, 'xml')\n",
    "folders = soup.Document.Folder\n",
    "list = soup.Document.Folder.find_all('Folder')\n",
    "\n",
    "# Create a list to store rows to append to the DataFrame\n",
    "rows = []\n",
    "\n",
    "# Loop through the folders and extract the data\n",
    "for folder in list:\n",
    "    try:\n",
    "        field_name = folder.find('name').text\n",
    "        foul = folder.find_all('coordinates')[0].text\n",
    "        fop = folder.find_all('coordinates')[1].text\n",
    "        notes = None\n",
    "\n",
    "        # Check if there is a description tag, if so, use it for notes\n",
    "        if folder.find('description') is not None:\n",
    "            notes = folder.find('description').text\n",
    "\n",
    "        row = {\n",
    "            'field': field_name,\n",
    "            'foul': foul,\n",
    "            'fop': fop,\n",
    "            'notes': notes\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Add name of folder to a list of failed folders\n",
    "        failed.append(folder.find('name').text)\n",
    "        print(f\"Error processing folder: {folder.find('name').text}. Error message: {str(e)}\")\n",
    "\n",
    "# Convert the list of rows to a DataFrame\n",
    "df = pd.DataFrame(rows, columns=['field', 'foul', 'fop', 'notes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams Butzel Complex</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-83.1678186,42.3966942,0 -83...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-83.1678186,42.3966942,0 -83...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adrian HS</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.0416584,41.9091676,0 -84...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-84.0416584,41.9091676,0 -84...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alcona HS</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-83.4068606,44.6597432,0 -83...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-83.4068606,44.6597432,0 -83...</td>\n",
       "      <td>tough treeline in center and left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algonac High School</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-82.58239759999999,42.628620...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-82.58239759999999,42.628620...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allen Park High School</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-83.2273711,42.2455509,0 -83...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t-83.2273711,42.2455509,0 -83...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    field                                               foul  \\\n",
       "0    Adams Butzel Complex  \\n\\t\\t\\t\\t\\t\\t\\t\\t-83.1678186,42.3966942,0 -83...   \n",
       "1               Adrian HS  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.0416584,41.9091676,0 -84...   \n",
       "2               Alcona HS  \\n\\t\\t\\t\\t\\t\\t\\t\\t-83.4068606,44.6597432,0 -83...   \n",
       "3     Algonac High School  \\n\\t\\t\\t\\t\\t\\t\\t\\t-82.58239759999999,42.628620...   \n",
       "4  Allen Park High School  \\n\\t\\t\\t\\t\\t\\t\\t\\t-83.2273711,42.2455509,0 -83...   \n",
       "\n",
       "                                                 fop  \\\n",
       "0  \\n\\t\\t\\t\\t\\t\\t\\t\\t-83.1678186,42.3966942,0 -83...   \n",
       "1  \\n\\t\\t\\t\\t\\t\\t\\t\\t-84.0416584,41.9091676,0 -84...   \n",
       "2  \\n\\t\\t\\t\\t\\t\\t\\t\\t-83.4068606,44.6597432,0 -83...   \n",
       "3  \\n\\t\\t\\t\\t\\t\\t\\t\\t-82.58239759999999,42.628620...   \n",
       "4  \\n\\t\\t\\t\\t\\t\\t\\t\\t-83.2273711,42.2455509,0 -83...   \n",
       "\n",
       "                               notes  \n",
       "0                               None  \n",
       "1                               None  \n",
       "2  tough treeline in center and left  \n",
       "3                               None  \n",
       "4                               None  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the new dataframe\n",
    "\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Remove new line and space characters from coordinates\n",
    "df_cleaned = df_cleaned.replace(r'\\n','', regex=True) \n",
    "df_cleaned = df_cleaned.replace(r'\\t','', regex=True) \n",
    "\n",
    "# Drop any duplicate rows\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['field'], keep='first')\n",
    "\n",
    "# Drop any rows with empty fields\n",
    "df_cleaned = df_cleaned[(df_cleaned != 0).all(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 140 entries, 0 to 143\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   field   140 non-null    object\n",
      " 1   foul    140 non-null    object\n",
      " 2   fop     140 non-null    object\n",
      " 3   notes   7 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 5.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams Butzel Complex</td>\n",
       "      <td>-83.1678186,42.3966942,0 -83.1678776,42.397648...</td>\n",
       "      <td>-83.1678186,42.3966942,0 -83.1665385,42.396724...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adrian HS</td>\n",
       "      <td>-84.0416584,41.9091676,0 -84.04166909999999,41...</td>\n",
       "      <td>-84.0416584,41.9091676,0 -84.0405493,41.909184...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alcona HS</td>\n",
       "      <td>-83.4068606,44.6597432,0 -83.40803409999999,44...</td>\n",
       "      <td>-83.4068606,44.6597432,0 -83.40680159999999,44...</td>\n",
       "      <td>tough treeline in center and left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algonac High School</td>\n",
       "      <td>-82.58239759999999,42.6286202,0 -82.5813153999...</td>\n",
       "      <td>-82.58239759999999,42.6286202,0 -82.5826256,42...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allen Park High School</td>\n",
       "      <td>-83.2273711,42.2455509,0 -83.2285244,42.245525...</td>\n",
       "      <td>-83.2273711,42.2455509,0 -83.22739919999999,42...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    field                                               foul  \\\n",
       "0    Adams Butzel Complex  -83.1678186,42.3966942,0 -83.1678776,42.397648...   \n",
       "1               Adrian HS  -84.0416584,41.9091676,0 -84.04166909999999,41...   \n",
       "2               Alcona HS  -83.4068606,44.6597432,0 -83.40803409999999,44...   \n",
       "3     Algonac High School  -82.58239759999999,42.6286202,0 -82.5813153999...   \n",
       "4  Allen Park High School  -83.2273711,42.2455509,0 -83.2285244,42.245525...   \n",
       "\n",
       "                                                 fop  \\\n",
       "0  -83.1678186,42.3966942,0 -83.1665385,42.396724...   \n",
       "1  -84.0416584,41.9091676,0 -84.0405493,41.909184...   \n",
       "2  -83.4068606,44.6597432,0 -83.40680159999999,44...   \n",
       "3  -82.58239759999999,42.6286202,0 -82.5826256,42...   \n",
       "4  -83.2273711,42.2455509,0 -83.22739919999999,42...   \n",
       "\n",
       "                               notes  \n",
       "0                               None  \n",
       "1                               None  \n",
       "2  tough treeline in center and left  \n",
       "3                               None  \n",
       "4                               None  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.info()\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Clean up polygon data and create a new home_plate column\n",
    "\n",
    "def parse_coordinates(coord_string):\n",
    "    coords = coord_string.split()\n",
    "    parsed_coords = [tuple(map(float, coord.split(',')[:2])) for coord in coords]\n",
    "    return parsed_coords\n",
    "\n",
    "# Create a new column for the home_plate location using the first set of coordinates in the 'fop' column\n",
    "df_cleaned['home_plate'] = df_cleaned['fop'].apply(lambda x: parse_coordinates(x)[0])\n",
    "\n",
    "# Apply the parse_coordinates function to the 'foul' and 'fop' columns\n",
    "df_cleaned['foul'] = df_cleaned['foul'].apply(parse_coordinates)\n",
    "df_cleaned['fop'] = df_cleaned['fop'].apply(parse_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## AREA CALCULATION ##############\n",
    "\n",
    "\n",
    "import pyproj\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import transform\n",
    "\n",
    "\n",
    "def calculate_area(coords):\n",
    "    # Create a Polygon object from the coordinates\n",
    "    polygon = Polygon(coords)\n",
    "\n",
    "    # Calculate the centroid of the polygon\n",
    "    centroid = polygon.centroid\n",
    "\n",
    "    # Create a custom LAEA projection centered on the centroid\n",
    "    custom_projection = f\"+proj=laea +lat_0={centroid.y} +lon_0={centroid.x} +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"\n",
    "\n",
    "    # Create a transformer for converting coordinates to the custom LAEA projection\n",
    "    transformer = pyproj.Transformer.from_crs(\n",
    "        pyproj.CRS(\"EPSG:4326\"),  # WGS 84 (latitude and longitude)\n",
    "        pyproj.CRS(custom_projection),  # Custom LAEA projection\n",
    "        always_xy=True\n",
    "    )\n",
    "\n",
    "    # Define a function to transform coordinates using the transformer\n",
    "    def transform_coordinates(x, y):\n",
    "        return transformer.transform(x, y)\n",
    "\n",
    "    # Convert the coordinates to the custom LAEA projection\n",
    "    polygon_laea = transform(transform_coordinates, polygon)\n",
    "\n",
    "    # Calculate the area in square meters\n",
    "    area_sqm = polygon_laea.area\n",
    "\n",
    "    # Convert the area to square feet (1 square meter = 10.764 square feet)\n",
    "    area_sqft = area_sqm * 10.764\n",
    "\n",
    "    return area_sqft\n",
    "\n",
    "\n",
    "\n",
    "### Call Function and add to dataframe\n",
    "df_cleaned['foul_area_sqft'] = df_cleaned['foul'].apply(calculate_area)\n",
    "df_cleaned['fop_area_sqft'] = df_cleaned['fop'].apply(calculate_area)\n",
    "\n",
    "## Calculate the total area of the field and the ratio of foul area to field area\n",
    "df_cleaned['field_area_sqft'] = df_cleaned['foul_area_sqft'] + df_cleaned['fop_area_sqft']\n",
    "## Percentage foul area\n",
    "df_cleaned['foul_area_per'] = df_cleaned['foul_area_sqft'] / df_cleaned['field_area_sqft']\n",
    "## Fair to Foul Ratio\n",
    "df_cleaned['fair_to_foul'] = df_cleaned['fop_area_sqft'] / df_cleaned['foul_area_sqft']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# FENCE DISTANCE CALCULATION #############\n",
    "\n",
    "from geopy.distance import great_circle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def interpolate_points(start, end, length_ratio):\n",
    "    start_np = np.array(start)\n",
    "    end_np = np.array(end)\n",
    "    return tuple(start_np + (end_np - start_np) * length_ratio)\n",
    "\n",
    "def calculate_distances(home_plate, outfield_coords, num_points=540):\n",
    "    def is_same_point(point1, point2, tolerance=1e-6):\n",
    "        return abs(point1[0] - point2[0]) < tolerance and abs(point1[1] - point2[1]) < tolerance\n",
    "\n",
    "    home_plate_lat_lon = (home_plate[1], home_plate[0])\n",
    "    distances = []\n",
    "\n",
    "    # Calculate total line length\n",
    "    total_length = 0\n",
    "    segments = []\n",
    "    for i in range(len(outfield_coords) - 1):\n",
    "        start = outfield_coords[i]\n",
    "        end = outfield_coords[i + 1]\n",
    "        if not is_same_point(home_plate, start) and not is_same_point(home_plate, end):\n",
    "            segment_length = great_circle((start[1], start[0]), (end[1], end[0])).feet\n",
    "            segments.append((start, end, segment_length))\n",
    "            total_length += segment_length\n",
    "\n",
    "    # Calculate the distance between equally spaced points\n",
    "    spacing = total_length / (num_points - 1)\n",
    "\n",
    "    # Interpolate points and calculate distances\n",
    "    current_length = 0\n",
    "    segment_index = 0\n",
    "    for i in range(num_points):\n",
    "        while segment_index < len(segments) - 1 and current_length > segments[segment_index][2]:\n",
    "            current_length -= segments[segment_index][2]\n",
    "            segment_index += 1\n",
    "\n",
    "        start, end, segment_length = segments[segment_index]\n",
    "        length_ratio = current_length / segment_length\n",
    "        point = interpolate_points(start, end, length_ratio)\n",
    "        distance = great_circle(home_plate_lat_lon, (point[1], point[0])).feet\n",
    "        distances.append(distance)\n",
    "\n",
    "        current_length += spacing\n",
    "\n",
    "    return distances\n",
    "\n",
    "# Calculate distances for each row\n",
    "df_cleaned['distances'] = df_cleaned.apply(lambda row: calculate_distances(row['home_plate'], row['fop']), axis=1)\n",
    "\n",
    "# Calculate max, min, and average distances for each row\n",
    "df_cleaned['max_distance'] = df_cleaned['distances'].apply(max)\n",
    "df_cleaned['min_distance'] = df_cleaned['distances'].apply(min)\n",
    "df_cleaned['avg_distance'] = df_cleaned['distances'].apply(lambda distances: sum(distances) / len(distances))\n",
    "# get the median distance\n",
    "df_cleaned['median_distance'] = df_cleaned['distances'].apply(lambda distances: np.median(distances))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540    140\n",
       "Name: num_distances, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## CHECK BLOCK ########\n",
    "\n",
    "## Check how long the distance list is for each row\n",
    "df_cleaned['num_distances'] = df_cleaned['distances'].apply(len)\n",
    "\n",
    "## Print the value counts for the 'num_distances' column\n",
    "df_cleaned['num_distances'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create ranks for each column\n",
    "\n",
    "def rank_fields(df):\n",
    "    # Calculate the rank for each category\n",
    "    df['max_distance_rank'] = df['max_distance'].rank(ascending=False, method='min')\n",
    "    df['min_distance_rank'] = df['min_distance'].rank(ascending=False, method='min')\n",
    "    df['avg_distance_rank'] = df['avg_distance'].rank(ascending=False, method='min')\n",
    "    df['median_distance_rank'] = df['median_distance'].rank(ascending=False, method='min')\n",
    "    df['field_area_rank'] = df['field_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['foul_area_rank'] = df['foul_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['fop_area_per_rank'] = df['fop_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['ratio_rank'] = df['fair_to_foul'].rank(ascending=False, method='min')\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Function\n",
    "\n",
    "df_cleaned = rank_fields(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Orienting the map to the home plate location ####\n",
    "\n",
    "### Find the center of the field\n",
    "def calculate_centroid(coords):\n",
    "    x_coords = [coord[0] for coord in coords]\n",
    "    y_coords = [coord[1] for coord in coords]\n",
    "    centroid_x = sum(x_coords) / len(coords)\n",
    "    centroid_y = sum(y_coords) / len(coords)\n",
    "    return (centroid_x, centroid_y)\n",
    "\n",
    "\n",
    "## Find the bearing between the home plate and the center of the field\n",
    "import math\n",
    "\n",
    "def calculate_bearing(point1, point2):\n",
    "    lat1, lon1 = math.radians(point1[1]), math.radians(point1[0])\n",
    "    lat2, lon2 = math.radians(point2[1]), math.radians(point2[0])\n",
    "\n",
    "    d_lon = lon2 - lon1\n",
    "\n",
    "    x = math.cos(lat2) * math.sin(d_lon)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(d_lon)\n",
    "\n",
    "    bearing = math.degrees(math.atan2(x, y))\n",
    "    bearing = (bearing + 360) % 360  # Normalize the bearing to the range [0, 360)\n",
    "\n",
    "    return bearing\n",
    "\n",
    "### Function to classify direction in laymans terms North, South, East, West, ect\n",
    "def degrees_to_cardinal_direction(degrees):\n",
    "    directions = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW', 'N']\n",
    "    index = round(degrees / 45)\n",
    "    return directions[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the centroid of the outfield fence coordinates for each row\n",
    "df_cleaned['fop_centroid'] = df_cleaned['fop'].apply(lambda coords: calculate_centroid(coords[1:]))\n",
    "\n",
    "# Calculate the bearing between home plate and the centroid for each row\n",
    "df_cleaned['field_orientation'] = df_cleaned.apply(lambda row: calculate_bearing(row['home_plate'], row['fop_centroid']), axis=1)\n",
    "\n",
    "# Convert the bearing to a cardinal direction\n",
    "df_cleaned['field_cardinal_direction'] = df_cleaned['field_orientation'].apply(degrees_to_cardinal_direction)\n",
    "\n",
    "# rename 'field' to 'park_name'\n",
    "df_cleaned.rename(columns={'field': 'park_name'}, inplace=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 20/140 [00:39<04:14,  2.12s/it]"
     ]
    }
   ],
   "source": [
    "## Need to rename dataframe to df for this block \n",
    "\n",
    "df = df_cleaned\n",
    "\n",
    "### Get the Altitiude of each field as well as city and state\n",
    "### This block will take a while to run, can process about 2 seconds per record\n",
    "\n",
    "## Get Altitudes of the ballparks\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Set your Google Maps API key here\n",
    "api_key = 'AIzaSyA_BhlTupRdBPBhRptQuR6pYorMVYQnRMA'\n",
    "\n",
    "# Get the altitude of a location from its latitude and longitude\n",
    "def get_altitude(lat, lon):\n",
    "    query = f'https://maps.googleapis.com/maps/api/elevation/json?locations={lat},{lon}&key={api_key}'\n",
    "    r = requests.get(query).json()\n",
    "    elevation = r['results'][0]['elevation']\n",
    "    return elevation\n",
    "\n",
    "# Get the city and state of a location from its latitude and longitude\n",
    "def get_city_state(lat, lon):\n",
    "    query = f'https://maps.googleapis.com/maps/api/geocode/json?latlng={lat},{lon}&key={api_key}'\n",
    "    r = requests.get(query).json()\n",
    "    results = r['results'][0]['address_components']\n",
    "    city = next((item['long_name'] for item in results if 'locality' in item['types']), '')\n",
    "    state = next((item['long_name'] for item in results if 'administrative_area_level_1' in item['types']), '')\n",
    "    return city, state\n",
    "\n",
    "# Initialize empty lists for the new columns\n",
    "altitudes = []\n",
    "cities = []\n",
    "states = []\n",
    "\n",
    "# Loop through each row in the dataframe\n",
    "for coords in tqdm(df['home_plate']):\n",
    "    # Get altitude and add to list\n",
    "    altitude = get_altitude(coords[1], coords[0])\n",
    "    altitudes.append(altitude)\n",
    "\n",
    "    # Get city and state and add to lists\n",
    "    city, state = get_city_state(coords[1], coords[0])\n",
    "    cities.append(city)\n",
    "    states.append(state)\n",
    "\n",
    "    # Sleep for a bit to avoid hitting rate limits\n",
    "    time.sleep(1)  # Adjust this value as needed\n",
    "\n",
    "# Add the new columns to the dataframe\n",
    "df['altitude'] = altitudes\n",
    "df['city'] = cities\n",
    "df['state'] = states\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the geo transformation should take place above this\n",
    "\n",
    "## starting the process of matching in data from other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sAVE THE CITY STATE AND ALTITUDE TO A CSV SO i CAN REFERENCE IT AND SKIP THE STEP\n",
    "\n",
    "df.to_csv('data/2023_mhsaa_POST_LOOKUP.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename back to df_cleaned to continue with the following blocks\n",
    "df_cleaned = df\n",
    "\n",
    "# ## output to csv \n",
    "# df_cleaned.to_csv('data/fields_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned.info()\n",
    "\n",
    "# # Load the host team info with nickname and team colors\n",
    "path = 'data/MHSAA/2023_MHSAA_sites.csv'\n",
    "df_hosts = pd.read_csv(path)\n",
    "\n",
    "df_parks = df_cleaned\n",
    "\n",
    "df_hosts.head()\n",
    "\n",
    "park_df = df_parks\n",
    "host_df = df_hosts\n",
    "# df_hosts.info()\n",
    "\n",
    "# # Merge the host team info with the field info\n",
    "# df_cleaned = df_cleaned.merge(df_hosts, on='host_team', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_df.info()\n",
    "\n",
    "## find the Lowell High School field\n",
    "host_df[host_df['park_name'] == 'Lowell High School - high school']\n",
    "\n",
    "\n",
    "\n",
    "# park_df.info()\n",
    "\n",
    "## find the Lowell High School field\n",
    "park_df[park_df['park_name'] == 'Lowell High School - high school']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple Merge, should work because the park_name columns should match exactly\n",
    "## Do not detroy any data\n",
    "df_merged = park_df.merge(host_df, on='park_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop index 1 and 137 (Osborn and Concorida) because they screw up the graphs\n",
    "df_merged = df_merged.drop([1,137])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Osbourn - MAX Outlier - plot is correct it is just a strange fenceless field\n",
    "# df_merged.drop([28], inplace=True)\n",
    "\n",
    "# Drop AA Greenhills because something is wrong witht the plot\n",
    "# Division is hosted at Concordia University AA - I have that ploted but it is not apearing in the data\n",
    "# df_merged.drop([139], inplace=True) \n",
    "\n",
    "## Reset index\n",
    "df_merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename to use next block\n",
    "\n",
    "df = df_merged\n",
    "\n",
    "### Updated to create Standard Div +/- Lines\n",
    "\n",
    "## create the min max and mean fence distance rows\n",
    "# Transpose the dataframe to get the \n",
    "transposed_df = pd.DataFrame(df['distances'].to_list()).transpose()\n",
    "\n",
    "# Calculate min, max, mean, median, Q1 and Q3 for each row\n",
    "min_fence_distances = transposed_df.min(axis=1)\n",
    "max_fence_distances = transposed_df.max(axis=1)\n",
    "mean_fence_distances = transposed_df.mean(axis=1)\n",
    "median_fence_distances = transposed_df.median(axis=1)\n",
    "## create profiles for standard deviation\n",
    "std_fence_distances = transposed_df.std(axis=1)\n",
    "first_fence_distances = mean_fence_distances + std_fence_distances\n",
    "third_fence_distances = mean_fence_distances - std_fence_distances\n",
    "\n",
    "# Create a new DataFrame to store these values\n",
    "new_df = pd.DataFrame({\n",
    "    'park_name': ['Min', 'Max', 'Mean', 'Median', 'Q1', 'Q3'],\n",
    "    'distances': [\n",
    "        min_fence_distances.tolist(), \n",
    "        max_fence_distances.tolist(),\n",
    "        mean_fence_distances.tolist(),\n",
    "        median_fence_distances.tolist(), # Add a comma here\n",
    "        first_fence_distances.tolist(),\n",
    "        third_fence_distances.tolist()\n",
    "    ]\n",
    "})\n",
    "\n",
    "# For all other columns in the original DataFrame, add a column of NaN values in the new DataFrame\n",
    "for column in df.columns:\n",
    "    if column not in new_df.columns:\n",
    "        new_df[column] = np.nan\n",
    "\n",
    "# Concatenate the new DataFrame with the original one\n",
    "df = pd.concat([df, new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Add Osbourn back to the end of the dataframe\n",
    "# df = df.append(park_df.iloc[1])\n",
    "# df = df.append(park_df.iloc[137])\n",
    "\n",
    "# # df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Merge the display names into the dataframe\n",
    "\n",
    "# ## Load the display names from csv\n",
    "# path = 'data/MHSAA/2023_district_teams.csv'\n",
    "# places_df = pd.read_csv(path)\n",
    "\n",
    "# places_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Rename District to district\n",
    "# places_df.rename(columns={'District': 'district'}, inplace=True)\n",
    "\n",
    "# ## Merge the display names into the dataframe\n",
    "# ## merged_df column District, places_df column district\n",
    "# df_merged = df_merged.merge(places_df, on='district', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS BLOCK CREATES THE RANKING OF PITCHER VS HITTER FRIENDLY FIELDS\n",
    "def rank_fields(data):\n",
    "    # Define weights for each parameter\n",
    "    weights = {\n",
    "        'max_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'min_distance': 1,  # positive weight since shorter fences favor hitters\n",
    "        'avg_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'median_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'field_area_sqft': -1,  # negative weight since larger fields favor pitchers\n",
    "        'fair_to_foul': -1,  # negative weight since larger ratio (more foul territory) favors pitchers\n",
    "        'foul_area_sqft': -1, # negative weight since larger foul area favors pitchers\n",
    "        'fop_area_sqft': -1, # negative weight since larger out of play area favors pitchers\n",
    "    }\n",
    "\n",
    "    # Standardize features (subtract mean and divide by standard deviation)\n",
    "    standardized_data = data.copy()\n",
    "    for column in weights.keys():\n",
    "        standardized_data[column] = (standardized_data[column] - standardized_data[column].mean()) / standardized_data[column].std()\n",
    "\n",
    "    # Calculate score for each field\n",
    "    standardized_data['score'] = standardized_data.apply(lambda row: sum(row[param] * weight for param, weight in weights.items()), axis=1)\n",
    "\n",
    "    # Save scores to original dataframe\n",
    "    data['score'] = standardized_data['score']\n",
    "\n",
    "    # Rank fields based on score (higher scores are more hitter-friendly)\n",
    "    ranked_fields = data.sort_values('score', ascending=False)\n",
    "\n",
    "    return ranked_fields\n",
    "\n",
    "# Suppose 'df' is your DataFrame containing the field data\n",
    "ranked_fields = rank_fields(df)\n",
    "print(ranked_fields[['park_name', 'score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_fields.info()\n",
    "merged_df = ranked_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import webcolors\n",
    "\n",
    "# Assuming df is your DataFrame and it has columns 'color1' and 'color2'\n",
    "\n",
    "custom_colors = {\n",
    "    'Maize': '#F2C649',\n",
    "    'Columbia Blue': '#C4D8E2',\n",
    "    'Carolina Blue': '#56A0D3',\n",
    "    'Cardinal': '#C41E3A',\n",
    "    'Burgundy': '#800020',\n",
    "    'Forrest Green': '#18453B',\n",
    "    'Forest Green': '#18453B',\n",
    "    'Columbia': '#C4D8E2',\n",
    "    'Royal': '#4169e1',\n",
    "    'Royal Blue': '#4169e1',\n",
    "    'Vegas Gold': '#C5B358',\n",
    "    'Navy Blue': '#000080'\n",
    "}\n",
    "\n",
    "def convert_to_hex(color_name):\n",
    "    if isinstance(color_name, str):  # Check if color_name is a string\n",
    "        try:\n",
    "            return webcolors.name_to_hex(color_name)\n",
    "        except ValueError:\n",
    "            return custom_colors.get(color_name, '#00FF00')  # default to green if color name not recognized\n",
    "    else:\n",
    "        return '#000000'  # default to black if color_name is not a string\n",
    "\n",
    "# Convert the color columns to string and strip any trailing spaces\n",
    "df['color1'] = df['color1'].astype(str).str.strip()\n",
    "df['color2'] = df['color2'].astype(str).str.strip()\n",
    "\n",
    "# Convert color names to hex values\n",
    "df['color1'] = df['color1'].apply(convert_to_hex)\n",
    "df['color2'] = df['color2'].apply(convert_to_hex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recreate the division_final and level columns\n",
    "\n",
    "## If division column is not null use that value as division_final. if it is null use the value in the regional_division column\n",
    "df['division_final'] = df['division'].fillna(df['regional_div'])\n",
    "\n",
    "## Create a level column based if the field hosts a district the value should be 1\n",
    "## if region_semi_number is present assign level 2 and if region_final_number is present assign level 3\n",
    "## if finals is present assign level 4\n",
    "df['level'] = np.where(df['district'].notnull(), 1, 0)\n",
    "df['level'] = np.where(df['region_semi_number'].notnull(), 2, df['level'])\n",
    "df['level'] = np.where(df['region_final_quarter'].notnull(), 3, df['level'])\n",
    "df['level'] = np.where(df['finals'].notnull(), 4, df['level'])\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()\n",
    "\n",
    "## load the display_names csv\n",
    "path = 'data/MHSAA/MHSAA_display_names.csv'\n",
    "\n",
    "display_df = pd.read_csv(path)\n",
    "\n",
    "display_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## match the park name column from the display_df to the park_name column in the df dataframe\n",
    "\n",
    "df = df.merge(display_df, on='park_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the output directory for the Outfield Fence Plots\n",
    "\n",
    "output_dir = 'data/MHSAA/assets/plots/'\n",
    "\n",
    "def plot_distances(df, row_index):\n",
    "    # Get rows with 'Min', 'Max', 'Mean', 'Q1', 'Q3' in 'park_name'\n",
    "    rows_to_plot = df[df['park_name'].isin(['Min', 'Max', 'Mean', 'Q1', 'Q3'])]\n",
    "    \n",
    "    # Get the row to be highlighted\n",
    "    highlighted_row = df.loc[row_index]\n",
    "    \n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    # Loop over these rows and plot a line graph for each\n",
    "    for index, row in rows_to_plot.iterrows():\n",
    "        if row['park_name'] in ['Q1', 'Q3']: # If Q1 or Q3, plot thinner, dotted line\n",
    "            plt.plot(row['distances'], linestyle='dotted', alpha=0.3, color='grey', label=row['park_name'])\n",
    "        else:\n",
    "            plt.plot(row['distances'], linestyle='dashed', alpha=0.5, label=row['park_name'])\n",
    "\n",
    "        # Add text labels for Min, Max and Mean lines\n",
    "        if row['park_name'] in ['Min', 'Max', 'Mean']:\n",
    "            plt.text(len(row['distances'])-1, row['distances'][-1], row['park_name'], color='blue', va='center')\n",
    "\n",
    "        # Check if the current row is 'Min', if so, add shading\n",
    "        if row['park_name'] == 'Min':\n",
    "            plt.fill_between(range(len(row['distances'])), row['distances'], color='green', alpha=0.2)\n",
    "\n",
    "        # Check if the current row is 'Max', if so, add shading\n",
    "        if row['park_name'] == 'Max':\n",
    "            plt.fill_between(range(len(row['distances'])), row['distances'], color='yellow', alpha=0.2)\n",
    "\n",
    "        # Check if the current row is 'Max', if so, add shading above\n",
    "        if row['park_name'] == 'Max':\n",
    "            plt.fill_between(range(len(row['distances'])), plt.ylim()[1], row['distances'], color='red', alpha=0.2)\n",
    "            \n",
    "    # Plot the highlighted row with a thicker line\n",
    "    plt.plot(highlighted_row['distances'], linewidth=2, label=highlighted_row['park_name'])\n",
    "    \n",
    "    # Set the minimum and maximum values of y-axis\n",
    "    plt.ylim([270, 420])\n",
    "\n",
    "    # Change y-axis labels and tick marks to be white\n",
    "    plt.ylabel('Distance (feet)', color='white')\n",
    "    plt.tick_params(axis='y', colors='white')\n",
    "\n",
    "    # Hide x axis ticks\n",
    "    plt.xticks([])\n",
    "\n",
    "    # Move the title to the inside the plot, centered, just above the x axis\n",
    "    plt.text(len(highlighted_row['distances'])/2, 270, highlighted_row['display_name'], ha='center', va='bottom', fontsize=16)\n",
    "\n",
    "    # Reverse the x-axis\n",
    "    plt.gca().invert_xaxis()\n",
    "\n",
    "    # Generate the file path\n",
    "    file_path = os.path.join(output_dir, f\"plot_{row_index}.png\")\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(file_path)\n",
    "\n",
    "    # Close the figure to free up memory\n",
    "    plt.close()\n",
    "\n",
    "    # Return the file path\n",
    "    return file_path\n",
    "\n",
    "# Add a new column 'file_path' to the DataFrame to store the file paths\n",
    "df['file_path'] = [plot_distances(df, i) for i in df.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df\n",
    "\n",
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the rows with null values in the 'fop', 'foul' or 'home_plate' columns\n",
    "df.dropna(subset=['fop', 'foul', 'home_plate'], inplace=True)\n",
    "\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show me the fields without a display name\n",
    "\n",
    "df_merged[df_merged['display_name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### outpus csv to check\n",
    "df_merged.to_csv('data/MHSAA_FINAL_TEST.csv', index=False)\n",
    "\n",
    "### OUTPUT JSON TO USE IN MAP\n",
    "df_merged.to_json('data/html/mhsaa/data/map.json', orient='records')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_merged.iloc[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find Lowell High School in final Dataframe\n",
    "\n",
    "df_parks[df_parks['park_name'] == 'Lowell High School - high school']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END BLOCK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Do a FUZZY MATCH OF DF_HOSTS AND DF_PARKS\n",
    "# # Debugging step to check for non-string values in host_teams\n",
    "# for team in host_teams:\n",
    "#     if not isinstance(team, str):\n",
    "#         print(f\"Non-string value found in host_teams: {team}\")\n",
    "\n",
    "# # Debugging step to check for non-string values in park_names\n",
    "# for park in park_names:\n",
    "#     if not isinstance(park, str):\n",
    "#         print(f\"Non-string value found in park_names: {park}\")\n",
    "\n",
    "# # Continue with fuzzy matching if no non-string values are found\n",
    "# matches = [(team, process.extractOne(team, park_names)) for team in host_teams]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## List the values from the team column\n",
    "\n",
    "# print(len(df_hosts['team'].unique()))\n",
    "# df_hosts['team'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parks = df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_df = df_cleaned.copy()\n",
    "\n",
    "parks_df.info()\n",
    "host_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(parks_df, host_df, min_score=90):\n",
    "    dict_list = []\n",
    "    unmatched_rows = []\n",
    "    # Drop NaN values in 'team' and 'park_name' before matching\n",
    "    host_names = host_df.team.dropna().unique()\n",
    "    for name in parks_df.park_name.dropna():  # ignore NaN values\n",
    "        match = match_team(name, host_names, min_score)\n",
    "        \n",
    "        # If no match found, add to unmatched_rows and continue to next iteration\n",
    "        if match[0] == \"\":\n",
    "            unmatched_rows.append(name)\n",
    "            continue\n",
    "\n",
    "        dict_ = {}\n",
    "        dict_.update({\"park_name_parks\" : name})\n",
    "        dict_.update({\"match_name_host\" : match[0]})\n",
    "        dict_.update({\"score\" : match[1]})\n",
    "        dict_list.append(dict_)\n",
    "\n",
    "    merge_table = pd.DataFrame(dict_list)\n",
    "    \n",
    "    # Remove duplicates in merge_table, keeping only the row with the highest score\n",
    "    merge_table = merge_table.sort_values('score', ascending=False).drop_duplicates(['park_name_parks'], keep='first')\n",
    "\n",
    "    if 'match_name_host' in merge_table.columns:\n",
    "        merged_df = pd.merge(parks_df, merge_table, left_on='park_name', right_on='park_name_parks', how='left')\n",
    "        merged_df = pd.merge(merged_df, host_df, left_on='match_name_host', right_on='team', how='left')\n",
    "    else:\n",
    "        print(\"No matches found.\")\n",
    "        merged_df = None\n",
    "\n",
    "    return merged_df, unmatched_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = merged_df.sort_values('score', ascending=False).drop_duplicates(subset=['park_name', 'team'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df, unmatched_rows = merge_dataframes(parks_df, host_df, min_score=90)\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we first match each team name in host_df with the park names in parks_df. If a match with a similarity score greater than the threshold (85 in your case) is found, we record the match in merge_table. If no match is found, we record the team name in unmatched_rows. After going through all team names, we merge host_df and parks_df based on the matches in merge_table.\n",
    "\n",
    "The function merge_dataframes returns two objects. The first object, merged_df, is a DataFrame that contains the merged data. The second object, unmatched_rows, is a list of team names in host_df for which no match in parks_df could be found. You can inspect unmatched_rows to see which rows couldn't be matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(parks_df, host_df, min_score=90):\n",
    "    dict_list = []\n",
    "    unmatched_rows = []\n",
    "    # Drop NaN values in 'team' and 'park_name' before matching\n",
    "    host_names = host_df.team.dropna().unique()\n",
    "    for name in parks_df.park_name.dropna():  # ignore NaN values\n",
    "        match = match_team(name, host_names, min_score)\n",
    "        \n",
    "        # If no match found, add to unmatched_rows and continue to next iteration\n",
    "        if match[0] == \"\":\n",
    "            unmatched_rows.append(name)\n",
    "            continue\n",
    "\n",
    "        dict_ = {}\n",
    "        dict_.update({\"park_name_parks\" : name})\n",
    "        dict_.update({\"match_name_host\" : match[0]})\n",
    "        dict_.update({\"score\" : match[1]})\n",
    "        dict_list.append(dict_)\n",
    "\n",
    "    merge_table = pd.DataFrame(dict_list)\n",
    "    \n",
    "    # Remove duplicates in merge_table, keeping only the row with the highest score\n",
    "    merge_table = merge_table.sort_values('score', ascending=False).drop_duplicates(['park_name_parks'], keep='first')\n",
    "\n",
    "    if 'match_name_host' in merge_table.columns:\n",
    "        merged_df = pd.merge(parks_df, merge_table, left_on='park_name', right_on='park_name_parks', how='left')\n",
    "        merged_df = pd.merge(merged_df, host_df, left_on='match_name_host', right_on='team', how='left')\n",
    "    else:\n",
    "        print(\"No matches found.\")\n",
    "        merged_df = None\n",
    "\n",
    "    return merged_df, unmatched_rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORKING HERE DOWN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here down are simple plots to do spot check of data and hold example of polar chart\n",
    "\n",
    "### FILL IN THE REST OF JSON WITH THE DATA FOR THE 2023 TOURNEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Build the file path using os.path.join\n",
    "file_path = os.path.join('data', 'html', 'mhsaa', 'data', 'tourney_2023.json')\n",
    "\n",
    "# Save the dataframe to JSON using the constructed file path\n",
    "parks_df.to_json(file_path, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the max distance, min distance, average distance, and median distance\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "ax[0, 0].hist(df_cleaned['max_distance'], bins=20)\n",
    "\n",
    "ax[0, 1].hist(df_cleaned['min_distance'], bins=20)\n",
    "\n",
    "ax[1, 0].hist(df_cleaned['avg_distance'], bins=20)\n",
    "\n",
    "ax[1, 1].hist(df_cleaned['median_distance'], bins=20)\n",
    "\n",
    "ax[0, 0].set_title('Max Distance')\n",
    "ax[0, 1].set_title('Min Distance')\n",
    "\n",
    "ax[1, 0].set_title('Average Distance')\n",
    "ax[1, 1].set_title('Median Distance')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile a list of fields that are outliers\n",
    "\n",
    "outlier_fields = df_cleaned[(df_cleaned['max_distance'] > 400) | (df_cleaned['min_distance'] < 200) | (df_cleaned['avg_distance'] > 400) | (df_cleaned['median_distance'] > 400)]\n",
    "\n",
    "len(outlier_fields)\n",
    "\n",
    "print(outlier_fields['park_name'].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW WITH AUTO SCALING\n",
    "\n",
    "def calculate_max_y(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    return max(bin_counts)\n",
    "\n",
    "\n",
    "def create_polar_chart(data, num_bins=36, level_filter=None, y_min=-20, background_color='#2b2b2b', color_map=plt.cm.viridis, bar_alpha=0.8):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 2 * np.pi / num_bins\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "\n",
    "    ax.set_facecolor('#808080')\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "    # Set dark background\n",
    "    ax.set_facecolor(background_color)\n",
    "    plt.gca().set_rlabel_position(22.5)\n",
    "    y_max = calculate_max_y(data, num_bins=num_bins, level_filter=level_filter) + 5\n",
    "    ax.set_ylim(y_min, y_max)  # Adjust based on max count\n",
    "\n",
    "    # Add bars for negative values\n",
    "    zero_height_bars = ax.bar(bin_edges[:-1], np.abs(ax.get_ylim()[0]), width=bin_width, bottom=0.0, color='k', alpha=0.3)\n",
    "\n",
    "    bars = ax.bar(bin_edges[:-1], bin_counts, width=bin_width, bottom=0)\n",
    "    \n",
    "    # Use custom colors and opacity\n",
    "    for r, bar in zip(bin_counts, bars):\n",
    "        bar.set_facecolor(color_map(r / max(bin_counts)))\n",
    "        bar.set_alpha(bar_alpha)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a polar chart showing the direction of all the tournment fields\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a function to process the data, counting the orientations and filtering by level.\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_data(data, level_filter=None):\n",
    "    count_by_orientation = defaultdict(int)\n",
    "    \n",
    "    for record in data:\n",
    "        if level_filter is None or record['level'] == level_filter:\n",
    "            orientation = round(record['field_orientation'])\n",
    "            count_by_orientation[orientation] += 1\n",
    "\n",
    "    return count_by_orientation\n",
    "\n",
    "def create_polar_chart(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 2 * np.pi / num_bins\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "\n",
    "    ax.set_facecolor('#808080')\n",
    "    ###\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "    \n",
    "    # # Set dark background\n",
    "    ax.set_facecolor('#2b2b2b')\n",
    "    plt.gca().set_rlabel_position(22.5)\n",
    "    ax.set_ylim(-20, 130)  # Adjust based on max count\n",
    "\n",
    "    # Add bars for negative values\n",
    "    zero_height_bars = ax.bar(bin_edges[:-1], np.abs(ax.get_ylim()[0]), width=bin_width, bottom=0.0, color='k', alpha=0.3)\n",
    "\n",
    "    bars = ax.bar(bin_edges[:-1], bin_counts, width=bin_width, bottom=0)\n",
    "    \n",
    "    # Use custom colors and opacity\n",
    "    for r, bar in zip(bin_counts, bars):\n",
    "        bar.set_facecolor(plt.cm.viridis(r / max(bin_counts)))\n",
    "        # bar.set_facecolor(plt.cm.plasma(r / max(bin_counts)))\n",
    "        bar.set_alpha(0.8)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_polar_chart(data, num_bins=50, level_filter=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
