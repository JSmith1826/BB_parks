{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from   bs4 import BeautifulSoup\n",
    "from random import choice\n",
    "import requests\n",
    "from time import sleep\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from time import sleep\n",
    "## import the timedelta function from the datetime module\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from datetime import date   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESPN SIMPLE SCORE BOARD GOES BACK TO 2015\n",
    "\n",
    "# years to scrape 2016-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a loop through the years\n",
    "for year in tqdm(range(2016,2022)):\n",
    "    # year = str(year)\n",
    "\n",
    "    \n",
    "    # Helper function to generate dates\n",
    "    def daterange(start_date, end_date):\n",
    "        for n in range(int((end_date - start_date).days)):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Start and end dates\n",
    "    start_date = date(year, 2, 1)\n",
    "    end_date = date(year, 6, 30)  # today's date\n",
    "\n",
    "    base_url = 'https://www.espn.com/college-baseball/scoreboard/_/date/'\n",
    "    game_data = []  # this list will hold our game data\n",
    "\n",
    "    # Loop over each date\n",
    "    for single_date in tqdm(daterange(start_date, end_date)):\n",
    "        url = base_url + single_date.strftime(\"%Y%m%d\")\n",
    "        \n",
    "        # Try accessing the page\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # raise exception if invalid response\n",
    "        except (requests.HTTPError, requests.ConnectionError):\n",
    "            # Handle the exception if it occurs\n",
    "            print(f\"No data for {single_date.strftime('%Y-%m-%d')}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "\n",
    "        # Find all game sections\n",
    "        game_sections = soup.find_all('section', class_='Scoreboard bg-clr-white flex flex-auto justify-between')\n",
    "\n",
    "        # For each game section, get the required data\n",
    "        # For each game section, get the required data\n",
    "        for section in game_sections:   \n",
    "            game = {}    \n",
    "\n",
    "            ## Add the date to the game data\n",
    "            game['date'] = single_date.strftime('%Y-%m-%d')\n",
    "\n",
    "            team_sections = section.find_all('div', class_='ScoreCell__TeamName ScoreCell__TeamName--shortDisplayName truncate db')\n",
    "            if team_sections is not None and len(team_sections) == 2:\n",
    "                away_team = team_sections[0].text\n",
    "                home_team = team_sections[1].text\n",
    "                game['away_team'] = away_team\n",
    "                game['home_team'] = home_team\n",
    "\n",
    "\n",
    "            \n",
    "            # Find the link to the box score and extract the game Id\n",
    "            box_score_link = section.find('a', text='Box Score')\n",
    "            if box_score_link is not None:\n",
    "                game['game_id'] = box_score_link['href'].split('/')[-1]\n",
    "\n",
    "            game_info = section.find('div', class_='ScoreboardScoreCell__Note clr-gray-04 n9 w-auto ml0')\n",
    "            if game_info is not None:\n",
    "                game['game_info'] = game_info.text\n",
    "                \n",
    "            game_run_elements = section.find_all('div', class_='ScoreboardScoreCell__Value flex justify-center pl2 baseball')\n",
    "            if game_run_elements is not None and len(game_run_elements) >= 4:  # 4 because we have runs, hits, and errors for both teams\n",
    "                game['away_team_runs'] = game_run_elements[0].text\n",
    "                game['away_team_hits'] = game_run_elements[1].text\n",
    "                game['away_team_errors'] = game_run_elements[2].text\n",
    "                game['home_team_runs'] = game_run_elements[3].text\n",
    "                game['home_team_hits'] = game_run_elements[4].text\n",
    "                game['home_team_errors'] = game_run_elements[5].text\n",
    "\n",
    "\n",
    "            # Add game data to the list\n",
    "            game_data.append(game)\n",
    "\n",
    "            sleep(.1)  # sleep for 100 milliseconds to avoid spamming the server\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Convert the list of games to a DataFrame\n",
    "    df = pd.DataFrame(game_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to csv\n",
    "df.to_csv(f'./data/espn_college_baseball_2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "\n",
    "## desccribe the date column\n",
    "df['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Helper function to generate dates\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "from datetime import date, timedelta\n",
    "\n",
    "year = 2021\n",
    "# Start and end dates\n",
    "start_date = date(year, 2, 1)\n",
    "end_date = date(year, 6, 30)  # today's date\n",
    "\n",
    "base_url = 'https://www.espn.com/college-baseball/scoreboard/_/date/'\n",
    "game_data = []  # this list will hold our game data\n",
    "\n",
    "# Loop over each date\n",
    "for single_date in tqdm(daterange(start_date, end_date)):\n",
    "    url = base_url + single_date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # Try accessing the page\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # raise exception if invalid response\n",
    "    except (requests.HTTPError, requests.ConnectionError):\n",
    "        # Handle the exception if it occurs\n",
    "        print(f\"No data for {single_date.strftime('%Y-%m-%d')}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "\n",
    "    # Find all game sections\n",
    "    game_sections = soup.find_all('section', class_='Scoreboard bg-clr-white flex flex-auto justify-between')\n",
    "\n",
    "    # For each game section, get the required data\n",
    "    # For each game section, get the required data\n",
    "    for section in game_sections:   \n",
    "        game = {}    \n",
    "\n",
    "        ## Add the date to the game data\n",
    "        game['date'] = single_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        team_sections = section.find_all('div', class_='ScoreCell__TeamName ScoreCell__TeamName--shortDisplayName truncate db')\n",
    "        if team_sections is not None and len(team_sections) == 2:\n",
    "            away_team = team_sections[0].text\n",
    "            home_team = team_sections[1].text\n",
    "            game['away_team'] = away_team\n",
    "            game['home_team'] = home_team\n",
    "\n",
    "\n",
    "        \n",
    "        # Find the link to the box score and extract the game Id\n",
    "        box_score_link = section.find('a', text='Box Score')\n",
    "        if box_score_link is not None:\n",
    "            game['game_id'] = box_score_link['href'].split('/')[-1]\n",
    "\n",
    "        game_info = section.find('div', class_='ScoreboardScoreCell__Note clr-gray-04 n9 w-auto ml0')\n",
    "        if game_info is not None:\n",
    "            game['game_info'] = game_info.text\n",
    "            \n",
    "        game_run_elements = section.find_all('div', class_='ScoreboardScoreCell__Value flex justify-center pl2 baseball')\n",
    "        if game_run_elements is not None and len(game_run_elements) >= 4:  # 4 because we have runs, hits, and errors for both teams\n",
    "            game['away_team_runs'] = game_run_elements[0].text\n",
    "            game['away_team_hits'] = game_run_elements[1].text\n",
    "            game['away_team_errors'] = game_run_elements[2].text\n",
    "            game['home_team_runs'] = game_run_elements[3].text\n",
    "            game['home_team_hits'] = game_run_elements[4].text\n",
    "            game['home_team_errors'] = game_run_elements[5].text\n",
    "\n",
    "\n",
    "        # Add game data to the list\n",
    "        game_data.append(game)\n",
    "\n",
    "        sleep(.1)  # sleep for 100 milliseconds to avoid spamming the server\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the list of games to a DataFrame\n",
    "df = pd.DataFrame(game_data)\n",
    "\n",
    "# Dropr rows wuth nulls in game id\n",
    "\n",
    "# df = df.dropna(subset=['game_id'])\n",
    "\n",
    "\n",
    "\n",
    "# Print DataFrame\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the list of games to a DataFrame\n",
    "# df = pd.DataFrame(game_data)\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save data for reference\n",
    "df.to_csv('espn_college_baseball_2021.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take all ove the csv files from this directory TEMP\\NCAA_Simple\n",
    "# and combine them into one csv file\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "os.chdir(\"TEMP/NCAA_Simple\")\n",
    "extension = 'csv'\n",
    "\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "# print(all_filenames)\n",
    "\n",
    "## concatenate all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv.info()\n",
    "\n",
    "# Save to a new file\n",
    "combined_csv.to_csv( \"ncaa_Baseball_simple_2016-2022.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reload the combined csv file\n",
    "combined_csv = pd.read_csv('TEMP/NCAA_Simple/ncaa_Baseball_simple_2016-2022.csv')\n",
    "\n",
    "### # Drop any rows without ame_ids\n",
    "new_df = combined_csv.dropna(subset=['game_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## RELOAD THE DATA FOR CLEANING\n",
    "import pandas as pd\n",
    "from   bs4 import BeautifulSoup\n",
    "from random import choice\n",
    "import requests\n",
    "from time import sleep\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "from fake_useragent import UserAgent\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# ## RELOAD THE DATA FOR CLEANING\n",
    "# df = pd.read_csv('espn_college_baseball_etl_1.csv')\n",
    "\n",
    "# ## LOAD DIFFERENT ONE\n",
    "# # df = pd.read_csv('TEMP/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remane the df that has all of the 2016-2022 games with game_ids\n",
    "df = new_df\n",
    "\n",
    "df.info()\n",
    "\n",
    "# df.head()\n",
    "\n",
    "## date histogram\n",
    "# df['date'].hist(bins=20)\n",
    "\n",
    "# ## line graph of games per day\n",
    "# df['date'].value_counts().plot.line()\n",
    "\n",
    "\n",
    "## create a list from the games_ids\n",
    "game_ids = df['game_id'].tolist()\n",
    "# randomize the list\n",
    "random.shuffle(game_ids)\n",
    "# Make sure they are strings\n",
    "game_ids = [str(x) for x in game_ids]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show 5 game_ids\n",
    "game_ids[:5]\n",
    "\n",
    "# remove the .0 from the end of the game_ids\n",
    "game_ids = [x[:-2] for x in game_ids]\n",
    "\n",
    "game_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_game_url = 'https://www.espn.com/college-baseball/boxscore/_/gameId/'\n",
    "scraper_base = \"http://api.scraperapi.com?api_key=7efbd1c22cb922745d8db50ce534d460&url=\"\n",
    "\n",
    "\n",
    "######### SETTINGS #########\n",
    "####################################################################\n",
    "# List of urls\n",
    "urls = [scraper_base + base_game_url + game_id for game_id in game_ids]\n",
    "\n",
    "\n",
    "\n",
    "save_interval = 10\n",
    "request_counter = 0\n",
    "\n",
    "# Initialize DataFrame for results\n",
    "results = pd.DataFrame(columns=['location', 'date', 'date2', 'time', 'team_1', 'team_2', 'runs_1', 'hits_1', 'errors_1', 'home_runs_1', 'runs_2', 'hits_2', 'errors_2', 'home_runs_2'])\n",
    "\n",
    "# Initialize UserAgent\n",
    "ua = UserAgent()\n",
    "\n",
    "# You have to define proxy and headers before using them\n",
    "proxy = {}  # replace this with your proxy dictionary\n",
    "headers = {\"User-Agent\": ua.random}\n",
    "\n",
    "for url in tqdm(urls):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Increment the request counter\n",
    "            request_counter += 1\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Load tables from url content\n",
    "            tables = pd.read_html(response.content)\n",
    "\n",
    "            # The second table contains the data we're interested in\n",
    "            score_table = tables[1]\n",
    "\n",
    "            # The team names are in the first table\n",
    "            team_table = tables[0]\n",
    "            team_1 = team_table.iloc[0, 0]\n",
    "            team_2 = team_table.iloc[1, 0]\n",
    "\n",
    "            # Extract the runs, hits, and errors for each team\n",
    "            runs_1, hits_1, errors_1 = score_table.iloc[0, -3:]\n",
    "            runs_2, hits_2, errors_2 = score_table.iloc[1, -3:]\n",
    "\n",
    "            # The team's home runs are in tables 3 and 9\n",
    "            # Convert the 'HR' column to numeric before summing\n",
    "            home_runs_1 = pd.to_numeric(tables[3]['HR'].iloc[-1])  # Sum the HR column for team 1\n",
    "            home_runs_2 = pd.to_numeric(tables[5]['HR'].iloc[-1])  # Sum the HR column for team 2\n",
    "            home_runs = home_runs_1 + home_runs_2\n",
    "\n",
    "            location = soup.find('div', {'class': 'n6 clr-gray-03 GameInfo__Location__Name--noImg'}).text\n",
    "            meta = soup.find('div', {'class': 'n8 GameInfo__Meta'}).text.split(',')\n",
    "            time = meta[0].strip()\n",
    "            date2 = meta[1].strip()\n",
    "\n",
    "            # Add the results to the results dataframe\n",
    "            results = results.append({\n",
    "                'date2': date2, 'location': location, 'time': time,\n",
    "                'team_1': team_1, 'team_2': team_2,\n",
    "                'runs_1': runs_1, 'hits_1': hits_1, 'errors_1': errors_1, \n",
    "                'runs_2': runs_2, 'hits_2': hits_2, 'errors_2': errors_2,\n",
    "                \n",
    "                'home_runs_1': home_runs_1, 'home_runs_2': home_runs_2, 'home_runs': home_runs,\n",
    "            }, ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {url}\")\n",
    "            continue\n",
    "\n",
    "        sleep(.1)  # waits for 1 second\n",
    "\n",
    "        # If we've hit the saving interval, write the DataFrame to a CSV\n",
    "        if request_counter % save_interval == 0:\n",
    "            results.to_csv(f'TEMP/NCAA_Partial/{request_counter}_data_saved.csv', index=False)\n",
    "            print(f\"Data saved to CSV after {request_counter} requests.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.tail()\n",
    "\n",
    "results.info()\n",
    "\n",
    "## Sum of home runs\n",
    "results['home_runs'].sum()\n",
    "\n",
    "## location value counts\n",
    "results['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save to CSV\n",
    "\n",
    "## Partial file because of the http error\n",
    "results.to_csv('TEMP/collegebaseball_scrape_2016-2022_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
