{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from   bs4 import BeautifulSoup\n",
    "from random import choice\n",
    "import requests\n",
    "from time import sleep\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Helper function to generate dates\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "from datetime import date, timedelta\n",
    "# Start and end dates\n",
    "start_date = date(2023, 2, 17)\n",
    "end_date = date(2023, 6, 30)  # today's date\n",
    "\n",
    "base_url = 'https://www.espn.com/college-baseball/scoreboard/_/date/'\n",
    "game_data = []  # this list will hold our game data\n",
    "\n",
    "# Loop over each date\n",
    "for single_date in tqdm(daterange(start_date, end_date)):\n",
    "    url = base_url + single_date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # Try accessing the page\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # raise exception if invalid response\n",
    "    except (requests.HTTPError, requests.ConnectionError):\n",
    "        # Handle the exception if it occurs\n",
    "        print(f\"No data for {single_date.strftime('%Y-%m-%d')}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Find all game sections\n",
    "    game_sections = soup.find_all('section', class_='Scoreboard bg-clr-white flex flex-auto justify-between')\n",
    "\n",
    "    # For each game section, get the required data\n",
    "    for section in game_sections:   \n",
    "        game = {}\n",
    "\n",
    "        ## Add the date to the game data\n",
    "        game['date'] = single_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Find the link to the box score and extract the game Id\n",
    "        box_score_link = section.find('a', text='Box Score')\n",
    "        if box_score_link is not None:\n",
    "            game['game_id'] = box_score_link['href'].split('/')[-1]\n",
    "\n",
    "        game_info = section.find('div', class_='ScoreboardScoreCell__Note clr-gray-04 n9 w-auto ml0')\n",
    "        if game_info is not None:\n",
    "            game['game_info'] = game_info.text\n",
    "            \n",
    "        game_run_elements = section.find_all('div', class_='ScoreboardScoreCell__Value flex justify-center pl2 baseball')\n",
    "        if game_run_elements is not None and len(game_run_elements) >= 4:  # 4 because we have runs, hits, and errors for both teams\n",
    "            game['away_team_runs'] = game_run_elements[0].text\n",
    "            game['home_team_runs'] = game_run_elements[3].text\n",
    "\n",
    "                    # Add game data to the list\n",
    "        game_data.append(game)\n",
    "\n",
    "        sleep(.1)  # sleep for 100 milliseconds to avoid spamming the server\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert the list of games to a DataFrame\n",
    "df = pd.DataFrame(game_data)\n",
    "\n",
    "# Dropr rows wuth nulls in game id\n",
    "\n",
    "df = df.dropna(subset=['game_id'])\n",
    "\n",
    "\n",
    "\n",
    "# Print DataFrame\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save data for reference\n",
    "df.to_csv('espn_college_baseball_etl_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RELOAD THE DATA FOR CLEANING\n",
    "import pandas as pd\n",
    "from   bs4 import BeautifulSoup\n",
    "from random import choice\n",
    "import requests\n",
    "from time import sleep\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "## RELOAD THE DATA FOR CLEANING\n",
    "df = pd.read_csv('espn_college_baseball_etl_1.csv')\n",
    "\n",
    "## LOAD DIFFERENT ONE\n",
    "# df = pd.read_csv('TEMP/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1190 entries, 0 to 1189\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   date            1190 non-null   object\n",
      " 1   game_id         1190 non-null   int64 \n",
      " 2   away_team_runs  1190 non-null   int64 \n",
      " 3   home_team_runs  1190 non-null   int64 \n",
      " 4   game_info       229 non-null    object\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 46.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "\n",
    "# df.head()\n",
    "\n",
    "## date histogram\n",
    "# df['date'].hist(bins=20)\n",
    "\n",
    "# ## line graph of games per day\n",
    "# df['date'].value_counts().plot.line()\n",
    "\n",
    "\n",
    "## create a list from the games_ids\n",
    "game_ids = df['game_id'].tolist()\n",
    "# randomize the list\n",
    "random.shuffle(game_ids)\n",
    "# Make sure they are strings\n",
    "game_ids = [str(x) for x in game_ids]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1190 [00:00<?, ?it/s]C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_20012\\1700731311.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n",
      "  0%|          | 1/1190 [00:02<49:58,  2.52s/it]C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_20012\\1700731311.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n",
      "  0%|          | 2/1190 [00:05<59:36,  3.01s/it]C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_20012\\1700731311.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n",
      "  0%|          | 3/1190 [00:07<46:01,  2.33s/it]C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_20012\\1700731311.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n",
      "  0%|          | 4/1190 [00:09<45:32,  2.30s/it]C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_20012\\1700731311.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n",
      "  0%|          | 5/1190 [00:10<37:26,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to CSV after 5 requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_20012\\1700731311.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n",
      "  1%|          | 6/1190 [00:15<59:01,  2.99s/it]C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_20012\\1700731311.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n",
      "  1%|          | 7/1190 [00:17<51:35,  2.62s/it]C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_20012\\1700731311.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n",
      "  1%|          | 8/1190 [00:18<42:08,  2.14s/it]C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_20012\\1700731311.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n",
      "  1%|          | 9/1190 [00:20<39:07,  1.99s/it]C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_20012\\1700731311.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n",
      "  1%|          | 10/1190 [00:21<33:01,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to CSV after 10 requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_20012\\1700731311.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n",
      "  1%|          | 11/1190 [00:26<49:49,  2.54s/it]C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_20012\\1700731311.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n",
      "  1%|          | 12/1190 [00:48<2:46:37,  8.49s/it]"
     ]
    }
   ],
   "source": [
    "base_game_url = 'https://www.espn.com/college-baseball/boxscore/_/gameId/'\n",
    "scraper_base = \"http://api.scraperapi.com?api_key=7efbd1c22cb922745d8db50ce534d460&url=\"\n",
    "\n",
    "\n",
    "######### SETTINGS #########\n",
    "####################################################################\n",
    "# List of urls\n",
    "urls = [scraper_base + base_game_url + game_id for game_id in game_ids]\n",
    "\n",
    "\n",
    "\n",
    "save_interval = 5\n",
    "request_counter = 0\n",
    "\n",
    "# Initialize DataFrame for results\n",
    "results = pd.DataFrame(columns=['location', 'date', 'date2', 'time', 'team_1', 'team_2', 'runs_1', 'hits_1', 'errors_1', 'home_runs_1', 'runs_2', 'hits_2', 'errors_2', 'home_runs_2'])\n",
    "\n",
    "# Initialize UserAgent\n",
    "ua = UserAgent()\n",
    "\n",
    "# You have to define proxy and headers before using them\n",
    "proxy = {}  # replace this with your proxy dictionary\n",
    "headers = {\"User-Agent\": ua.random}\n",
    "\n",
    "for url in tqdm(urls):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Increment the request counter\n",
    "            request_counter += 1\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Load tables from url content\n",
    "            tables = pd.read_html(response.content)\n",
    "\n",
    "            # The second table contains the data we're interested in\n",
    "            score_table = tables[1]\n",
    "\n",
    "            # The team names are in the first table\n",
    "            team_table = tables[0]\n",
    "            team_1 = team_table.iloc[0, 0]\n",
    "            team_2 = team_table.iloc[1, 0]\n",
    "\n",
    "            # Extract the runs, hits, and errors for each team\n",
    "            runs_1, hits_1, errors_1 = score_table.iloc[0, -3:]\n",
    "            runs_2, hits_2, errors_2 = score_table.iloc[1, -3:]\n",
    "\n",
    "            # The team's home runs are in tables 3 and 9\n",
    "            # Convert the 'HR' column to numeric before summing\n",
    "            home_runs_1 = pd.to_numeric(tables[3]['HR'].iloc[-1])  # Sum the HR column for team 1\n",
    "            home_runs_2 = pd.to_numeric(tables[5]['HR'].iloc[-1])  # Sum the HR column for team 2\n",
    "            home_runs = home_runs_1 + home_runs_2\n",
    "\n",
    "            location = soup.find('div', {'class': 'n6 clr-gray-03 GameInfo__Location__Name--noImg'}).text\n",
    "            meta = soup.find('div', {'class': 'n8 GameInfo__Meta'}).text.split(',')\n",
    "            time = meta[0].strip()\n",
    "            date2 = meta[1].strip()\n",
    "\n",
    "            # Add the results to the results dataframe\n",
    "            results = results.append({\n",
    "                'date2': date2, 'location': location, 'time': time,\n",
    "                'team_1': team_1, 'team_2': team_2,\n",
    "                'runs_1': runs_1, 'hits_1': hits_1, 'errors_1': errors_1, \n",
    "                'runs_2': runs_2, 'hits_2': hits_2, 'errors_2': errors_2,\n",
    "                \n",
    "                'home_runs_1': home_runs_1, 'home_runs_2': home_runs_2, 'home_runs': home_runs,\n",
    "            }, ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {url}\")\n",
    "            continue\n",
    "\n",
    "        sleep(.1)  # waits for 1 second\n",
    "\n",
    "        # If we've hit the saving interval, write the DataFrame to a CSV\n",
    "        if request_counter % save_interval == 0:\n",
    "            results.to_csv(f'TEMP/NCAA_Partial/{request_counter}_data_saved.csv', index=False)\n",
    "            print(f\"Data saved to CSV after {request_counter} requests.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.tail()\n",
    "\n",
    "results.info()\n",
    "\n",
    "## Sum of home runs\n",
    "results['home_runs'].sum()\n",
    "\n",
    "## location value counts\n",
    "results['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save to CSV\n",
    "\n",
    "## Partial file because of the http error\n",
    "results.to_csv('TEMP/SUNDAY_NIGHT_collegebaseball_scrape.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
