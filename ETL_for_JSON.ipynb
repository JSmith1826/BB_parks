{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dependencies\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is for extracting the data from the original kml file and outputing a JSON that will be easy to use with Google Maps API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD BLOCK###\n",
    "#### Load data from kml file exported from Google Earth\n",
    "\n",
    "file_path = ('data/kml/ballparks.kml') # file path to kml file\n",
    "\n",
    "with open(file_path) as file:\n",
    "\n",
    "    xml_data = file.read()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize soup variables for parsing file\n",
    "soup = BeautifulSoup(xml_data, 'xml')\n",
    "folders = soup.Document.Folder\n",
    "list = soup.Document.Folder.find_all('Folder')\n",
    "\n",
    "## Create a dataframe to hold the data parsed from xml\n",
    "df = pd.DataFrame(columns=['field', 'foul', 'fop'])\n",
    "\n",
    "failed = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing folder: Progressive Field - Cleveland - MLB. Error message: list index out of range\n",
      "Failed to process 1 folders: Progressive Field - Cleveland - MLB\n"
     ]
    }
   ],
   "source": [
    "#### EXTRACTION BLOCK ####\n",
    "#### Extract data from kml file\n",
    "\n",
    "# Create an empty list to store the rows to append to the DataFrame\n",
    "rows = []\n",
    "\n",
    "# Loop through the folders and extract the data\n",
    "for folder in list:\n",
    "    try:\n",
    "        field_name = folder.find('name').text\n",
    "        foul = folder.find_all('coordinates')[0].text\n",
    "        fop = folder.find_all('coordinates')[1].text\n",
    "\n",
    "        row = {\n",
    "            'field': field_name,\n",
    "            'foul': foul,\n",
    "            'fop': fop\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Add name of folder to a list of failed folders\n",
    "        failed.append(folder.find('name').text)\n",
    "        print(f\"Error processing folder: {folder.find('name').text}. Error message: {str(e)}\")\n",
    "\n",
    "# Convert the list of rows to a DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Print a list of failed folders\n",
    "print(f\"Failed to process {len(failed)} folders: {', '.join(failed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary that maps level indicators to levels and size factors\n",
    "level_dict = {\n",
    "    'International': 'international',\n",
    "    'Major Leagues': 'mlb', \n",
    "    'Professional': 'pro', \n",
    "    'College': 'college', \n",
    "    'High School': 'high_school',\n",
    "    'Youth': 'youth',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original DataFrame\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Remove new line and space characters from coordinates\n",
    "df_cleaned = df_cleaned.replace(r'\\n','', regex=True) \n",
    "df_cleaned = df_cleaned.replace(r'\\t','', regex=True) \n",
    "\n",
    "# Drop any duplicate rows\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['field'], keep='first')\n",
    "\n",
    "# Drop any rows with empty fields\n",
    "df_cleaned = df_cleaned[(df_cleaned != 0).all(1)]\n",
    "\n",
    "# Define the regex patterns for each level\n",
    "re_mlb = re.compile(r'mlb', re.IGNORECASE)\n",
    "re_pro = re.compile(r'pro|semi[-\\s]*pro', re.IGNORECASE)\n",
    "re_college = re.compile(r'college', re.IGNORECASE)\n",
    "re_high_school = re.compile(r'high school|hs', re.IGNORECASE)  # Include the abbreviation 'hs'\n",
    "re_youth = re.compile(r'youth', re.IGNORECASE)\n",
    "re_muni = re.compile(r'muni', re.IGNORECASE)\n",
    "re_international = re.compile(r'international', re.IGNORECASE)\n",
    "\n",
    "# Define a function to classify the fields based on the regex patterns\n",
    "def classify_field(field_name):\n",
    "    if re_mlb.search(field_name):\n",
    "        return 'Major League'\n",
    "    elif re_pro.search(field_name):\n",
    "        return 'Professional'\n",
    "    elif re_college.search(field_name):\n",
    "        return 'College'\n",
    "    elif re_high_school.search(field_name):\n",
    "        return 'High School'\n",
    "    elif re_youth.search(field_name):\n",
    "        return 'Youth'\n",
    "    elif re_muni.search(field_name):\n",
    "        return 'State / County / Municipal'\n",
    "    elif re_international.search(field_name):\n",
    "        return 'International'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Apply the classify_field function to the 'field' column\n",
    "df_cleaned['level'] = df_cleaned['field'].apply(classify_field)\n",
    "\n",
    "# Clean up the 'field' column by removing the level indicator and any trailing '-' characters\n",
    "level_regex = r'\\s*(%s)\\s*' % '|'.join(re.escape(level) for level in level_dict.values())\n",
    "df_cleaned['field'] = df_cleaned['field'].str.replace(level_regex, '', regex=True, flags=re.IGNORECASE)\n",
    "df_cleaned['field'] = df_cleaned['field'].str.replace(r'-\\s*$', '', regex=True)\n",
    "\n",
    "# Rename field column to park_name to avoid confusion down the line\n",
    "df_cleaned = df_cleaned.rename(columns={'field': 'park_name'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Professional' 'College' 'International' 'Major League' 'Unknown'\n",
      " 'High School' 'Youth' 'State / County / Municipal']\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.head()\n",
    "\n",
    "## print a list of all the values in the level column\n",
    "print(df_cleaned['level'].unique())\n",
    "\n",
    "## Print the two the headers and two rows of data to a txt file for reference\n",
    "\n",
    "\n",
    "# with open('data/rows.txt', 'w') as f:\n",
    "#     f.write(df_cleaned.iloc[0].to_string())\n",
    "#     f.write(df_cleaned.iloc[1].to_string())\n",
    "#     f.write(df_cleaned.iloc[14].to_string())\n",
    "    \n",
    "\n",
    "# print(df_cleaned.iloc[15].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Clean up polygon data and create a new home_plate column\n",
    "\n",
    "def parse_coordinates(coord_string):\n",
    "    coords = coord_string.split()\n",
    "    parsed_coords = [tuple(map(float, coord.split(',')[:2])) for coord in coords]\n",
    "    return parsed_coords\n",
    "\n",
    "# Create a new column for the home_plate location using the first set of coordinates in the 'fop' column\n",
    "df_cleaned['home_plate'] = df_cleaned['fop'].apply(lambda x: parse_coordinates(x)[0])\n",
    "\n",
    "# Apply the parse_coordinates function to the 'foul' and 'fop' columns\n",
    "df_cleaned['foul'] = df_cleaned['foul'].apply(parse_coordinates)\n",
    "df_cleaned['fop'] = df_cleaned['fop'].apply(parse_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doesn't seem to be returning useful data - will need to revisit\n",
    "\n",
    "# def determine_direction(coordinates):\n",
    "#     num_points = len(coordinates)\n",
    "#     if num_points < 2:\n",
    "#         return \"Not enough points\"\n",
    "\n",
    "#     # Get the latitude (y-coordinate) values\n",
    "#     latitudes = [point[1] for point in coordinates]\n",
    "\n",
    "#     # Check the change in latitude values\n",
    "#     increasing_latitudes = all(latitudes[i] <= latitudes[i+1] for i in range(num_points-1))\n",
    "#     decreasing_latitudes = all(latitudes[i] >= latitudes[i+1] for i in range(num_points-1))\n",
    "\n",
    "#     if increasing_latitudes:\n",
    "#         return \"Left Field\"\n",
    "#     elif decreasing_latitudes:\n",
    "#         return \"Right Field\"\n",
    "#     else:\n",
    "#         return \"Collinear\"\n",
    "\n",
    "# # Apply the determine_direction function to the 'fop' and 'foul' columns\n",
    "# df_cleaned['fop_direction'] = df_cleaned['fop'].apply(determine_direction)\n",
    "# df_cleaned['foul_direction'] = df_cleaned['foul'].apply(determine_direction)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park_name</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>level</th>\n",
       "      <th>home_plate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Day Air Ballpark - Dayton Dragons</td>\n",
       "      <td>[(-84.18557, 39.7642091), (-84.1849444, 39.764...</td>\n",
       "      <td>[(-84.18557, 39.7642091), (-84.1846064, 39.763...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>(-84.18557, 39.7642091)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Parkview Field - Fort Wayne TinCaps</td>\n",
       "      <td>[(-85.1429982, 41.0741133), (-85.1432208, 41.0...</td>\n",
       "      <td>[(-85.1429982, 41.0741133), (-85.1418777, 41.0...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>(-85.1429982, 41.0741133)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic Park- Lake County Captains</td>\n",
       "      <td>[(-81.4362001, 41.6407581), (-81.4362008, 41.6...</td>\n",
       "      <td>[(-81.4362001, 41.6407581), (-81.4350347, 41.6...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>(-81.4362001, 41.6407581)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABC Supply Stadium - Beloit Sky Carp</td>\n",
       "      <td>[(-89.0406651, 42.4971349), (-89.0408006, 42.4...</td>\n",
       "      <td>[(-89.0406651, 42.4971349), (-89.0394729, 42.4...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>(-89.0406651, 42.4971349)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Veterans Memorial Stadium - Cedar Rapids Kernels</td>\n",
       "      <td>[(-91.6867962, 41.9677456), (-91.6867801, 41.9...</td>\n",
       "      <td>[(-91.6867962, 41.9677456), (-91.6856006, 41.9...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>(-91.6867962, 41.9677456)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           park_name  \\\n",
       "0                 Day Air Ballpark - Dayton Dragons    \n",
       "1               Parkview Field - Fort Wayne TinCaps    \n",
       "2                Classic Park- Lake County Captains    \n",
       "3              ABC Supply Stadium - Beloit Sky Carp    \n",
       "4  Veterans Memorial Stadium - Cedar Rapids Kernels    \n",
       "\n",
       "                                                foul  \\\n",
       "0  [(-84.18557, 39.7642091), (-84.1849444, 39.764...   \n",
       "1  [(-85.1429982, 41.0741133), (-85.1432208, 41.0...   \n",
       "2  [(-81.4362001, 41.6407581), (-81.4362008, 41.6...   \n",
       "3  [(-89.0406651, 42.4971349), (-89.0408006, 42.4...   \n",
       "4  [(-91.6867962, 41.9677456), (-91.6867801, 41.9...   \n",
       "\n",
       "                                                 fop         level  \\\n",
       "0  [(-84.18557, 39.7642091), (-84.1846064, 39.763...  Professional   \n",
       "1  [(-85.1429982, 41.0741133), (-85.1418777, 41.0...  Professional   \n",
       "2  [(-81.4362001, 41.6407581), (-81.4350347, 41.6...  Professional   \n",
       "3  [(-89.0406651, 42.4971349), (-89.0394729, 42.4...  Professional   \n",
       "4  [(-91.6867962, 41.9677456), (-91.6856006, 41.9...  Professional   \n",
       "\n",
       "                  home_plate  \n",
       "0    (-84.18557, 39.7642091)  \n",
       "1  (-85.1429982, 41.0741133)  \n",
       "2  (-81.4362001, 41.6407581)  \n",
       "3  (-89.0406651, 42.4971349)  \n",
       "4  (-91.6867962, 41.9677456)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()\n",
    "\n",
    "## Value counts for the fop_direction column\n",
    "df_cleaned['level'].value_counts()\n",
    "\n",
    "## Value counts for the foul_direction column\n",
    "# df_cleaned['foul_direction'].value_counts()\n",
    "\n",
    "df_cleaned.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate Areas of Foul and FOP\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "import pyproj\n",
    "\n",
    "\n",
    "#### Functions to calculate the area of a polygon\n",
    "def utm_zone_number(lat, lon):\n",
    "    return int((lon + 180) / 6) + 1\n",
    "\n",
    "def utm_epsg_code(lat, lon):\n",
    "    zone_number = utm_zone_number(lat, lon)\n",
    "    if lat >= 0:\n",
    "        return 32600 + zone_number\n",
    "    else:\n",
    "        return 32700 + zone_number\n",
    "\n",
    "def calculate_area(coords):\n",
    "    # Create a Polygon object from the coordinates\n",
    "    polygon = Polygon(coords)\n",
    "\n",
    "    # Get the appropriate UTM EPSG code\n",
    "    epsg_code = utm_epsg_code(*coords[0])\n",
    "\n",
    "    # Create a transformer for converting coordinates to UTM\n",
    "    transformer = pyproj.Transformer.from_crs(\n",
    "        pyproj.CRS(\"EPSG:4326\"),  # WGS 84 (latitude and longitude)\n",
    "        pyproj.CRS(f\"EPSG:{epsg_code}\"),  # UTM zone\n",
    "        always_xy=True\n",
    "    )\n",
    "\n",
    "    # Convert the coordinates to UTM\n",
    "    coords_utm = [transformer.transform(*coord) for coord in coords]\n",
    "\n",
    "    # Create a Polygon object from the UTM coordinates\n",
    "    polygon_utm = Polygon(coords_utm)\n",
    "\n",
    "    # Calculate the area in square meters\n",
    "    area_sqm = polygon_utm.area\n",
    "\n",
    "    # Convert the area to square feet (1 square meter = 10.764 square feet)\n",
    "    area_sqft = area_sqm * 10.764\n",
    "\n",
    "    return area_sqft\n",
    "\n",
    "\n",
    "### Call Function and add to dataframe\n",
    "df_cleaned['foul_area_sqft'] = df_cleaned['foul'].apply(calculate_area)\n",
    "df_cleaned['fop_area_sqft'] = df_cleaned['fop'].apply(calculate_area)\n",
    "\n",
    "## Calculate the total area of the field and the ratio of foul area to field area\n",
    "df_cleaned['field_area_sqft'] = df_cleaned['foul_area_sqft'] + df_cleaned['fop_area_sqft']\n",
    "## Percentage foul area\n",
    "df_cleaned['foul_area_per'] = df_cleaned['foul_area_sqft'] / df_cleaned['field_area_sqft']\n",
    "## Fair to Foul Ratio\n",
    "df_cleaned['fair_to_foul'] = df_cleaned['fop_area_sqft'] / df_cleaned['foul_area_sqft']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SIMPLE DISTANCE CALCULATION USING GEOPY ####\n",
    "\n",
    "## Returns a list of distances from home plate to each outfield coordinate\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "def calculate_distances(home_plate, outfield_coords):\n",
    "    def is_same_point(point1, point2, tolerance=1e-6):\n",
    "        return abs(point1[0] - point2[0]) < tolerance and abs(point1[1] - point2[1]) < tolerance\n",
    "\n",
    "    home_plate_lat_lon = (home_plate[1], home_plate[0])\n",
    "    distances = [\n",
    "        round(great_circle(home_plate_lat_lon, (coord[1], coord[0])).feet)\n",
    "        for coord in outfield_coords\n",
    "        if not is_same_point(home_plate, coord)\n",
    "    ]\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run Distance Functions and add to dataframe\n",
    "\n",
    "# Calculate distances for each row\n",
    "df_cleaned['distances'] = df_cleaned.apply(lambda row: calculate_distances(row['home_plate'], row['fop']), axis=1)\n",
    "\n",
    "# Calculate max, min, and average distances for each row\n",
    "df_cleaned['max_distance'] = df_cleaned['distances'].apply(max)\n",
    "df_cleaned['min_distance'] = df_cleaned['distances'].apply(min)\n",
    "df_cleaned['avg_distance'] = df_cleaned['distances'].apply(lambda distances: sum(distances) / len(distances))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculate Ranks for each field\n",
    "### Grouped by level\n",
    "\n",
    "def rank_fields(df):\n",
    "    # Calculate the rank for each category\n",
    "    df['max_distance_rank'] = df['max_distance'].rank(ascending=False, method='min')\n",
    "    df['min_distance_rank'] = df['min_distance'].rank(ascending=False, method='min')\n",
    "    df['avg_distance_rank'] = df['avg_distance'].rank(ascending=False, method='min')\n",
    "    df['field_area_rank'] = df['field_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['ratio_rank'] = df['fair_to_foul'].rank(ascending=False, method='min')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Group the DataFrame by level and apply the rank_fields function to each group\n",
    "df_ranked = df_cleaned.groupby('level').apply(rank_fields)\n",
    "\n",
    "# Reset the index to get the original DataFrame structure\n",
    "df_ranked.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park_name</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>level</th>\n",
       "      <th>home_plate</th>\n",
       "      <th>foul_area_sqft</th>\n",
       "      <th>fop_area_sqft</th>\n",
       "      <th>field_area_sqft</th>\n",
       "      <th>foul_area_per</th>\n",
       "      <th>fair_to_foul</th>\n",
       "      <th>distances</th>\n",
       "      <th>max_distance</th>\n",
       "      <th>min_distance</th>\n",
       "      <th>avg_distance</th>\n",
       "      <th>max_distance_rank</th>\n",
       "      <th>min_distance_rank</th>\n",
       "      <th>avg_distance_rank</th>\n",
       "      <th>field_area_rank</th>\n",
       "      <th>ratio_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [park_name, foul, fop, level, home_plate, foul_area_sqft, fop_area_sqft, field_area_sqft, foul_area_per, fair_to_foul, distances, max_distance, min_distance, avg_distance, max_distance_rank, min_distance_rank, avg_distance_rank, field_area_rank, ratio_rank]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Show samples of the data from each level\n",
    "\n",
    "df_ranked[df_ranked['level'] == 'high_school'].head(10)\n",
    "\n",
    "# df_ranked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not sure if this is useful - will need to revisit\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def calculate_orientation(coordinates):\n",
    "#     num_points = len(coordinates)\n",
    "#     if num_points < 3:\n",
    "#         return \"Not enough points\"\n",
    "\n",
    "#     signed_area = 0\n",
    "\n",
    "#     for i in range(num_points):\n",
    "#         x1, y1 = coordinates[i]\n",
    "#         x2, y2 = coordinates[(i + 1) % num_points]\n",
    "#         signed_area += (x2 - x1) * (y2 + y1)\n",
    "\n",
    "#     if signed_area > 0:\n",
    "#         return \"Counterclockwise\"\n",
    "#     elif signed_area < 0:\n",
    "#         return \"Clockwise\"\n",
    "#     else:\n",
    "#         return \"Collinear\"\n",
    "\n",
    "# # Apply the calculate_orientation function to the 'fop' and 'foul' columns\n",
    "# df_cleaned['fop_orientation'] = df_cleaned['fop'].apply(calculate_orientation)\n",
    "# df_cleaned['foul_orientation'] = df_cleaned['foul'].apply(calculate_orientation)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Show value counts od orientation columns\n",
    "# print(df_cleaned['fop_orientation'].value_counts())\n",
    "# print(df_cleaned['foul_orientation'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Print the DataFrame to a string for reference\n",
    "\n",
    "# df_cleaned_str = df_cleaned.head().to_string()\n",
    "# print(df_cleaned_str)\n",
    "\n",
    "# # Output the string to a txt file for reference\n",
    "# with open('data/df_cleaned_str.txt', 'w') as f:\n",
    "#     f.write(df_cleaned_str)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Orienting the map to the home plate location ####\n",
    "\n",
    "### Find the center of the field\n",
    "def calculate_centroid(coords):\n",
    "    x_coords = [coord[0] for coord in coords]\n",
    "    y_coords = [coord[1] for coord in coords]\n",
    "    centroid_x = sum(x_coords) / len(coords)\n",
    "    centroid_y = sum(y_coords) / len(coords)\n",
    "    return (centroid_x, centroid_y)\n",
    "\n",
    "\n",
    "## Find the bearing between the home plate and the center of the field\n",
    "import math\n",
    "\n",
    "def calculate_bearing(point1, point2):\n",
    "    lat1, lon1 = math.radians(point1[1]), math.radians(point1[0])\n",
    "    lat2, lon2 = math.radians(point2[1]), math.radians(point2[0])\n",
    "\n",
    "    d_lon = lon2 - lon1\n",
    "\n",
    "    x = math.cos(lat2) * math.sin(d_lon)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(d_lon)\n",
    "\n",
    "    bearing = math.degrees(math.atan2(x, y))\n",
    "    bearing = (bearing + 360) % 360  # Normalize the bearing to the range [0, 360)\n",
    "\n",
    "    return bearing\n",
    "\n",
    "### Function to classify direction in laymans terms North, South, East, West, ect\n",
    "def degrees_to_cardinal_direction(degrees):\n",
    "    directions = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW', 'N']\n",
    "    index = round(degrees / 45)\n",
    "    return directions[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the centroid of the outfield fence coordinates for each row\n",
    "df_cleaned['fop_centroid'] = df_cleaned['fop'].apply(lambda coords: calculate_centroid(coords[1:]))\n",
    "\n",
    "# Calculate the bearing between home plate and the centroid for each row\n",
    "df_cleaned['field_orientation'] = df_cleaned.apply(lambda row: calculate_bearing(row['home_plate'], row['fop_centroid']), axis=1)\n",
    "\n",
    "# Convert the bearing to a cardinal direction\n",
    "df_cleaned['field_cardinal_direction'] = df_cleaned['field_orientation'].apply(degrees_to_cardinal_direction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rename the Cleaned Dataframe back to the default name\n",
    "df = df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### In the fop, foul and home_plate columns, the coordinates are in the format (longitude, latitude).\n",
    "# ### This is the opposite of the format that is used in Google Maps, so we need to reverse the order of the coordinates.\n",
    "# df['fop'] = df['fop'].apply(lambda coords: [(coord[1], coord[0]) for coord in coords])\n",
    "# df['foul'] = df['foul'].apply(lambda coords: [(coord[1], coord[0]) for coord in coords])\n",
    "# df['home_plate'] = df['home_plate'].apply(lambda coord: (coord[1], coord[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['park_name', 'foul', 'fop', 'level', 'home_plate', 'foul_area_sqft',\n",
      "       'fop_area_sqft', 'field_area_sqft', 'foul_area_per', 'fair_to_foul',\n",
      "       'distances', 'max_distance', 'min_distance', 'avg_distance',\n",
      "       'fop_centroid', 'field_orientation', 'field_cardinal_direction'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "### Create a dataframe of just high school fields\n",
    "hs_df = df[df['level'] == 'High School']\n",
    "\n",
    "### Create a dataframe of all other fields\n",
    "other_df = df[df['level'] != 'High School']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park_name</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>level</th>\n",
       "      <th>home_plate</th>\n",
       "      <th>foul_area_sqft</th>\n",
       "      <th>fop_area_sqft</th>\n",
       "      <th>field_area_sqft</th>\n",
       "      <th>foul_area_per</th>\n",
       "      <th>fair_to_foul</th>\n",
       "      <th>distances</th>\n",
       "      <th>max_distance</th>\n",
       "      <th>min_distance</th>\n",
       "      <th>avg_distance</th>\n",
       "      <th>fop_centroid</th>\n",
       "      <th>field_orientation</th>\n",
       "      <th>field_cardinal_direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>SW Christian HS Fort Worth - high school</td>\n",
       "      <td>[(-97.4421601, 32.6418776), (-97.4431985, 32.6...</td>\n",
       "      <td>[(-97.4421601, 32.6418776), (-97.4421306000000...</td>\n",
       "      <td>High School</td>\n",
       "      <td>(-97.4421601, 32.6418776)</td>\n",
       "      <td>37106.931904</td>\n",
       "      <td>169215.122852</td>\n",
       "      <td>206322.054756</td>\n",
       "      <td>0.179850</td>\n",
       "      <td>4.560202</td>\n",
       "      <td>[314, 337, 371, 342, 354, 370, 362, 407, 413, ...</td>\n",
       "      <td>413</td>\n",
       "      <td>314</td>\n",
       "      <td>357.272727</td>\n",
       "      <td>(-97.442824625, 32.64242071666667)</td>\n",
       "      <td>314.145471</td>\n",
       "      <td>NW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Soo High School</td>\n",
       "      <td>[(-84.36583, 46.4813183), (-84.3658206, 46.482...</td>\n",
       "      <td>[(-84.36583, 46.4813183), (-84.3645398, 46.481...</td>\n",
       "      <td>High School</td>\n",
       "      <td>(-84.36583, 46.4813183)</td>\n",
       "      <td>48045.311892</td>\n",
       "      <td>132184.170959</td>\n",
       "      <td>180229.482852</td>\n",
       "      <td>0.266579</td>\n",
       "      <td>2.751240</td>\n",
       "      <td>[324, 353, 357, 370, 373, 359, 326]</td>\n",
       "      <td>373</td>\n",
       "      <td>324</td>\n",
       "      <td>351.714286</td>\n",
       "      <td>(-84.3650061875, 46.4818190875)</td>\n",
       "      <td>48.561414</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Frankfort-Elberta Area HS</td>\n",
       "      <td>[(-86.2258505, 44.6344082), (-86.2258357, 44.6...</td>\n",
       "      <td>[(-86.2258505, 44.6344082), (-86.2270454, 44.6...</td>\n",
       "      <td>High School</td>\n",
       "      <td>(-86.2258505, 44.6344082)</td>\n",
       "      <td>35981.554245</td>\n",
       "      <td>121616.932836</td>\n",
       "      <td>157598.487081</td>\n",
       "      <td>0.228312</td>\n",
       "      <td>3.379980</td>\n",
       "      <td>[310, 310, 316, 336, 360, 388, 384, 382, 381, ...</td>\n",
       "      <td>388</td>\n",
       "      <td>284</td>\n",
       "      <td>333.466667</td>\n",
       "      <td>(-86.22663886875, 44.633879087500006)</td>\n",
       "      <td>226.676255</td>\n",
       "      <td>SW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Ironwood HS</td>\n",
       "      <td>[(-90.1514578, 46.457154), (-90.1505766, 46.45...</td>\n",
       "      <td>[(-90.1514578, 46.457154), (-90.15234150000002...</td>\n",
       "      <td>High School</td>\n",
       "      <td>(-90.1514578, 46.457154)</td>\n",
       "      <td>48494.539685</td>\n",
       "      <td>125485.905623</td>\n",
       "      <td>173980.445308</td>\n",
       "      <td>0.278736</td>\n",
       "      <td>2.587630</td>\n",
       "      <td>[317, 327, 338, 327, 331, 336, 346, 361, 378, ...</td>\n",
       "      <td>380</td>\n",
       "      <td>308</td>\n",
       "      <td>344.120000</td>\n",
       "      <td>(-90.15137326538463, 46.456347834615386)</td>\n",
       "      <td>175.868194</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Morenci HS</td>\n",
       "      <td>[(-84.2069811, 41.7223814), (-84.2058371, 41.7...</td>\n",
       "      <td>[(-84.2069811, 41.7223814), (-84.2071112, 41.7...</td>\n",
       "      <td>High School</td>\n",
       "      <td>(-84.2069811, 41.7223814)</td>\n",
       "      <td>58086.513759</td>\n",
       "      <td>151942.087288</td>\n",
       "      <td>210028.601047</td>\n",
       "      <td>0.276565</td>\n",
       "      <td>2.615789</td>\n",
       "      <td>[317, 318, 323, 338, 352, 359, 365, 369, 371, ...</td>\n",
       "      <td>373</td>\n",
       "      <td>313</td>\n",
       "      <td>347.217391</td>\n",
       "      <td>(-84.206239, 41.72175010833334)</td>\n",
       "      <td>138.736309</td>\n",
       "      <td>SE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    park_name  \\\n",
       "49   SW Christian HS Fort Worth - high school   \n",
       "60                            Soo High School   \n",
       "61                  Frankfort-Elberta Area HS   \n",
       "62                                Ironwood HS   \n",
       "101                                Morenci HS   \n",
       "\n",
       "                                                  foul  \\\n",
       "49   [(-97.4421601, 32.6418776), (-97.4431985, 32.6...   \n",
       "60   [(-84.36583, 46.4813183), (-84.3658206, 46.482...   \n",
       "61   [(-86.2258505, 44.6344082), (-86.2258357, 44.6...   \n",
       "62   [(-90.1514578, 46.457154), (-90.1505766, 46.45...   \n",
       "101  [(-84.2069811, 41.7223814), (-84.2058371, 41.7...   \n",
       "\n",
       "                                                   fop        level  \\\n",
       "49   [(-97.4421601, 32.6418776), (-97.4421306000000...  High School   \n",
       "60   [(-84.36583, 46.4813183), (-84.3645398, 46.481...  High School   \n",
       "61   [(-86.2258505, 44.6344082), (-86.2270454, 44.6...  High School   \n",
       "62   [(-90.1514578, 46.457154), (-90.15234150000002...  High School   \n",
       "101  [(-84.2069811, 41.7223814), (-84.2071112, 41.7...  High School   \n",
       "\n",
       "                    home_plate  foul_area_sqft  fop_area_sqft  \\\n",
       "49   (-97.4421601, 32.6418776)    37106.931904  169215.122852   \n",
       "60     (-84.36583, 46.4813183)    48045.311892  132184.170959   \n",
       "61   (-86.2258505, 44.6344082)    35981.554245  121616.932836   \n",
       "62    (-90.1514578, 46.457154)    48494.539685  125485.905623   \n",
       "101  (-84.2069811, 41.7223814)    58086.513759  151942.087288   \n",
       "\n",
       "     field_area_sqft  foul_area_per  fair_to_foul  \\\n",
       "49     206322.054756       0.179850      4.560202   \n",
       "60     180229.482852       0.266579      2.751240   \n",
       "61     157598.487081       0.228312      3.379980   \n",
       "62     173980.445308       0.278736      2.587630   \n",
       "101    210028.601047       0.276565      2.615789   \n",
       "\n",
       "                                             distances  max_distance  \\\n",
       "49   [314, 337, 371, 342, 354, 370, 362, 407, 413, ...           413   \n",
       "60                 [324, 353, 357, 370, 373, 359, 326]           373   \n",
       "61   [310, 310, 316, 336, 360, 388, 384, 382, 381, ...           388   \n",
       "62   [317, 327, 338, 327, 331, 336, 346, 361, 378, ...           380   \n",
       "101  [317, 318, 323, 338, 352, 359, 365, 369, 371, ...           373   \n",
       "\n",
       "     min_distance  avg_distance                              fop_centroid  \\\n",
       "49            314    357.272727        (-97.442824625, 32.64242071666667)   \n",
       "60            324    351.714286           (-84.3650061875, 46.4818190875)   \n",
       "61            284    333.466667     (-86.22663886875, 44.633879087500006)   \n",
       "62            308    344.120000  (-90.15137326538463, 46.456347834615386)   \n",
       "101           313    347.217391           (-84.206239, 41.72175010833334)   \n",
       "\n",
       "     field_orientation field_cardinal_direction  \n",
       "49          314.145471                       NW  \n",
       "60           48.561414                       NE  \n",
       "61          226.676255                       SW  \n",
       "62          175.868194                        S  \n",
       "101         138.736309                       SE  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_13900\\3874162956.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hs_df[col] = hs_df['park_name'].apply(lambda x: mhsaa_df.loc[mhsaa_df['best_match'] == x, col].iloc[0] if not mhsaa_df.loc[mhsaa_df['best_match'] == x, col].empty else None)\n",
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_13900\\3874162956.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hs_df[col] = hs_df['park_name'].apply(lambda x: mhsaa_df.loc[mhsaa_df['best_match'] == x, col].iloc[0] if not mhsaa_df.loc[mhsaa_df['best_match'] == x, col].empty else None)\n",
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_13900\\3874162956.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hs_df[col] = hs_df['park_name'].apply(lambda x: mhsaa_df.loc[mhsaa_df['best_match'] == x, col].iloc[0] if not mhsaa_df.loc[mhsaa_df['best_match'] == x, col].empty else None)\n",
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_13900\\3874162956.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hs_df[col] = hs_df['park_name'].apply(lambda x: mhsaa_df.loc[mhsaa_df['best_match'] == x, col].iloc[0] if not mhsaa_df.loc[mhsaa_df['best_match'] == x, col].empty else None)\n",
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_13900\\3874162956.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hs_df[col] = hs_df['park_name'].apply(lambda x: name_color_df.loc[name_color_df['best_match'] == x, col].iloc[0] if not name_color_df.loc[name_color_df['best_match'] == x, col].empty else None)\n",
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_13900\\3874162956.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hs_df[col] = hs_df['park_name'].apply(lambda x: name_color_df.loc[name_color_df['best_match'] == x, col].iloc[0] if not name_color_df.loc[name_color_df['best_match'] == x, col].empty else None)\n",
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_13900\\3874162956.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hs_df[col] = hs_df['park_name'].apply(lambda x: name_color_df.loc[name_color_df['best_match'] == x, col].iloc[0] if not name_color_df.loc[name_color_df['best_match'] == x, col].empty else None)\n",
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_13900\\3874162956.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hs_df[col] = hs_df['park_name'].apply(lambda x: name_color_df.loc[name_color_df['best_match'] == x, col].iloc[0] if not name_color_df.loc[name_color_df['best_match'] == x, col].empty else None)\n",
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_13900\\3874162956.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hs_df[col] = hs_df['park_name'].apply(lambda x: name_color_df.loc[name_color_df['best_match'] == x, col].iloc[0] if not name_color_df.loc[name_color_df['best_match'] == x, col].empty else None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park_name</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>level</th>\n",
       "      <th>home_plate</th>\n",
       "      <th>foul_area_sqft</th>\n",
       "      <th>fop_area_sqft</th>\n",
       "      <th>field_area_sqft</th>\n",
       "      <th>foul_area_per</th>\n",
       "      <th>fair_to_foul</th>\n",
       "      <th>...</th>\n",
       "      <th>field_cardinal_direction</th>\n",
       "      <th>school_id</th>\n",
       "      <th>school_name</th>\n",
       "      <th>students</th>\n",
       "      <th>division</th>\n",
       "      <th>Nickname</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>Color4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>SW Christian HS Fort Worth - high school</td>\n",
       "      <td>[(-97.4421601, 32.6418776), (-97.4431985, 32.6...</td>\n",
       "      <td>[(-97.4421601, 32.6418776), (-97.4421306000000...</td>\n",
       "      <td>High School</td>\n",
       "      <td>(-97.4421601, 32.6418776)</td>\n",
       "      <td>37106.931904</td>\n",
       "      <td>169215.122852</td>\n",
       "      <td>206322.054756</td>\n",
       "      <td>0.179850</td>\n",
       "      <td>4.560202</td>\n",
       "      <td>...</td>\n",
       "      <td>NW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Soo High School</td>\n",
       "      <td>[(-84.36583, 46.4813183), (-84.3658206, 46.482...</td>\n",
       "      <td>[(-84.36583, 46.4813183), (-84.3645398, 46.481...</td>\n",
       "      <td>High School</td>\n",
       "      <td>(-84.36583, 46.4813183)</td>\n",
       "      <td>48045.311892</td>\n",
       "      <td>132184.170959</td>\n",
       "      <td>180229.482852</td>\n",
       "      <td>0.266579</td>\n",
       "      <td>2.751240</td>\n",
       "      <td>...</td>\n",
       "      <td>NE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Frankfort-Elberta Area HS</td>\n",
       "      <td>[(-86.2258505, 44.6344082), (-86.2258357, 44.6...</td>\n",
       "      <td>[(-86.2258505, 44.6344082), (-86.2270454, 44.6...</td>\n",
       "      <td>High School</td>\n",
       "      <td>(-86.2258505, 44.6344082)</td>\n",
       "      <td>35981.554245</td>\n",
       "      <td>121616.932836</td>\n",
       "      <td>157598.487081</td>\n",
       "      <td>0.228312</td>\n",
       "      <td>3.379980</td>\n",
       "      <td>...</td>\n",
       "      <td>SW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Ironwood HS</td>\n",
       "      <td>[(-90.1514578, 46.457154), (-90.1505766, 46.45...</td>\n",
       "      <td>[(-90.1514578, 46.457154), (-90.15234150000002...</td>\n",
       "      <td>High School</td>\n",
       "      <td>(-90.1514578, 46.457154)</td>\n",
       "      <td>48494.539685</td>\n",
       "      <td>125485.905623</td>\n",
       "      <td>173980.445308</td>\n",
       "      <td>0.278736</td>\n",
       "      <td>2.587630</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Red Devils</td>\n",
       "      <td>Red</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Morenci HS</td>\n",
       "      <td>[(-84.2069811, 41.7223814), (-84.2058371, 41.7...</td>\n",
       "      <td>[(-84.2069811, 41.7223814), (-84.2071112, 41.7...</td>\n",
       "      <td>High School</td>\n",
       "      <td>(-84.2069811, 41.7223814)</td>\n",
       "      <td>58086.513759</td>\n",
       "      <td>151942.087288</td>\n",
       "      <td>210028.601047</td>\n",
       "      <td>0.276565</td>\n",
       "      <td>2.615789</td>\n",
       "      <td>...</td>\n",
       "      <td>SE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Bulldogs</td>\n",
       "      <td>Maroon</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    park_name  \\\n",
       "49   SW Christian HS Fort Worth - high school   \n",
       "60                            Soo High School   \n",
       "61                  Frankfort-Elberta Area HS   \n",
       "62                                Ironwood HS   \n",
       "101                                Morenci HS   \n",
       "\n",
       "                                                  foul  \\\n",
       "49   [(-97.4421601, 32.6418776), (-97.4431985, 32.6...   \n",
       "60   [(-84.36583, 46.4813183), (-84.3658206, 46.482...   \n",
       "61   [(-86.2258505, 44.6344082), (-86.2258357, 44.6...   \n",
       "62   [(-90.1514578, 46.457154), (-90.1505766, 46.45...   \n",
       "101  [(-84.2069811, 41.7223814), (-84.2058371, 41.7...   \n",
       "\n",
       "                                                   fop        level  \\\n",
       "49   [(-97.4421601, 32.6418776), (-97.4421306000000...  High School   \n",
       "60   [(-84.36583, 46.4813183), (-84.3645398, 46.481...  High School   \n",
       "61   [(-86.2258505, 44.6344082), (-86.2270454, 44.6...  High School   \n",
       "62   [(-90.1514578, 46.457154), (-90.15234150000002...  High School   \n",
       "101  [(-84.2069811, 41.7223814), (-84.2071112, 41.7...  High School   \n",
       "\n",
       "                    home_plate  foul_area_sqft  fop_area_sqft  \\\n",
       "49   (-97.4421601, 32.6418776)    37106.931904  169215.122852   \n",
       "60     (-84.36583, 46.4813183)    48045.311892  132184.170959   \n",
       "61   (-86.2258505, 44.6344082)    35981.554245  121616.932836   \n",
       "62    (-90.1514578, 46.457154)    48494.539685  125485.905623   \n",
       "101  (-84.2069811, 41.7223814)    58086.513759  151942.087288   \n",
       "\n",
       "     field_area_sqft  foul_area_per  fair_to_foul  ...  \\\n",
       "49     206322.054756       0.179850      4.560202  ...   \n",
       "60     180229.482852       0.266579      2.751240  ...   \n",
       "61     157598.487081       0.228312      3.379980  ...   \n",
       "62     173980.445308       0.278736      2.587630  ...   \n",
       "101    210028.601047       0.276565      2.615789  ...   \n",
       "\n",
       "    field_cardinal_direction  school_id  school_name  students division  \\\n",
       "49                        NW        NaN         None       NaN     None   \n",
       "60                        NE        NaN         None       NaN     None   \n",
       "61                        SW        NaN         None       NaN     None   \n",
       "62                         S        NaN         None       NaN     None   \n",
       "101                       SE        NaN         None       NaN     None   \n",
       "\n",
       "       Nickname   Color1  Color2 Color3  Color4  \n",
       "49         None     None    None   None     NaN  \n",
       "60         None     None    None   None     NaN  \n",
       "61         None     None    None   None     NaN  \n",
       "62   Red Devils     Red    White    NaN     NaN  \n",
       "101    Bulldogs  Maroon    White    NaN     NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "def find_best_match(school_name, choices, score_cutoff=80):\n",
    "    best_match = process.extractOne(school_name, choices, scorer=fuzz.token_sort_ratio, score_cutoff=score_cutoff)\n",
    "    if best_match:\n",
    "        return best_match[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Read CSV files\n",
    "mhsaa_df = pd.read_csv('data/school_info/mhsaa_enrolment_2022.csv')\n",
    "name_color_df = pd.read_csv('data\\school_info\\mhsaa_school_nickname_color_2020.csv')\n",
    "\n",
    "# Get the list of park names from hs_df\n",
    "park_names = hs_df['park_name'].tolist()\n",
    "\n",
    "# Apply find_best_match function to create a new column 'best_match' in mhsaa_df\n",
    "mhsaa_df['best_match'] = mhsaa_df['school_name'].apply(find_best_match, choices=park_names, score_cutoff=90)\n",
    "\n",
    "## Pull the school_id, school_name, students, and division columns from mhsaa_df and add to hs_df\n",
    "columns_to_extract = ['school_id', 'school_name', 'students', 'division']\n",
    "for col in columns_to_extract:\n",
    "    hs_df[col] = hs_df['park_name'].apply(lambda x: mhsaa_df.loc[mhsaa_df['best_match'] == x, col].iloc[0] if not mhsaa_df.loc[mhsaa_df['best_match'] == x, col].empty else None)\n",
    "\n",
    "# Apply find_best_match function to create a new column 'best_match' in name_color_df\n",
    "name_color_df['best_match'] = name_color_df['School'].apply(find_best_match, choices=park_names, score_cutoff=80)\n",
    "\n",
    "## Pull the data from the name_color_df and add to hs_df (Nickname,Color1,Color2,Color3,Color4)\n",
    "columns_to_extract = ['Nickname', 'Color1', 'Color2', 'Color3', 'Color4']\n",
    "for col in columns_to_extract:\n",
    "    hs_df[col] = hs_df['park_name'].apply(lambda x: name_color_df.loc[name_color_df['best_match'] == x, col].iloc[0] if not name_color_df.loc[name_color_df['best_match'] == x, col].empty else None)\n",
    "\n",
    "# Drop the 'best_match' columns\n",
    "# hs_df.drop(columns=['best_match'], inplace=True)\n",
    "\n",
    "## Take a look at the new hs_df\n",
    "hs_df.head()\n",
    "\n",
    "\n",
    "\n",
    "# # Lookup the mhsaa_df['best_match'] in hs_df and return the columns: 'school_id', 'school_name', 'students', 'division'\n",
    "# columns_to_extract = ['school_id', 'school_name', 'students', 'division']\n",
    "# for col in columns_to_extract:\n",
    "#     mhsaa_df[col] = mhsaa_df['best_match'].apply(lambda x: hs_df.loc[hs_df['park_name'] == x, col].iloc[0] if not hs_df.loc[hs_df['park_name'] == x, col].empty else None)\n",
    "\n",
    "# # Apply find_best_match function to create a new column 'best_match' in name_color_df\n",
    "# name_color_df['best_match'] = name_color_df['School'].apply(find_best_match, choices=park_names, score_cutoff=80)\n",
    "\n",
    "# # Merge hs_df with mhsaa_df and name_color_df on the 'park_name' and 'best_match' columns\n",
    "# hs_df = hs_df.merge(mhsaa_df, left_on='park_name', right_on='best_match', how='left', suffixes=('', '_from_mhsaa'))\n",
    "# hs_df = hs_df.merge(name_color_df, left_on='park_name', right_on='best_match', how='left', suffixes=('', '_from_name_color'))\n",
    "\n",
    "# # Drop the 'best_match' columns\n",
    "# hs_df.drop(columns=['best_match_x', 'best_match_y'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 494 entries, 49 to 722\n",
      "Data columns (total 26 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   park_name                 494 non-null    object \n",
      " 1   foul                      494 non-null    object \n",
      " 2   fop                       494 non-null    object \n",
      " 3   level                     494 non-null    object \n",
      " 4   home_plate                494 non-null    object \n",
      " 5   foul_area_sqft            494 non-null    float64\n",
      " 6   fop_area_sqft             494 non-null    float64\n",
      " 7   field_area_sqft           494 non-null    float64\n",
      " 8   foul_area_per             494 non-null    float64\n",
      " 9   fair_to_foul              494 non-null    float64\n",
      " 10  distances                 494 non-null    object \n",
      " 11  max_distance              494 non-null    int64  \n",
      " 12  min_distance              494 non-null    int64  \n",
      " 13  avg_distance              494 non-null    float64\n",
      " 14  fop_centroid              494 non-null    object \n",
      " 15  field_orientation         494 non-null    float64\n",
      " 16  field_cardinal_direction  494 non-null    object \n",
      " 17  school_id                 311 non-null    float64\n",
      " 18  school_name               311 non-null    object \n",
      " 19  students                  311 non-null    float64\n",
      " 20  division                  311 non-null    object \n",
      " 21  Nickname                  326 non-null    object \n",
      " 22  Color1                    326 non-null    object \n",
      " 23  Color2                    326 non-null    object \n",
      " 24  Color3                    17 non-null     object \n",
      " 25  Color4                    0 non-null      float64\n",
      "dtypes: float64(10), int64(2), object(14)\n",
      "memory usage: 104.2+ KB\n"
     ]
    }
   ],
   "source": [
    "### examin the output\n",
    "\n",
    "hs_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ORIGINAL MATCHING CODE ####\n",
    "\n",
    "# ########## TURNED OFF FOR NOW ###########\n",
    "# #****** Don't need school info. want to test new fields\n",
    "\n",
    "# #### MATCHING HIGH SCHOOL NAMES TO THE MHSAA DATA #####\n",
    "# import pandas as pd\n",
    "# from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# # Read the enrollment table from MHSAA website - 2022 enrollment\n",
    "# mhsaa_df = pd.read_csv('data/school_info/mhsaa_enrolment_2022.csv')\n",
    "\n",
    "# # Select just the high school level\n",
    "# hs_df = df[df['level'] == 'high_school']\n",
    "# other_df = df[df['level'] != 'high_school']\n",
    "\n",
    "# # Set the threshold for the fuzzy match\n",
    "# threshold = 90\n",
    "\n",
    "# # Define a function to get the best fuzzy match with the threshold\n",
    "# def get_best_match(field_name, school_names, threshold):\n",
    "#     best_match = process.extractOne(field_name, school_names, scorer=fuzz.token_set_ratio)\n",
    "#     if best_match[1] >= threshold:\n",
    "#         return best_match[0]\n",
    "#     return None\n",
    "\n",
    "# # Apply the function to the 'field' column and store the result in a new 'match' column\n",
    "# hs_df['match'] = hs_df['park_name'].apply(lambda x: get_best_match(x, mhsaa_df['school_name'], threshold))\n",
    "\n",
    "# # #### This destroys a bunch of the data, at least every high school outside of michigan\n",
    "# # # Drop rows with no match\n",
    "# # # hs_df = hs_df.dropna(subset=['match'])\n",
    "\n",
    "# # Lookup the hs_df['match'] in the mhsaa_df and return the columns: 'school_id', 'school_name', 'students', 'division'\n",
    "# columns_to_extract = ['school_id', 'school_name', 'students', 'division']\n",
    "# for col in columns_to_extract:\n",
    "#     hs_df[col] = hs_df['match'].apply(lambda x: mhsaa_df.loc[mhsaa_df['school_name'] == x, col].iloc[0] if not mhsaa_df.loc[mhsaa_df['school_name'] == x, col].empty else None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge the two dataframes back to a single one (using append)\n",
    "\n",
    "# Merge the hs_df and other_df back together\n",
    "merged_df = pd.concat([hs_df, other_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing coordinates:  15%|â–ˆâ–        | 106/716 [00:52<05:02,  2.01it/s]"
     ]
    }
   ],
   "source": [
    "### Get Geolocation of each field based on home plate coordinates and return state and country\n",
    "### This block takes a long time to run - will need to revisit\n",
    "## up to ten minutes\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "from tqdm import tqdm\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"baseball_field_locator\")\n",
    "\n",
    "# Function to get location information\n",
    "def get_location_info(lng, lat):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lng), timeout=10)\n",
    "        state = location.raw['address'].get('state', None)\n",
    "        country = location.raw['address'].get('country', None)\n",
    "        return state, country\n",
    "    except GeocoderTimedOut:\n",
    "        print(f\"GeocoderTimedOut error for coordinates: ({lng}, {lat})\")\n",
    "        return None, None\n",
    "    except GeocoderServiceError:\n",
    "        print(f\"GeocoderServiceError for coordinates: ({lng}, {lat})\")\n",
    "        return None, None\n",
    "\n",
    "# Extract the first coordinate for each field\n",
    "df_cleaned['lng'], df_cleaned['lat'] = zip(*df_cleaned['home_plate'].apply(lambda x: x))\n",
    "\n",
    "# Wrap the DataFrame apply function with tqdm for progress indication\n",
    "tqdm.pandas(desc=\"Processing coordinates\")\n",
    "\n",
    "# Get state and country information for each field\n",
    "df_cleaned[['state', 'country']] = df_cleaned.progress_apply(lambda row: get_location_info(row['lng'], row['lat']), axis=1, result_type='expand')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = df\n",
    "merged_df.head(10)\n",
    "\n",
    "# # Save the merged_df DataFrame as a JSON file\n",
    "merged_df.to_json('data/default_updated_output.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = merged_df.columns\n",
    "print(column_list)\n",
    "\n",
    "# ## Print a list of ten high school field names to test mascot lookup\n",
    "# print(merged_df[merged_df['level'] == 'high_school']['park_name'].head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Code to create report on the json file structure to be used as a reference later\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# from collections import defaultdict\n",
    "# from builtins import list, dict\n",
    "\n",
    "# # Load the JSON data from the file\n",
    "# with open('data/updated_output_data.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# # Analyze the JSON data\n",
    "# def analyze_structure(data, prefix=''):\n",
    "#     structure = defaultdict(set)\n",
    "    \n",
    "#     if isinstance(data, dict):\n",
    "#         for key, value in data.items():\n",
    "#             new_prefix = f'{prefix}.{key}' if prefix else key\n",
    "#             structure[new_prefix].add(type(value))\n",
    "#             structure.update(analyze_structure(value, new_prefix))\n",
    "#     elif isinstance(data, list):\n",
    "#         for item in data:\n",
    "#             structure.update(analyze_structure(item, prefix))\n",
    "    \n",
    "#     return structure\n",
    "\n",
    "# # Generate the report\n",
    "# structure = analyze_structure(data)\n",
    "# descriptions = {\n",
    "#     'field_name': 'The name of the baseball field.',\n",
    "#     'foul': 'A list of coordinates representing the foul territory of the field. (lat, lon)',\n",
    "#     'fop': 'A list of coordinates representing the fair territory of the field. (lat, lon)',\n",
    "#     'level': 'The level of the field, e.g., high_school, college, etc.',\n",
    "#     'home_plate': 'A list of coordinates representing the home plate location on the field. (lat, lon)',\n",
    "#     'foul_area_sqft': 'The total area of the foul territory in square feet.',\n",
    "#     'fop_area_sqft': 'The total area of the fair territory in square feet.',\n",
    "#     'distances': 'A list of distances from home plate to the outfield fence at the vertices of the wall.',\n",
    "#     'max_distance': 'The maximum distance from home plate to the outfield fence.',\n",
    "#     'min_distance': 'The minimum distance from home plate to the outfield fence.',\n",
    "#     'avg_distance': 'The average distance from home plate to the outfield fence.',\n",
    "#     'fop_centroid': 'A list of coordinates representing the centroid of the fair territory.',\n",
    "#     'field_orientation': \"The angle (in degrees) of the field's orientation, with 0 degrees being North.\",\n",
    "#     'field_cardinal_direction': \"The cardinal direction abbreviation (N, S, E, W, NE, NW, SE, SW) representing the field's orientation.\",\n",
    "#     'match': 'The matched school name found using fuzzy matching.',\n",
    "#     'school_id': 'The unique identifier of the matched school.',\n",
    "#     'school_name': 'The name of the matched school.',\n",
    "#     'students': 'The number of students enrolled in the matched school.',\n",
    "#     'division': 'The athletic division the matched school belongs to.'\n",
    "# }\n",
    "\n",
    "# # Replace <filename> with your desired filename without the extension\n",
    "# filename = \"output_data\"\n",
    "\n",
    "# def get_sample_value(data, key):\n",
    "#     for item in data:\n",
    "#         if key in item and item[key] is not None:\n",
    "#             return item[key]\n",
    "#     return None\n",
    "\n",
    "# # Generate the report with sample values\n",
    "# report = pd.DataFrame([(key, ', '.join([t.__name__ for t in types]), descriptions.get(key, ''), get_sample_value(data, key)) \n",
    "#                        for key, types in structure.items()],\n",
    "#                       columns=['Key', 'Data Types', 'Description', 'Sample Value'])\n",
    "\n",
    "# # Save the report as a CSV file\n",
    "# report.to_csv(f\"{filename}_report.csv\", index=False)\n",
    "\n",
    "# print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code to create report on some interesting stats like total fields, total area, ect\n",
    "\n",
    "## Create a function to measure the total distance of the outside of each polygon\n",
    "from shapely.geometry import Polygon\n",
    "import pyproj\n",
    "\n",
    "def calculate_perimeter(coords):\n",
    "\n",
    "    perimeter = 0\n",
    "\n",
    "    for i in range(len(coords)):\n",
    "        x1, y1 = coords[i]\n",
    "        x2, y2 = coords[(i + 1) % len(coords)]\n",
    "        perimeter += math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "    return perimeter\n",
    "\n",
    "# Get a sum of all of the perimeters\n",
    "total_perimeter_fop = df['fop'].apply(calculate_perimeter).sum()\n",
    "total_perimeter_foul = df['foul'].apply(calculate_perimeter).sum()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/default_updated_output.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to process the data, counting the orientations and filtering by level.\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_data(data, level_filter=None):\n",
    "    count_by_orientation = defaultdict(int)\n",
    "    \n",
    "    for record in data:\n",
    "        if level_filter is None or record['level'] == level_filter:\n",
    "            orientation = round(record['field_orientation'])\n",
    "            count_by_orientation[orientation] += 1\n",
    "\n",
    "    return count_by_orientation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polar_chart(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 2 * np.pi / num_bins\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "\n",
    "    ax.set_facecolor('#808080')\n",
    "    ###\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "    \n",
    "    # # Set dark background\n",
    "    # ax.set_facecolor('#2b2b2b')\n",
    "    plt.gca().set_rlabel_position(22.5)\n",
    "    ax.set_ylim(-10, 100)  # Adjust based on max count\n",
    "\n",
    "    bars = ax.bar(bin_edges[:-1], bin_counts, width=bin_width, bottom=-20)\n",
    "    \n",
    "\n",
    "\n",
    "    # Use custom colors and opacity\n",
    "    for r, bar in zip(bin_counts, bars):\n",
    "        bar.set_facecolor(plt.cm.viridis(r / max(bin_counts)))\n",
    "        # bar.set_facecolor(plt.cm.plasma(r / max(bin_counts)))\n",
    "        bar.set_alpha(0.8)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UPDATED GPT CODE - LATE NIGHT FRIDAY\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_polar_chart(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 2 * np.pi / num_bins\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "\n",
    "    ax.set_facecolor('#808080')\n",
    "    ###\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "    \n",
    "    # # Set dark background\n",
    "    ax.set_facecolor('#2b2b2b')\n",
    "    plt.gca().set_rlabel_position(22.5)\n",
    "    ax.set_ylim(-20, 130)  # Adjust based on max count\n",
    "\n",
    "    # Add bars for negative values\n",
    "    zero_height_bars = ax.bar(bin_edges[:-1], np.abs(ax.get_ylim()[0]), width=bin_width, bottom=0.0, color='k', alpha=0.3)\n",
    "\n",
    "    bars = ax.bar(bin_edges[:-1], bin_counts, width=bin_width, bottom=0)\n",
    "    \n",
    "    # Use custom colors and opacity\n",
    "    for r, bar in zip(bin_counts, bars):\n",
    "        bar.set_facecolor(plt.cm.viridis(r / max(bin_counts)))\n",
    "        # bar.set_facecolor(plt.cm.plasma(r / max(bin_counts)))\n",
    "        bar.set_alpha(0.8)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_polar_chart(data, num_bins=50, level_filter=None)\n",
    "\n",
    "\n",
    "#### GOAL\n",
    "## fill the center portion of the plot to create a heat map of the field orientations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_polar_chart(data, num_bins=180, level_filter=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HEATMAP CODE\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_heatmap(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "\n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "\n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "\n",
    "    # Reshape histogram data into a 2D array\n",
    "    heatmap_data = np.tile(bin_counts, (num_bins, 1))\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Create heatmap\n",
    "    plt.imshow(heatmap_data, cmap='viridis', aspect='auto', interpolation='nearest', origin='lower')\n",
    "    plt.colorbar(label='Counts')\n",
    "\n",
    "    # Set x-axis ticks and labels\n",
    "    plt.xticks(np.arange(0, num_bins, num_bins // 6), np.arange(0, 361, 60))\n",
    "    plt.xlabel('Orientation (degrees)')\n",
    "\n",
    "    # Set y-axis ticks and labels (assuming equal radial divisions)\n",
    "    max_radius_label = 'Max Radius'\n",
    "    plt.yticks(np.arange(0, num_bins, num_bins // 6), [0, 1, 2, 3, 4, max_radius_label])\n",
    "    plt.ylabel('Radial Division')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# create_heatmap(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "create_heatmap(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### EXAMPLE# CODE FROM MATPLOTLIB GALLERY\n",
    "# =======================\n",
    "# Pie chart on polar axis\n",
    "# =======================\n",
    "\n",
    "# Demo of bar plot on a polar axis.\n",
    "# \"\"\"\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # Compute pie slices\n",
    "# N = 20\n",
    "# theta = np.linspace(0.0, 2 * np.pi, N, endpoint=False)\n",
    "# radii = 10 * np.random.rand(N)\n",
    "# width = np.pi / 4 * np.random.rand(N)\n",
    "\n",
    "# ax = plt.subplot(111, projection='polar')\n",
    "# bars = ax.bar(theta, radii, width=width, bottom=0.0)\n",
    "\n",
    "# # Use custom colors and opacity\n",
    "# for r, bar in zip(radii, bars):\n",
    "#     bar.set_facecolor(plt.cm.viridis(r / 10.))\n",
    "#     bar.set_alpha(0.5)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot a histogram of the field orientations for all levels\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_histogram(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 360.0, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 360 / num_bins\n",
    "\n",
    "    # Plot the histogram\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(bin_edges[:-1], bin_counts, width=bin_width, edgecolor='black')\n",
    "    ax.set_xlabel('Field Orientation (degrees)')\n",
    "    ax.set_ylabel('Number of Fields')\n",
    "    ax.set_title('Field Orientation Histogram')\n",
    "\n",
    "    # Set the major tick locations\n",
    "    major_tick_locations = [45, 135, 225, 315]\n",
    "    plt.xticks(major_tick_locations, major_tick_locations)\n",
    "\n",
    "\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_histogram(data, num_bins=36, level_filter=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END WORK BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['field_cardinal_direction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Rename the dataframe back to the default name\n",
    "df = merged_df\n",
    "\n",
    "# Set the plot size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.scatter(df_cleaned['min_distance'], df_cleaned['max_distance'], alpha=0.8)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Min Distance (feet)')\n",
    "plt.ylabel('Max Distance (feet)')\n",
    "plt.title('Scatter Plot of Min and Max Distances to Outfield Fence')\n",
    "\n",
    "# Display the plot in the Jupyter Notebook\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CODE TO MAKE A LIST OF OUTLIERS #####\n",
    "\n",
    "# Filter the DataFrame for fields with min distances below 100 feet\n",
    "outliers = df[df['min_distance'] < 100]\n",
    "\n",
    "# Display the outlier fields in the Jupyter Notebook\n",
    "print(outliers[['park_name', 'min_distance', 'max_distance']])\n",
    "\n",
    "# Save the outlier fields to a CSV file\n",
    "outliers.to_csv('outlier_fields.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
