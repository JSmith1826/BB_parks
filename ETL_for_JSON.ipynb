{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dependencies\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is for extracting the data from the original kml file and outputing a JSON that will be easy to use with Google Maps API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD BLOCK###\n",
    "#### Load data from kml file exported from Google Earth\n",
    "\n",
    "file_path = ('data/kml/ballparks.kml') # file path to kml file\n",
    "\n",
    "with open(file_path) as file:\n",
    "\n",
    "    xml_data = file.read()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize soup variables for parsing file\n",
    "soup = BeautifulSoup(xml_data, 'xml')\n",
    "folders = soup.Document.Folder\n",
    "list = soup.Document.Folder.find_all('Folder')\n",
    "\n",
    "## Create a dataframe to hold the data parsed from xml\n",
    "df = pd.DataFrame(columns=['field', 'foul', 'fop'])\n",
    "\n",
    "failed = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing folder: Penn - Meiklejohn Stadium - college. Error message: list index out of range\n",
      "Error processing folder: Progressive Field - Cleveland - MLB. Error message: list index out of range\n",
      "Failed to process 2 folders: Penn - Meiklejohn Stadium - college, Progressive Field - Cleveland - MLB\n"
     ]
    }
   ],
   "source": [
    "#### EXTRACTION BLOCK ####\n",
    "#### Extract data from kml file\n",
    "\n",
    "# Create an empty list to store the rows to append to the DataFrame\n",
    "rows = []\n",
    "\n",
    "# Loop through the folders and extract the data\n",
    "for folder in list:\n",
    "    try:\n",
    "        field_name = folder.find('name').text\n",
    "        foul = folder.find_all('coordinates')[0].text\n",
    "        fop = folder.find_all('coordinates')[1].text\n",
    "\n",
    "        row = {\n",
    "            'field': field_name,\n",
    "            'foul': foul,\n",
    "            'fop': fop\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Add name of folder to a list of failed folders\n",
    "        failed.append(folder.find('name').text)\n",
    "        print(f\"Error processing folder: {folder.find('name').text}. Error message: {str(e)}\")\n",
    "\n",
    "# Convert the list of rows to a DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Print a list of failed folders\n",
    "print(f\"Failed to process {len(failed)} folders: {', '.join(failed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original DataFrame\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Remove new line and and space characters from coordinates\n",
    "df_cleaned = df_cleaned.replace(r'\\n','', regex=True) \n",
    "df_cleaned = df_cleaned.replace(r'\\t','', regex=True) \n",
    "\n",
    "# Drop any duplicate rows\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['field'], keep='first')\n",
    "\n",
    "# Drop any rows with empty fields\n",
    "df_cleaned = df_cleaned[(df_cleaned != 0).all(1)]\n",
    "\n",
    "re_mlb = re.compile(r'mlb', re.IGNORECASE)\n",
    "re_pro = re.compile(r'pro', re.IGNORECASE)\n",
    "re_college = re.compile(r'college', re.IGNORECASE)\n",
    "re_youth = re.compile(r'youth', re.IGNORECASE)\n",
    "re_muni = re.compile(r'muni', re.IGNORECASE)\n",
    "\n",
    "df_cleaned['level'] = df_cleaned['field'].apply(lambda x: 'mlb' \n",
    "        if re_mlb.search(x) else 'pro' \n",
    "        if re_pro.search(x) else 'college' \n",
    "        if re_college.search(x) else 'youth' \n",
    "        if re_youth.search(x) else 'muni' \n",
    "        if re_muni.search(x) else 'high_school')\n",
    "\n",
    "\n",
    "# Define a dictionary that maps level indicators to levels and size factors\n",
    "level_dict = {\n",
    "    'MLB': 'mlb', \n",
    "    'pro': 'pro', \n",
    "    'college': 'college', \n",
    "    'high school': 'high_school',\n",
    "}\n",
    "\n",
    "\n",
    "# Clean up the 'field' column by removing the level indicator and any trailing '-' characters\n",
    "df_cleaned['field'] = df_cleaned['field'].str.replace(r'\\s*(%s)\\s*' % '|'.join(level_dict.keys()), '', regex=True, flags=re.IGNORECASE)\n",
    "df_cleaned['field'] = df_cleaned['field'].str.replace(r'-\\s*$', '', regex=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field                          Whitehouse Field - cape_cod\n",
      "foul     -70.0663428,41.6926333,0 -70.06593100000001,41...\n",
      "fop      -70.0663428,41.6926333,0 -70.0652082,41.692334...\n",
      "level                                          high_school\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.head()\n",
    "\n",
    "## Print the two the headers and two rows of data to a txt file for reference\n",
    "\n",
    "\n",
    "with open('data/rows.txt', 'w') as f:\n",
    "    f.write(df_cleaned.iloc[0].to_string())\n",
    "    f.write(df_cleaned.iloc[1].to_string())\n",
    "    f.write(df_cleaned.iloc[14].to_string())\n",
    "    \n",
    "\n",
    "print(df_cleaned.iloc[15].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Code to clean up polygon data and creat a new home_plate column\n",
    "\n",
    "def parse_coordinates(coord_string):\n",
    "    coords = coord_string.split()\n",
    "    parsed_coords = [tuple(map(float, coord.split(',')[:2])) for coord in coords]\n",
    "    return parsed_coords\n",
    "\n",
    "# Create a new column for the home_plate location using the first set of coordinates in the 'fop' column\n",
    "df_cleaned['home_plate'] = df_cleaned['fop'].apply(lambda x: parse_coordinates(x)[0])\n",
    "\n",
    "# Apply the parse_coordinates function to the 'foul' and 'fop' columns\n",
    "df_cleaned['foul'] = df_cleaned['foul'].apply(parse_coordinates)\n",
    "df_cleaned['fop'] = df_cleaned['fop'].apply(parse_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "import pyproj\n",
    "\n",
    "\n",
    "#### Functions to calculate the area of a polygon\n",
    "def utm_zone_number(lat, lon):\n",
    "    return int((lon + 180) / 6) + 1\n",
    "\n",
    "def utm_epsg_code(lat, lon):\n",
    "    zone_number = utm_zone_number(lat, lon)\n",
    "    if lat >= 0:\n",
    "        return 32600 + zone_number\n",
    "    else:\n",
    "        return 32700 + zone_number\n",
    "\n",
    "def calculate_area(coords):\n",
    "    # Create a Polygon object from the coordinates\n",
    "    polygon = Polygon(coords)\n",
    "\n",
    "    # Get the appropriate UTM EPSG code\n",
    "    epsg_code = utm_epsg_code(*coords[0])\n",
    "\n",
    "    # Create a transformer for converting coordinates to UTM\n",
    "    transformer = pyproj.Transformer.from_crs(\n",
    "        pyproj.CRS(\"EPSG:4326\"),  # WGS 84 (latitude and longitude)\n",
    "        pyproj.CRS(f\"EPSG:{epsg_code}\"),  # UTM zone\n",
    "        always_xy=True\n",
    "    )\n",
    "\n",
    "    # Convert the coordinates to UTM\n",
    "    coords_utm = [transformer.transform(*coord) for coord in coords]\n",
    "\n",
    "    # Create a Polygon object from the UTM coordinates\n",
    "    polygon_utm = Polygon(coords_utm)\n",
    "\n",
    "    # Calculate the area in square meters\n",
    "    area_sqm = polygon_utm.area\n",
    "\n",
    "    # Convert the area to square feet (1 square meter = 10.764 square feet)\n",
    "    area_sqft = area_sqm * 10.764\n",
    "\n",
    "    return area_sqft\n",
    "\n",
    "\n",
    "### Call Function and add to dataframe\n",
    "df_cleaned['foul_area_sqft'] = df_cleaned['foul'].apply(calculate_area)\n",
    "df_cleaned['fop_area_sqft'] = df_cleaned['fop'].apply(calculate_area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SIMPLE DISTANCE CALCULATION USING GEOPY ####\n",
    "\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "def calculate_distances(home_plate, outfield_coords):\n",
    "    def is_same_point(point1, point2, tolerance=1e-6):\n",
    "        return abs(point1[0] - point2[0]) < tolerance and abs(point1[1] - point2[1]) < tolerance\n",
    "\n",
    "    home_plate_lat_lon = (home_plate[1], home_plate[0])\n",
    "    distances = [\n",
    "        round(great_circle(home_plate_lat_lon, (coord[1], coord[0])).feet)\n",
    "        for coord in outfield_coords\n",
    "        if not is_same_point(home_plate, coord)\n",
    "    ]\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run Function and add to dataframe\n",
    "\n",
    "# Calculate distances for each row\n",
    "df_cleaned['distances'] = df_cleaned.apply(lambda row: calculate_distances(row['home_plate'], row['fop']), axis=1)\n",
    "\n",
    "# Calculate max, min, and average distances for each row\n",
    "df_cleaned['max_distance'] = df_cleaned['distances'].apply(max)\n",
    "df_cleaned['min_distance'] = df_cleaned['distances'].apply(min)\n",
    "df_cleaned['avg_distance'] = df_cleaned['distances'].apply(lambda distances: sum(distances) / len(distances))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Orienting the map to the home plate location ####\n",
    "\n",
    "### Find the center of the field\n",
    "def calculate_centroid(coords):\n",
    "    x_coords = [coord[0] for coord in coords]\n",
    "    y_coords = [coord[1] for coord in coords]\n",
    "    centroid_x = sum(x_coords) / len(coords)\n",
    "    centroid_y = sum(y_coords) / len(coords)\n",
    "    return (centroid_x, centroid_y)\n",
    "\n",
    "\n",
    "## Find the bearing between the home plate and the center of the field\n",
    "import math\n",
    "\n",
    "def calculate_bearing(point1, point2):\n",
    "    lat1, lon1 = math.radians(point1[1]), math.radians(point1[0])\n",
    "    lat2, lon2 = math.radians(point2[1]), math.radians(point2[0])\n",
    "\n",
    "    d_lon = lon2 - lon1\n",
    "\n",
    "    x = math.cos(lat2) * math.sin(d_lon)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(d_lon)\n",
    "\n",
    "    bearing = math.degrees(math.atan2(x, y))\n",
    "    bearing = (bearing + 360) % 360  # Normalize the bearing to the range [0, 360)\n",
    "\n",
    "    return bearing\n",
    "\n",
    "### Function to classify direction in laymans terms North, South, East, West, ect\n",
    "def degrees_to_cardinal_direction(degrees):\n",
    "    directions = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW', 'N']\n",
    "    index = round(degrees / 45)\n",
    "    return directions[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the centroid of the outfield fence coordinates for each row\n",
    "df_cleaned['fop_centroid'] = df_cleaned['fop'].apply(lambda coords: calculate_centroid(coords[1:]))\n",
    "\n",
    "# Calculate the bearing between home plate and the centroid for each row\n",
    "df_cleaned['field_orientation'] = df_cleaned.apply(lambda row: calculate_bearing(row['home_plate'], row['fop_centroid']), axis=1)\n",
    "\n",
    "# Convert the bearing to a cardinal direction\n",
    "df_cleaned['field_cardinal_direction'] = df_cleaned['field_orientation'].apply(degrees_to_cardinal_direction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rename the Cleaned Dataframe back to the default name\n",
    "df = df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### In the fop, foul and home_plate columns, the coordinates are in the format (longitude, latitude).\n",
    "# ### This is the opposite of the format that is used in Google Maps, so we need to reverse the order of the coordinates.\n",
    "# df['fop'] = df['fop'].apply(lambda coords: [(coord[1], coord[0]) for coord in coords])\n",
    "# df['foul'] = df['foul'].apply(lambda coords: [(coord[1], coord[0]) for coord in coords])\n",
    "# df['home_plate'] = df['home_plate'].apply(lambda coord: (coord[1], coord[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### TURNED OFF FOR NOW ###########\n",
    "##****** Don't need school info. want to test new fields\n",
    "\n",
    "# #### MATCHING HIGH SCHOOL NAMES TO THE MHSAA DATA #####\n",
    "# import pandas as pd\n",
    "# from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# # Read the enrollment table from MHSAA website - 2022 enrollment\n",
    "# mhsaa_df = pd.read_csv('data/school_info/mhsaa_enrolment_2022.csv')\n",
    "\n",
    "# # Select just the high school level\n",
    "# hs_df = df[df['level'] == 'high_school']\n",
    "# other_df = df[df['level'] != 'high_school']\n",
    "\n",
    "# # Set the threshold for the fuzzy match\n",
    "# threshold = 90\n",
    "\n",
    "# # Define a function to get the best fuzzy match with the threshold\n",
    "# def get_best_match(field_name, school_names, threshold):\n",
    "#     best_match = process.extractOne(field_name, school_names, scorer=fuzz.token_set_ratio)\n",
    "#     if best_match[1] >= threshold:\n",
    "#         return best_match[0]\n",
    "#     return None\n",
    "\n",
    "# # Apply the function to the 'field' column and store the result in a new 'match' column\n",
    "# hs_df['match'] = hs_df['field'].apply(lambda x: get_best_match(x, mhsaa_df['school_name'], threshold))\n",
    "\n",
    "# #### This destroys a bunch of the data, at least every high school outside of michigan\n",
    "# # Drop rows with no match\n",
    "# # hs_df = hs_df.dropna(subset=['match'])\n",
    "\n",
    "# # Lookup the hs_df['match'] in the mhsaa_df and return the columns: 'school_id', 'school_name', 'students', 'division'\n",
    "# columns_to_extract = ['school_id', 'school_name', 'students', 'division']\n",
    "# for col in columns_to_extract:\n",
    "#     hs_df[col] = hs_df['match'].apply(lambda x: mhsaa_df.loc[mhsaa_df['school_name'] == x, col].iloc[0] if not mhsaa_df.loc[mhsaa_df['school_name'] == x, col].empty else None)\n",
    "\n",
    "# # Merge the hs_df and other_df back together\n",
    "# merged_df = pd.concat([hs_df, other_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df\n",
    "merged_df.head()\n",
    "\n",
    "# Save the merged_df DataFrame as a JSON file\n",
    "merged_df.to_json('data/default_updated_output.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Code to create report on the json file structure to be used as a reference later\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# from collections import defaultdict\n",
    "# from builtins import list, dict\n",
    "\n",
    "# # Load the JSON data from the file\n",
    "# with open('data/updated_output_data.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# # Analyze the JSON data\n",
    "# def analyze_structure(data, prefix=''):\n",
    "#     structure = defaultdict(set)\n",
    "    \n",
    "#     if isinstance(data, dict):\n",
    "#         for key, value in data.items():\n",
    "#             new_prefix = f'{prefix}.{key}' if prefix else key\n",
    "#             structure[new_prefix].add(type(value))\n",
    "#             structure.update(analyze_structure(value, new_prefix))\n",
    "#     elif isinstance(data, list):\n",
    "#         for item in data:\n",
    "#             structure.update(analyze_structure(item, prefix))\n",
    "    \n",
    "#     return structure\n",
    "\n",
    "# # Generate the report\n",
    "# structure = analyze_structure(data)\n",
    "# descriptions = {\n",
    "#     'field': 'The name of the baseball field.',\n",
    "#     'foul': 'A list of coordinates representing the foul territory of the field. (lat, lon)',\n",
    "#     'fop': 'A list of coordinates representing the fair territory of the field. (lat, lon)',\n",
    "#     'level': 'The level of the field, e.g., high_school, college, etc.',\n",
    "#     'home_plate': 'A list of coordinates representing the home plate location on the field. (lat, lon)',\n",
    "#     'foul_area_sqft': 'The total area of the foul territory in square feet.',\n",
    "#     'fop_area_sqft': 'The total area of the fair territory in square feet.',\n",
    "#     'distances': 'A list of distances from home plate to the outfield fence at the vertices of the wall.',\n",
    "#     'max_distance': 'The maximum distance from home plate to the outfield fence.',\n",
    "#     'min_distance': 'The minimum distance from home plate to the outfield fence.',\n",
    "#     'avg_distance': 'The average distance from home plate to the outfield fence.',\n",
    "#     'fop_centroid': 'A list of coordinates representing the centroid of the fair territory.',\n",
    "#     'field_orientation': \"The angle (in degrees) of the field's orientation, with 0 degrees being North.\",\n",
    "#     'field_cardinal_direction': \"The cardinal direction abbreviation (N, S, E, W, NE, NW, SE, SW) representing the field's orientation.\",\n",
    "#     'match': 'The matched school name found using fuzzy matching.',\n",
    "#     'school_id': 'The unique identifier of the matched school.',\n",
    "#     'school_name': 'The name of the matched school.',\n",
    "#     'students': 'The number of students enrolled in the matched school.',\n",
    "#     'division': 'The athletic division the matched school belongs to.'\n",
    "# }\n",
    "\n",
    "# # Replace <filename> with your desired filename without the extension\n",
    "# filename = \"output_data\"\n",
    "\n",
    "# def get_sample_value(data, key):\n",
    "#     for item in data:\n",
    "#         if key in item and item[key] is not None:\n",
    "#             return item[key]\n",
    "#     return None\n",
    "\n",
    "# # Generate the report with sample values\n",
    "# report = pd.DataFrame([(key, ', '.join([t.__name__ for t in types]), descriptions.get(key, ''), get_sample_value(data, key)) \n",
    "#                        for key, types in structure.items()],\n",
    "#                       columns=['Key', 'Data Types', 'Description', 'Sample Value'])\n",
    "\n",
    "# # Save the report as a CSV file\n",
    "# report.to_csv(f\"{filename}_report.csv\", index=False)\n",
    "\n",
    "# print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Rename the dataframe back to the default name\n",
    "df = merged_df\n",
    "\n",
    "# Set the plot size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.scatter(df_cleaned['min_distance'], df_cleaned['max_distance'], alpha=0.8)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Min Distance (feet)')\n",
    "plt.ylabel('Max Distance (feet)')\n",
    "plt.title('Scatter Plot of Min and Max Distances to Outfield Fence')\n",
    "\n",
    "# Display the plot in the Jupyter Notebook\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CODE TO MAKE A LIST OF OUTLIERS #####\n",
    "\n",
    "# Filter the DataFrame for fields with min distances below 100 feet\n",
    "outliers = df[df['min_distance'] < 100]\n",
    "\n",
    "# Display the outlier fields in the Jupyter Notebook\n",
    "print(outliers[['field', 'min_distance', 'max_distance']])\n",
    "\n",
    "# Save the outlier fields to a CSV file\n",
    "outliers.to_csv('outlier_fields.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
