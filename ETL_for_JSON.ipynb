{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dependencies\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is for extracting the data from the original kml file and outputing a JSON that will be easy to use with Google Maps API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD BLOCK###\n",
    "#### Load data from kml file exported from Google Earth\n",
    "\n",
    "file_path = ('data/kml/ballparks.kml') # file path to kml file\n",
    "\n",
    "with open(file_path) as file:\n",
    "\n",
    "    xml_data = file.read()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize soup variables for parsing file\n",
    "soup = BeautifulSoup(xml_data, 'xml')\n",
    "folders = soup.Document.Folder\n",
    "list = soup.Document.Folder.find_all('Folder')\n",
    "\n",
    "## Create a dataframe to hold the data parsed from xml\n",
    "df = pd.DataFrame(columns=['field', 'foul', 'fop'])\n",
    "\n",
    "failed = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing folder: Progressive Field - Cleveland - MLB. Error message: list index out of range\n",
      "Failed to process 1 folders: Progressive Field - Cleveland - MLB\n"
     ]
    }
   ],
   "source": [
    "#### EXTRACTION BLOCK ####\n",
    "#### Extract data from kml file\n",
    "\n",
    "# Create an empty list to store the rows to append to the DataFrame\n",
    "rows = []\n",
    "\n",
    "# Loop through the folders and extract the data\n",
    "for folder in list:\n",
    "    try:\n",
    "        field_name = folder.find('name').text\n",
    "        foul = folder.find_all('coordinates')[0].text\n",
    "        fop = folder.find_all('coordinates')[1].text\n",
    "\n",
    "        row = {\n",
    "            'field': field_name,\n",
    "            'foul': foul,\n",
    "            'fop': fop\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Add name of folder to a list of failed folders\n",
    "        failed.append(folder.find('name').text)\n",
    "        print(f\"Error processing folder: {folder.find('name').text}. Error message: {str(e)}\")\n",
    "\n",
    "# Convert the list of rows to a DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Print a list of failed folders\n",
    "print(f\"Failed to process {len(failed)} folders: {', '.join(failed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary that maps level indicators to levels and size factors\n",
    "level_dict = {\n",
    "    'Internationsl': 'international',\n",
    "    'Major Leagues': 'mlb', \n",
    "    'Professional': 'pro', \n",
    "    'College': 'college', \n",
    "    'High School': 'high_school',\n",
    "    'Youth': 'youth',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original DataFrame\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Remove new line and space characters from coordinates\n",
    "df_cleaned = df_cleaned.replace(r'\\n','', regex=True) \n",
    "df_cleaned = df_cleaned.replace(r'\\t','', regex=True) \n",
    "\n",
    "# Drop any duplicate rows\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['field'], keep='first')\n",
    "\n",
    "# Drop any rows with empty fields\n",
    "df_cleaned = df_cleaned[(df_cleaned != 0).all(1)]\n",
    "\n",
    "# Define the regex patterns for each level\n",
    "re_mlb = re.compile(r'mlb', re.IGNORECASE)\n",
    "re_pro = re.compile(r'pro|semi[-\\s]*pro', re.IGNORECASE)\n",
    "re_college = re.compile(r'college', re.IGNORECASE)\n",
    "re_high_school = re.compile(r'high school|hs', re.IGNORECASE)  # Include the abbreviation 'hs'\n",
    "re_youth = re.compile(r'youth', re.IGNORECASE)\n",
    "re_muni = re.compile(r'muni', re.IGNORECASE)\n",
    "re_international = re.compile(r'international', re.IGNORECASE)\n",
    "\n",
    "# Define a function to classify the fields based on the regex patterns\n",
    "def classify_field(field_name):\n",
    "    if re_mlb.search(field_name):\n",
    "        return 'Major League'\n",
    "    elif re_pro.search(field_name):\n",
    "        return 'Professional'\n",
    "    elif re_college.search(field_name):\n",
    "        return 'College'\n",
    "    elif re_high_school.search(field_name):\n",
    "        return 'High School'\n",
    "    elif re_youth.search(field_name):\n",
    "        return 'Youth'\n",
    "    elif re_muni.search(field_name):\n",
    "        return 'State / County / Municipal'\n",
    "    elif re_international.search(field_name):\n",
    "        return 'International'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Apply the classify_field function to the 'field' column\n",
    "df_cleaned['level'] = df_cleaned['field'].apply(classify_field)\n",
    "\n",
    "# Clean up the 'field' column by removing the level indicator and any trailing '-' characters\n",
    "level_regex = r'\\s*(%s)\\s*' % '|'.join(re.escape(level) for level in level_dict.values())\n",
    "df_cleaned['field'] = df_cleaned['field'].str.replace(level_regex, '', regex=True, flags=re.IGNORECASE)\n",
    "df_cleaned['field'] = df_cleaned['field'].str.replace(r'-\\s*$', '', regex=True)\n",
    "\n",
    "# Rename field column to park_name to avoid confusion down the line\n",
    "df_cleaned = df_cleaned.rename(columns={'field': 'park_name'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "park_name                                           Boston C. \n",
      "foul         -71.1603978,42.3443347,0 -71.1591854,42.344397...\n",
      "fop          -71.1603978,42.3443347,0 -71.16031460000001,42...\n",
      "level                                                  college\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.head()\n",
    "\n",
    "## Print the two the headers and two rows of data to a txt file for reference\n",
    "\n",
    "\n",
    "with open('data/rows.txt', 'w') as f:\n",
    "    f.write(df_cleaned.iloc[0].to_string())\n",
    "    f.write(df_cleaned.iloc[1].to_string())\n",
    "    f.write(df_cleaned.iloc[14].to_string())\n",
    "    \n",
    "\n",
    "print(df_cleaned.iloc[15].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Clean up polygon data and create a new home_plate column\n",
    "\n",
    "def parse_coordinates(coord_string):\n",
    "    coords = coord_string.split()\n",
    "    parsed_coords = [tuple(map(float, coord.split(',')[:2])) for coord in coords]\n",
    "    return parsed_coords\n",
    "\n",
    "# Create a new column for the home_plate location using the first set of coordinates in the 'fop' column\n",
    "df_cleaned['home_plate'] = df_cleaned['fop'].apply(lambda x: parse_coordinates(x)[0])\n",
    "\n",
    "# Apply the parse_coordinates function to the 'foul' and 'fop' columns\n",
    "df_cleaned['foul'] = df_cleaned['foul'].apply(parse_coordinates)\n",
    "df_cleaned['fop'] = df_cleaned['fop'].apply(parse_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doesn't seem to be returning useful data - will need to revisit\n",
    "\n",
    "# def determine_direction(coordinates):\n",
    "#     num_points = len(coordinates)\n",
    "#     if num_points < 2:\n",
    "#         return \"Not enough points\"\n",
    "\n",
    "#     # Get the latitude (y-coordinate) values\n",
    "#     latitudes = [point[1] for point in coordinates]\n",
    "\n",
    "#     # Check the change in latitude values\n",
    "#     increasing_latitudes = all(latitudes[i] <= latitudes[i+1] for i in range(num_points-1))\n",
    "#     decreasing_latitudes = all(latitudes[i] >= latitudes[i+1] for i in range(num_points-1))\n",
    "\n",
    "#     if increasing_latitudes:\n",
    "#         return \"Left Field\"\n",
    "#     elif decreasing_latitudes:\n",
    "#         return \"Right Field\"\n",
    "#     else:\n",
    "#         return \"Collinear\"\n",
    "\n",
    "# # Apply the determine_direction function to the 'fop' and 'foul' columns\n",
    "# df_cleaned['fop_direction'] = df_cleaned['fop'].apply(determine_direction)\n",
    "# df_cleaned['foul_direction'] = df_cleaned['foul'].apply(determine_direction)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park_name</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>level</th>\n",
       "      <th>home_plate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Day Air Ballpark - Dayton Dragons</td>\n",
       "      <td>[(-84.18557, 39.7642091), (-84.1849444, 39.764...</td>\n",
       "      <td>[(-84.18557, 39.7642091), (-84.1846064, 39.763...</td>\n",
       "      <td>pro</td>\n",
       "      <td>(-84.18557, 39.7642091)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Parkview Field - Fort Wayne TinCaps</td>\n",
       "      <td>[(-85.1429982, 41.0741133), (-85.1432208, 41.0...</td>\n",
       "      <td>[(-85.1429982, 41.0741133), (-85.1418777, 41.0...</td>\n",
       "      <td>pro</td>\n",
       "      <td>(-85.1429982, 41.0741133)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic Park- Lake County Captains</td>\n",
       "      <td>[(-81.4362001, 41.6407581), (-81.4362008, 41.6...</td>\n",
       "      <td>[(-81.4362001, 41.6407581), (-81.4350347, 41.6...</td>\n",
       "      <td>pro</td>\n",
       "      <td>(-81.4362001, 41.6407581)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABC Supply Stadium - Beloit Sky Carp</td>\n",
       "      <td>[(-89.0406651, 42.4971349), (-89.0408006, 42.4...</td>\n",
       "      <td>[(-89.0406651, 42.4971349), (-89.0394729, 42.4...</td>\n",
       "      <td>pro</td>\n",
       "      <td>(-89.0406651, 42.4971349)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Veterans Memorial Stadium - Cedar Rapids Kernels</td>\n",
       "      <td>[(-91.6867962, 41.9677456), (-91.6867801, 41.9...</td>\n",
       "      <td>[(-91.6867962, 41.9677456), (-91.6856006, 41.9...</td>\n",
       "      <td>pro</td>\n",
       "      <td>(-91.6867962, 41.9677456)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           park_name  \\\n",
       "0                 Day Air Ballpark - Dayton Dragons    \n",
       "1               Parkview Field - Fort Wayne TinCaps    \n",
       "2                Classic Park- Lake County Captains    \n",
       "3              ABC Supply Stadium - Beloit Sky Carp    \n",
       "4  Veterans Memorial Stadium - Cedar Rapids Kernels    \n",
       "\n",
       "                                                foul  \\\n",
       "0  [(-84.18557, 39.7642091), (-84.1849444, 39.764...   \n",
       "1  [(-85.1429982, 41.0741133), (-85.1432208, 41.0...   \n",
       "2  [(-81.4362001, 41.6407581), (-81.4362008, 41.6...   \n",
       "3  [(-89.0406651, 42.4971349), (-89.0408006, 42.4...   \n",
       "4  [(-91.6867962, 41.9677456), (-91.6867801, 41.9...   \n",
       "\n",
       "                                                 fop level  \\\n",
       "0  [(-84.18557, 39.7642091), (-84.1846064, 39.763...   pro   \n",
       "1  [(-85.1429982, 41.0741133), (-85.1418777, 41.0...   pro   \n",
       "2  [(-81.4362001, 41.6407581), (-81.4350347, 41.6...   pro   \n",
       "3  [(-89.0406651, 42.4971349), (-89.0394729, 42.4...   pro   \n",
       "4  [(-91.6867962, 41.9677456), (-91.6856006, 41.9...   pro   \n",
       "\n",
       "                  home_plate  \n",
       "0    (-84.18557, 39.7642091)  \n",
       "1  (-85.1429982, 41.0741133)  \n",
       "2  (-81.4362001, 41.6407581)  \n",
       "3  (-89.0406651, 42.4971349)  \n",
       "4  (-91.6867962, 41.9677456)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()\n",
    "\n",
    "## Value counts for the fop_direction column\n",
    "df_cleaned['level'].value_counts()\n",
    "\n",
    "## Value counts for the foul_direction column\n",
    "# df_cleaned['foul_direction'].value_counts()\n",
    "\n",
    "df_cleaned.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate Areas of Foul and FOP\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "import pyproj\n",
    "\n",
    "\n",
    "#### Functions to calculate the area of a polygon\n",
    "def utm_zone_number(lat, lon):\n",
    "    return int((lon + 180) / 6) + 1\n",
    "\n",
    "def utm_epsg_code(lat, lon):\n",
    "    zone_number = utm_zone_number(lat, lon)\n",
    "    if lat >= 0:\n",
    "        return 32600 + zone_number\n",
    "    else:\n",
    "        return 32700 + zone_number\n",
    "\n",
    "def calculate_area(coords):\n",
    "    # Create a Polygon object from the coordinates\n",
    "    polygon = Polygon(coords)\n",
    "\n",
    "    # Get the appropriate UTM EPSG code\n",
    "    epsg_code = utm_epsg_code(*coords[0])\n",
    "\n",
    "    # Create a transformer for converting coordinates to UTM\n",
    "    transformer = pyproj.Transformer.from_crs(\n",
    "        pyproj.CRS(\"EPSG:4326\"),  # WGS 84 (latitude and longitude)\n",
    "        pyproj.CRS(f\"EPSG:{epsg_code}\"),  # UTM zone\n",
    "        always_xy=True\n",
    "    )\n",
    "\n",
    "    # Convert the coordinates to UTM\n",
    "    coords_utm = [transformer.transform(*coord) for coord in coords]\n",
    "\n",
    "    # Create a Polygon object from the UTM coordinates\n",
    "    polygon_utm = Polygon(coords_utm)\n",
    "\n",
    "    # Calculate the area in square meters\n",
    "    area_sqm = polygon_utm.area\n",
    "\n",
    "    # Convert the area to square feet (1 square meter = 10.764 square feet)\n",
    "    area_sqft = area_sqm * 10.764\n",
    "\n",
    "    return area_sqft\n",
    "\n",
    "\n",
    "### Call Function and add to dataframe\n",
    "df_cleaned['foul_area_sqft'] = df_cleaned['foul'].apply(calculate_area)\n",
    "df_cleaned['fop_area_sqft'] = df_cleaned['fop'].apply(calculate_area)\n",
    "\n",
    "## Calculate the total area of the field and the ratio of foul area to field area\n",
    "df_cleaned['field_area_sqft'] = df_cleaned['foul_area_sqft'] + df_cleaned['fop_area_sqft']\n",
    "## Percentage foul area\n",
    "df_cleaned['foul_area_per'] = df_cleaned['foul_area_sqft'] / df_cleaned['field_area_sqft']\n",
    "## Fair to Foul Ratio\n",
    "df_cleaned['fair_to_foul'] = df_cleaned['fop_area_sqft'] / df_cleaned['foul_area_sqft']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SIMPLE DISTANCE CALCULATION USING GEOPY ####\n",
    "\n",
    "## Returns a list of distances from home plate to each outfield coordinate\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "def calculate_distances(home_plate, outfield_coords):\n",
    "    def is_same_point(point1, point2, tolerance=1e-6):\n",
    "        return abs(point1[0] - point2[0]) < tolerance and abs(point1[1] - point2[1]) < tolerance\n",
    "\n",
    "    home_plate_lat_lon = (home_plate[1], home_plate[0])\n",
    "    distances = [\n",
    "        round(great_circle(home_plate_lat_lon, (coord[1], coord[0])).feet)\n",
    "        for coord in outfield_coords\n",
    "        if not is_same_point(home_plate, coord)\n",
    "    ]\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run Distance Functions and add to dataframe\n",
    "\n",
    "# Calculate distances for each row\n",
    "df_cleaned['distances'] = df_cleaned.apply(lambda row: calculate_distances(row['home_plate'], row['fop']), axis=1)\n",
    "\n",
    "# Calculate max, min, and average distances for each row\n",
    "df_cleaned['max_distance'] = df_cleaned['distances'].apply(max)\n",
    "df_cleaned['min_distance'] = df_cleaned['distances'].apply(min)\n",
    "df_cleaned['avg_distance'] = df_cleaned['distances'].apply(lambda distances: sum(distances) / len(distances))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculate Ranks for each field\n",
    "### Grouped by level\n",
    "\n",
    "def rank_fields(df):\n",
    "    # Calculate the rank for each category\n",
    "    df['max_distance_rank'] = df['max_distance'].rank(ascending=False, method='min')\n",
    "    df['min_distance_rank'] = df['min_distance'].rank(ascending=False, method='min')\n",
    "    df['avg_distance_rank'] = df['avg_distance'].rank(ascending=False, method='min')\n",
    "    df['field_area_rank'] = df['field_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['ratio_rank'] = df['fair_to_foul'].rank(ascending=False, method='min')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Group the DataFrame by level and apply the rank_fields function to each group\n",
    "df_ranked = df_cleaned.groupby('level').apply(rank_fields)\n",
    "\n",
    "# Reset the index to get the original DataFrame structure\n",
    "df_ranked.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park_name</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>level</th>\n",
       "      <th>home_plate</th>\n",
       "      <th>foul_area_sqft</th>\n",
       "      <th>fop_area_sqft</th>\n",
       "      <th>field_area_sqft</th>\n",
       "      <th>foul_area_per</th>\n",
       "      <th>fair_to_foul</th>\n",
       "      <th>distances</th>\n",
       "      <th>max_distance</th>\n",
       "      <th>min_distance</th>\n",
       "      <th>avg_distance</th>\n",
       "      <th>max_distance_rank</th>\n",
       "      <th>min_distance_rank</th>\n",
       "      <th>avg_distance_rank</th>\n",
       "      <th>field_area_rank</th>\n",
       "      <th>ratio_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>SW Christian HS Fort Worth - high school</td>\n",
       "      <td>[(-97.4421601, 32.6418776), (-97.4431985, 32.6...</td>\n",
       "      <td>[(-97.4421601, 32.6418776), (-97.4421306000000...</td>\n",
       "      <td>high_school</td>\n",
       "      <td>(-97.4421601, 32.6418776)</td>\n",
       "      <td>37106.931904</td>\n",
       "      <td>169215.122852</td>\n",
       "      <td>206322.054756</td>\n",
       "      <td>0.179850</td>\n",
       "      <td>4.560202</td>\n",
       "      <td>[314, 337, 371, 342, 354, 370, 362, 407, 413, ...</td>\n",
       "      <td>413</td>\n",
       "      <td>314</td>\n",
       "      <td>357.272727</td>\n",
       "      <td>41.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Soo High School</td>\n",
       "      <td>[(-84.36583, 46.4813183), (-84.3658206, 46.482...</td>\n",
       "      <td>[(-84.36583, 46.4813183), (-84.3645398, 46.481...</td>\n",
       "      <td>high_school</td>\n",
       "      <td>(-84.36583, 46.4813183)</td>\n",
       "      <td>48045.311892</td>\n",
       "      <td>132184.170959</td>\n",
       "      <td>180229.482852</td>\n",
       "      <td>0.266579</td>\n",
       "      <td>2.751240</td>\n",
       "      <td>[324, 353, 357, 370, 373, 359, 326]</td>\n",
       "      <td>373</td>\n",
       "      <td>324</td>\n",
       "      <td>351.714286</td>\n",
       "      <td>162.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>372.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Frankfort-Elberta Area HS</td>\n",
       "      <td>[(-86.2258505, 44.6344082), (-86.2258357, 44.6...</td>\n",
       "      <td>[(-86.2258505, 44.6344082), (-86.2270454, 44.6...</td>\n",
       "      <td>high_school</td>\n",
       "      <td>(-86.2258505, 44.6344082)</td>\n",
       "      <td>35981.554245</td>\n",
       "      <td>121616.932836</td>\n",
       "      <td>157598.487081</td>\n",
       "      <td>0.228312</td>\n",
       "      <td>3.379980</td>\n",
       "      <td>[310, 310, 316, 336, 360, 388, 384, 382, 381, ...</td>\n",
       "      <td>388</td>\n",
       "      <td>284</td>\n",
       "      <td>333.466667</td>\n",
       "      <td>78.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Ironwood HS</td>\n",
       "      <td>[(-90.1514578, 46.457154), (-90.1505766, 46.45...</td>\n",
       "      <td>[(-90.1514578, 46.457154), (-90.15234150000002...</td>\n",
       "      <td>high_school</td>\n",
       "      <td>(-90.1514578, 46.457154)</td>\n",
       "      <td>48494.539685</td>\n",
       "      <td>125485.905623</td>\n",
       "      <td>173980.445308</td>\n",
       "      <td>0.278736</td>\n",
       "      <td>2.587630</td>\n",
       "      <td>[317, 327, 338, 327, 331, 336, 346, 361, 378, ...</td>\n",
       "      <td>380</td>\n",
       "      <td>308</td>\n",
       "      <td>344.120000</td>\n",
       "      <td>117.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>419.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Morenci HS</td>\n",
       "      <td>[(-84.2069811, 41.7223814), (-84.2058371, 41.7...</td>\n",
       "      <td>[(-84.2069811, 41.7223814), (-84.2071112, 41.7...</td>\n",
       "      <td>high_school</td>\n",
       "      <td>(-84.2069811, 41.7223814)</td>\n",
       "      <td>58086.513759</td>\n",
       "      <td>151942.087288</td>\n",
       "      <td>210028.601047</td>\n",
       "      <td>0.276565</td>\n",
       "      <td>2.615789</td>\n",
       "      <td>[317, 318, 323, 338, 352, 359, 365, 369, 371, ...</td>\n",
       "      <td>373</td>\n",
       "      <td>313</td>\n",
       "      <td>347.217391</td>\n",
       "      <td>162.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>407.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Peck HS</td>\n",
       "      <td>[(-82.8102804, 43.2558198), (-82.8101611, 43.2...</td>\n",
       "      <td>[(-82.8102804, 43.2558198), (-82.8114552, 43.2...</td>\n",
       "      <td>high_school</td>\n",
       "      <td>(-82.8102804, 43.2558198)</td>\n",
       "      <td>43909.781357</td>\n",
       "      <td>120815.200910</td>\n",
       "      <td>164724.982266</td>\n",
       "      <td>0.266564</td>\n",
       "      <td>2.751442</td>\n",
       "      <td>[313, 312, 313, 313, 314, 316, 318, 319, 321, ...</td>\n",
       "      <td>329</td>\n",
       "      <td>309</td>\n",
       "      <td>317.434783</td>\n",
       "      <td>470.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>426.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>371.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>North Farmington HS</td>\n",
       "      <td>[(-83.3755168, 42.5173338), (-83.3755249, 42.5...</td>\n",
       "      <td>[(-83.3755168, 42.5173338), (-83.3743165, 42.5...</td>\n",
       "      <td>high_school</td>\n",
       "      <td>(-83.3755168, 42.5173338)</td>\n",
       "      <td>41693.623084</td>\n",
       "      <td>149235.258216</td>\n",
       "      <td>190928.881300</td>\n",
       "      <td>0.218373</td>\n",
       "      <td>3.579331</td>\n",
       "      <td>[323, 325, 331, 341, 349, 356, 364, 370, 375, ...</td>\n",
       "      <td>388</td>\n",
       "      <td>323</td>\n",
       "      <td>362.960000</td>\n",
       "      <td>78.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Walled Lake Western HS</td>\n",
       "      <td>[(-83.5120908, 42.5313788), (-83.5121565, 42.5...</td>\n",
       "      <td>[(-83.5120908, 42.5313788), (-83.5109066, 42.5...</td>\n",
       "      <td>high_school</td>\n",
       "      <td>(-83.5120908, 42.5313788)</td>\n",
       "      <td>37115.609386</td>\n",
       "      <td>135866.247409</td>\n",
       "      <td>172981.856795</td>\n",
       "      <td>0.214564</td>\n",
       "      <td>3.660623</td>\n",
       "      <td>[319, 318, 320, 325, 333, 339, 351, 357, 365, ...</td>\n",
       "      <td>379</td>\n",
       "      <td>318</td>\n",
       "      <td>340.894737</td>\n",
       "      <td>123.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Petersburg Summerfield HS</td>\n",
       "      <td>[(-83.7031615, 41.9032963), (-83.7020551, 41.9...</td>\n",
       "      <td>[(-83.7031615, 41.9032963), (-83.7031293, 41.9...</td>\n",
       "      <td>high_school</td>\n",
       "      <td>(-83.7031615, 41.9032963)</td>\n",
       "      <td>67425.740499</td>\n",
       "      <td>136231.035720</td>\n",
       "      <td>203656.776219</td>\n",
       "      <td>0.331075</td>\n",
       "      <td>2.020460</td>\n",
       "      <td>[301, 305, 313, 326, 341, 345, 347, 348, 350, ...</td>\n",
       "      <td>351</td>\n",
       "      <td>300</td>\n",
       "      <td>333.120000</td>\n",
       "      <td>378.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>489.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Mount Pleasant HS</td>\n",
       "      <td>[(-84.7619912, 43.5902784), (-84.7620006, 43.5...</td>\n",
       "      <td>[(-84.7619912, 43.5902784), (-84.7608405, 43.5...</td>\n",
       "      <td>high_school</td>\n",
       "      <td>(-84.7619912, 43.5902784)</td>\n",
       "      <td>39369.979087</td>\n",
       "      <td>131171.999952</td>\n",
       "      <td>170541.979039</td>\n",
       "      <td>0.230852</td>\n",
       "      <td>3.331777</td>\n",
       "      <td>[304, 309, 314, 318, 322, 327, 332, 336, 342, ...</td>\n",
       "      <td>373</td>\n",
       "      <td>304</td>\n",
       "      <td>337.172414</td>\n",
       "      <td>162.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>226.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    park_name  \\\n",
       "49   SW Christian HS Fort Worth - high school   \n",
       "60                            Soo High School   \n",
       "61                  Frankfort-Elberta Area HS   \n",
       "62                                Ironwood HS   \n",
       "101                                Morenci HS   \n",
       "102                                   Peck HS   \n",
       "103                       North Farmington HS   \n",
       "104                    Walled Lake Western HS   \n",
       "105                 Petersburg Summerfield HS   \n",
       "106                         Mount Pleasant HS   \n",
       "\n",
       "                                                  foul  \\\n",
       "49   [(-97.4421601, 32.6418776), (-97.4431985, 32.6...   \n",
       "60   [(-84.36583, 46.4813183), (-84.3658206, 46.482...   \n",
       "61   [(-86.2258505, 44.6344082), (-86.2258357, 44.6...   \n",
       "62   [(-90.1514578, 46.457154), (-90.1505766, 46.45...   \n",
       "101  [(-84.2069811, 41.7223814), (-84.2058371, 41.7...   \n",
       "102  [(-82.8102804, 43.2558198), (-82.8101611, 43.2...   \n",
       "103  [(-83.3755168, 42.5173338), (-83.3755249, 42.5...   \n",
       "104  [(-83.5120908, 42.5313788), (-83.5121565, 42.5...   \n",
       "105  [(-83.7031615, 41.9032963), (-83.7020551, 41.9...   \n",
       "106  [(-84.7619912, 43.5902784), (-84.7620006, 43.5...   \n",
       "\n",
       "                                                   fop        level  \\\n",
       "49   [(-97.4421601, 32.6418776), (-97.4421306000000...  high_school   \n",
       "60   [(-84.36583, 46.4813183), (-84.3645398, 46.481...  high_school   \n",
       "61   [(-86.2258505, 44.6344082), (-86.2270454, 44.6...  high_school   \n",
       "62   [(-90.1514578, 46.457154), (-90.15234150000002...  high_school   \n",
       "101  [(-84.2069811, 41.7223814), (-84.2071112, 41.7...  high_school   \n",
       "102  [(-82.8102804, 43.2558198), (-82.8114552, 43.2...  high_school   \n",
       "103  [(-83.3755168, 42.5173338), (-83.3743165, 42.5...  high_school   \n",
       "104  [(-83.5120908, 42.5313788), (-83.5109066, 42.5...  high_school   \n",
       "105  [(-83.7031615, 41.9032963), (-83.7031293, 41.9...  high_school   \n",
       "106  [(-84.7619912, 43.5902784), (-84.7608405, 43.5...  high_school   \n",
       "\n",
       "                    home_plate  foul_area_sqft  fop_area_sqft  \\\n",
       "49   (-97.4421601, 32.6418776)    37106.931904  169215.122852   \n",
       "60     (-84.36583, 46.4813183)    48045.311892  132184.170959   \n",
       "61   (-86.2258505, 44.6344082)    35981.554245  121616.932836   \n",
       "62    (-90.1514578, 46.457154)    48494.539685  125485.905623   \n",
       "101  (-84.2069811, 41.7223814)    58086.513759  151942.087288   \n",
       "102  (-82.8102804, 43.2558198)    43909.781357  120815.200910   \n",
       "103  (-83.3755168, 42.5173338)    41693.623084  149235.258216   \n",
       "104  (-83.5120908, 42.5313788)    37115.609386  135866.247409   \n",
       "105  (-83.7031615, 41.9032963)    67425.740499  136231.035720   \n",
       "106  (-84.7619912, 43.5902784)    39369.979087  131171.999952   \n",
       "\n",
       "     field_area_sqft  foul_area_per  fair_to_foul  \\\n",
       "49     206322.054756       0.179850      4.560202   \n",
       "60     180229.482852       0.266579      2.751240   \n",
       "61     157598.487081       0.228312      3.379980   \n",
       "62     173980.445308       0.278736      2.587630   \n",
       "101    210028.601047       0.276565      2.615789   \n",
       "102    164724.982266       0.266564      2.751442   \n",
       "103    190928.881300       0.218373      3.579331   \n",
       "104    172981.856795       0.214564      3.660623   \n",
       "105    203656.776219       0.331075      2.020460   \n",
       "106    170541.979039       0.230852      3.331777   \n",
       "\n",
       "                                             distances  max_distance  \\\n",
       "49   [314, 337, 371, 342, 354, 370, 362, 407, 413, ...           413   \n",
       "60                 [324, 353, 357, 370, 373, 359, 326]           373   \n",
       "61   [310, 310, 316, 336, 360, 388, 384, 382, 381, ...           388   \n",
       "62   [317, 327, 338, 327, 331, 336, 346, 361, 378, ...           380   \n",
       "101  [317, 318, 323, 338, 352, 359, 365, 369, 371, ...           373   \n",
       "102  [313, 312, 313, 313, 314, 316, 318, 319, 321, ...           329   \n",
       "103  [323, 325, 331, 341, 349, 356, 364, 370, 375, ...           388   \n",
       "104  [319, 318, 320, 325, 333, 339, 351, 357, 365, ...           379   \n",
       "105  [301, 305, 313, 326, 341, 345, 347, 348, 350, ...           351   \n",
       "106  [304, 309, 314, 318, 322, 327, 332, 336, 342, ...           373   \n",
       "\n",
       "     min_distance  avg_distance  max_distance_rank  min_distance_rank  \\\n",
       "49            314    357.272727               41.0              148.0   \n",
       "60            324    351.714286              162.0               44.0   \n",
       "61            284    333.466667               78.0              425.0   \n",
       "62            308    344.120000              117.0              242.0   \n",
       "101           313    347.217391              162.0              165.0   \n",
       "102           309    317.434783              470.0              225.0   \n",
       "103           323    362.960000               78.0               64.0   \n",
       "104           318    340.894737              123.0              116.0   \n",
       "105           300    333.120000              378.0              320.0   \n",
       "106           304    337.172414              162.0              284.0   \n",
       "\n",
       "     avg_distance_rank  field_area_rank  ratio_rank  \n",
       "49                42.0             16.0        47.0  \n",
       "60                79.0            174.0       372.0  \n",
       "61               300.0            412.0       214.0  \n",
       "62               163.0            242.0       419.0  \n",
       "101              123.0             12.0       407.0  \n",
       "102              426.0            348.0       371.0  \n",
       "103               26.0             84.0       172.0  \n",
       "104              202.0            259.0       159.0  \n",
       "105              308.0             27.0       489.0  \n",
       "106              260.0            284.0       226.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Show samples of the data from each level\n",
    "\n",
    "df_ranked[df_ranked['level'] == 'high_school'].head(10)\n",
    "\n",
    "# df_ranked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not sure if this is useful - will need to revisit\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def calculate_orientation(coordinates):\n",
    "#     num_points = len(coordinates)\n",
    "#     if num_points < 3:\n",
    "#         return \"Not enough points\"\n",
    "\n",
    "#     signed_area = 0\n",
    "\n",
    "#     for i in range(num_points):\n",
    "#         x1, y1 = coordinates[i]\n",
    "#         x2, y2 = coordinates[(i + 1) % num_points]\n",
    "#         signed_area += (x2 - x1) * (y2 + y1)\n",
    "\n",
    "#     if signed_area > 0:\n",
    "#         return \"Counterclockwise\"\n",
    "#     elif signed_area < 0:\n",
    "#         return \"Clockwise\"\n",
    "#     else:\n",
    "#         return \"Collinear\"\n",
    "\n",
    "# # Apply the calculate_orientation function to the 'fop' and 'foul' columns\n",
    "# df_cleaned['fop_orientation'] = df_cleaned['fop'].apply(calculate_orientation)\n",
    "# df_cleaned['foul_orientation'] = df_cleaned['foul'].apply(calculate_orientation)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Show value counts od orientation columns\n",
    "# print(df_cleaned['fop_orientation'].value_counts())\n",
    "# print(df_cleaned['foul_orientation'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing coordinates:  46%|████▌     | 329/716 [03:30<10:35,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeocoderServiceError for coordinates: (-82.5080461, 42.7145549)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing coordinates:  68%|██████▊   | 484/716 [05:40<04:45,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeocoderServiceError for coordinates: (-85.712159, 42.9943182)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing coordinates:  68%|██████▊   | 488/716 [05:46<04:57,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeocoderServiceError for coordinates: (-84.7006386, 42.7518189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing coordinates:  68%|██████▊   | 489/716 [05:47<04:01,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeocoderServiceError for coordinates: (-84.7539894, 42.7472938)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing coordinates:  68%|██████▊   | 490/716 [05:47<03:22,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeocoderServiceError for coordinates: (-84.7588678, 42.7462458)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing coordinates:  69%|██████▊   | 492/716 [05:52<05:28,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeocoderServiceError for coordinates: (-85.6282667, 42.8167582)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing coordinates:  69%|██████▉   | 494/716 [05:52<03:25,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeocoderServiceError for coordinates: (-84.20022232995262, 42.2449163182406)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing coordinates:  90%|████████▉ | 643/716 [07:47<01:41,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeocoderServiceError for coordinates: (-83.684233, 43.4537664)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing coordinates:  90%|█████████ | 645/716 [07:48<01:03,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeocoderServiceError for coordinates: (-83.683368, 43.4548134)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing coordinates:  91%|█████████ | 649/716 [07:56<01:56,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeocoderServiceError for coordinates: (-83.1761582, 42.1760001)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing coordinates: 100%|██████████| 716/716 [08:57<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "### Get Geolocation of each field based on home plate coordinates and return state and country\n",
    "### This block takes a long time to run - will need to revisit\n",
    "## up to ten minutes\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "from tqdm import tqdm\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"baseball_field_locator\")\n",
    "\n",
    "# Function to get location information\n",
    "def get_location_info(lng, lat):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lng), timeout=10)\n",
    "        state = location.raw['address'].get('state', None)\n",
    "        country = location.raw['address'].get('country', None)\n",
    "        return state, country\n",
    "    except GeocoderTimedOut:\n",
    "        print(f\"GeocoderTimedOut error for coordinates: ({lng}, {lat})\")\n",
    "        return None, None\n",
    "    except GeocoderServiceError:\n",
    "        print(f\"GeocoderServiceError for coordinates: ({lng}, {lat})\")\n",
    "        return None, None\n",
    "\n",
    "# Extract the first coordinate for each field\n",
    "df_cleaned['lng'], df_cleaned['lat'] = zip(*df_cleaned['home_plate'].apply(lambda x: x))\n",
    "\n",
    "# Wrap the DataFrame apply function with tqdm for progress indication\n",
    "tqdm.pandas(desc=\"Processing coordinates\")\n",
    "\n",
    "# Get state and country information for each field\n",
    "df_cleaned[['state', 'country']] = df_cleaned.progress_apply(lambda row: get_location_info(row['lng'], row['lat']), axis=1, result_type='expand')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           park_name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            foul                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          fop level                 home_plate  foul_area_sqft  fop_area_sqft  field_area_sqft  foul_area_per  fair_to_foul                                                                                                                                                                                                                                                             distances  max_distance  min_distance  avg_distance        lng        lat      state        country\n",
      "0                 Day Air Ballpark - Dayton Dragons   [(-84.18557, 39.7642091), (-84.1849444, 39.7649498), (-84.1849786, 39.7649436), (-84.1851033, 39.7647859), (-84.1851925, 39.7647416), (-84.1855606, 39.7644498), (-84.1855586, 39.7644406), (-84.1857068, 39.7643241), (-84.1857155, 39.7643282), (-84.185737, 39.7643102), (-84.185751, 39.7642947), (-84.1857651, 39.764272), (-84.1857745, 39.7642468), (-84.1857812, 39.7642246), (-84.1857819, 39.7642009), (-84.1857792, 39.7641767), (-84.1857698, 39.7641529), (-84.1857497, 39.7641261), (-84.1857282, 39.7641066), (-84.1857101, 39.7640916), (-84.185688, 39.7640808), (-84.1856605, 39.7640633), (-84.1856411, 39.7640566), (-84.185627, 39.7640607), (-84.185405, 39.7639916), (-84.1854097, 39.7639844), (-84.1852629, 39.7639432), (-84.1849987, 39.7638628), (-84.1849491, 39.7638468), (-84.1848478, 39.7638334), (-84.18465, 39.7637334), (-84.1846064, 39.763738), (-84.18557, 39.7642091)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [(-84.18557, 39.7642091), (-84.1846064, 39.763738), (-84.1845642, 39.7637483), (-84.1845139, 39.7637597), (-84.1844924, 39.7637808), (-84.1844327, 39.7638375), (-84.1843583, 39.7639117), (-84.1842832, 39.7639813), (-84.1842356, 39.7641117), (-84.1841893, 39.764237), (-84.1841471, 39.7643452), (-84.1841806, 39.764454), (-84.1842041, 39.7645586), (-84.184249, 39.7645844), (-84.1843255, 39.7646406), (-84.184469, 39.7647457), (-84.1845695, 39.7648189), (-84.1846446, 39.7648081), (-84.1847171, 39.764856), (-84.1848632, 39.7649596), (-84.1849444, 39.7649498), (-84.18557, 39.7642091)]   pro    (-84.18557, 39.7642091)    35842.340679  179080.935713    214923.276392       0.166768      4.996352                                                                                                                                                                  [320, 328, 339, 340, 347, 357, 370, 376, 387, 402, 400, 404, 395, 383, 366, 358, 339, 336, 338, 322]           404           320    360.350000 -84.185570  39.764209       Ohio  United States\n",
      "1               Parkview Field - Fort Wayne TinCaps                                                                              [(-85.1429982, 41.0741133), (-85.1432208, 41.0750167), (-85.1432409, 41.0750187), (-85.1431665, 41.0747316), (-85.1432181, 41.07468), (-85.1432013, 41.0743585), (-85.1431953, 41.0741604), (-85.1431698, 41.0741598), (-85.1431712, 41.0741416), (-85.1431712, 41.0741164), (-85.1431685, 41.0740956), (-85.1431631, 41.0740785), (-85.1431504, 41.0740562), (-85.1431343, 41.074033), (-85.1431162, 41.0740188), (-85.1430907, 41.0740052), (-85.1430686, 41.0739945), (-85.1430431, 41.073987), (-85.1430169, 41.0739865), (-85.1429915, 41.0739834), (-85.1429606, 41.073988), (-85.1429231, 41.0739961), (-85.1428848, 41.0740057), (-85.1428768, 41.0739895), (-85.1424892, 41.0740987), (-85.1421781, 41.0741821), (-85.142166, 41.074222), (-85.1418897, 41.0742685), (-85.1418777, 41.0742807), (-85.1429982, 41.0741133)]  [(-85.1429982, 41.0741133), (-85.1418777, 41.0742807), (-85.1418689, 41.0742877), (-85.1418595, 41.0742942), (-85.1418468, 41.0743033), (-85.141834, 41.0743104), (-85.1418273, 41.0743119), (-85.1418287, 41.0743948), (-85.141832, 41.0744696), (-85.141834, 41.0745338), (-85.141834, 41.0745692), (-85.1418367, 41.0746006), (-85.1418441, 41.074639), (-85.1418548, 41.0746789), (-85.1418669, 41.0747118), (-85.141881, 41.0747411), (-85.1419031, 41.07478), (-85.1419212, 41.0748088), (-85.141948, 41.0748457), (-85.1419688, 41.07487), (-85.142005, 41.0749023), (-85.1420285, 41.0749246), (-85.142062, 41.0749529), (-85.1420962, 41.0749761), (-85.1421344, 41.0749999), (-85.142172, 41.0750232), (-85.1422203, 41.0750454), (-85.1422766, 41.0750681), (-85.1423249, 41.0750843), (-85.1423798, 41.0751), (-85.1424362, 41.0751106), (-85.1424905, 41.0751202), (-85.14252, 41.0751237), (-85.1426078, 41.0751126), (-85.1426742, 41.075103), (-85.1427077, 41.0750975), (-85.1427668, 41.0750914), (-85.1427835, 41.0750853), (-85.1428117, 41.0750732), (-85.1428378, 41.0750656), (-85.1428633, 41.0750565), (-85.1428848, 41.0750489), (-85.1429116, 41.0750414), (-85.1429431, 41.0750343), (-85.142976, 41.0750292), (-85.1430088, 41.0750232), (-85.143039, 41.0750196), (-85.1430812, 41.0750156), (-85.1431148, 41.0750135), (-85.143153, 41.0750105), (-85.1431731, 41.0750135), (-85.1432019, 41.0750115), (-85.1432208, 41.0750167), (-85.1429982, 41.0741133)]   pro  (-85.1429982, 41.0741133)    30974.660210  174006.866640    204981.526850       0.151110      5.617717  [314, 317, 320, 324, 328, 330, 338, 346, 355, 361, 366, 371, 376, 380, 383, 387, 390, 393, 395, 397, 398, 400, 401, 401, 402, 402, 401, 400, 398, 395, 393, 391, 380, 372, 368, 362, 359, 354, 350, 346, 343, 339, 336, 334, 332, 331, 330, 330, 330, 332, 332, 335]           402           314    362.461538 -85.142998  41.074113    Indiana  United States\n",
      "2                Classic Park- Lake County Captains                                                                                                                                                                                                                                                                                                                                                                                                              [(-81.4362001, 41.6407581), (-81.4362008, 41.6416521), (-81.4362987, 41.6416546), (-81.4362967, 41.6412853), (-81.4363939, 41.6412262), (-81.4363932, 41.6408268), (-81.4363919, 41.6407491), (-81.4363791, 41.64069), (-81.4362973, 41.6406243), (-81.4362216, 41.6406143), (-81.4361364, 41.6406143), (-81.4358601, 41.6406143), (-81.4355778, 41.6406133), (-81.4354987, 41.640686), (-81.4352244, 41.6406905), (-81.4350347, 41.6406915), (-81.4350347, 41.6407561), (-81.4362001, 41.6407581)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [(-81.4362001, 41.6407581), (-81.4350347, 41.6407561), (-81.4350307, 41.640763), (-81.4350301, 41.6407891), (-81.4350301, 41.6408973), (-81.4350301, 41.6410301), (-81.4350294, 41.6411649), (-81.4350294, 41.6412661), (-81.4350435, 41.6413478), (-81.4350576, 41.641421), (-81.4351112, 41.6414666), (-81.4351695, 41.6415172), (-81.4352869, 41.6416194), (-81.4353902, 41.641635), (-81.4355383, 41.6416535), (-81.4357261, 41.641653), (-81.4360024, 41.6416525), (-81.4362008, 41.6416521), (-81.4362001, 41.6407581)]   pro  (-81.4362001, 41.6407581)    48647.683251  171063.165183    219710.848434       0.221417      3.516368                                                                                                                                                                                 [318, 319, 319, 323, 334, 352, 369, 382, 394, 394, 395, 401, 389, 373, 351, 331, 326]           401           318    357.058824 -81.436200  41.640758       Ohio  United States\n",
      "3              ABC Supply Stadium - Beloit Sky Carp                                                                                                                                                                                                                                                                         [(-89.0406651, 42.4971349), (-89.0408006, 42.4980792), (-89.040822, 42.4980772), (-89.040763, 42.4976491), (-89.0408247, 42.4975947), (-89.0409213, 42.4975799), (-89.0408797, 42.4973841), (-89.0408556, 42.4973841), (-89.0408207, 42.4971636), (-89.0408583, 42.4971616), (-89.0408502, 42.4970963), (-89.0408234, 42.4970439), (-89.040755, 42.4970004), (-89.0406853, 42.4969905), (-89.0405874, 42.4970054), (-89.04059, 42.4970242), (-89.0403057, 42.4970459), (-89.0403004, 42.4970281), (-89.0400268, 42.4970459), (-89.0397572, 42.4971922), (-89.0394742, 42.4972259), (-89.0394729, 42.4972358), (-89.0406651, 42.4971349)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        [(-89.0406651, 42.4971349), (-89.0394729, 42.4972358), (-89.0394742, 42.4973584), (-89.0394689, 42.4974711), (-89.0394689, 42.4975621), (-89.0394675, 42.4976283), (-89.0394648, 42.4976738), (-89.0394716, 42.4977143), (-89.0394809, 42.4977539), (-89.039497, 42.4977915), (-89.0395185, 42.4978241), (-89.039544, 42.4978567), (-89.0395855, 42.4978943), (-89.0396177, 42.497922), (-89.0396942, 42.4979714), (-89.0397921, 42.4980308), (-89.0398457, 42.4980575), (-89.0399329, 42.4981079), (-89.0399932, 42.4981464), (-89.0401944, 42.4981296), (-89.0404318, 42.4981079), (-89.0406638, 42.4980921), (-89.0408006, 42.4980792), (-89.0406651, 42.4971349)]   pro  (-89.0406651, 42.4971349)    30336.095693  151369.981351    181706.077044       0.166951      4.989765                                                                                                                                                        [323, 331, 344, 358, 369, 378, 384, 390, 395, 398, 400, 401, 402, 402, 402, 402, 406, 411, 384, 360, 349, 346]           411           323    378.863636 -89.040665  42.497135  Wisconsin  United States\n",
      "4  Veterans Memorial Stadium - Cedar Rapids Kernels                                                                          [(-91.6867962, 41.9677456), (-91.6867801, 41.9686166), (-91.6868056, 41.9686166), (-91.68680959999999, 41.9684541), (-91.6869276, 41.9683374), (-91.6869819, 41.9680513), (-91.686935, 41.9680468), (-91.68694099999999, 41.9680144), (-91.6869893, 41.9678), (-91.68699740000001, 41.9677661), (-91.6869947, 41.9677466), (-91.6869913, 41.9677202), (-91.6869806, 41.9676973), (-91.68696650000001, 41.9676798), (-91.6869531, 41.9676654), (-91.6869337, 41.9676429), (-91.6869156, 41.9676335), (-91.68688400000002, 41.9676195), (-91.6868478, 41.967607), (-91.68681359999998, 41.9676016), (-91.6867788, 41.9675976), (-91.68639989999998, 41.9676345), (-91.6863872, 41.9676026), (-91.68602239999998, 41.9677093), (-91.6859412, 41.9677078), (-91.6856107, 41.9677058), (-91.6856006, 41.9677352), (-91.6867962, 41.9677456)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [(-91.6867962, 41.9677456), (-91.6856006, 41.9677352), (-91.6855939, 41.9677676), (-91.6855677, 41.9678568), (-91.6855362, 41.967976), (-91.6855107, 41.9680792), (-91.6854839, 41.9681754), (-91.6855081, 41.9682213), (-91.6855677, 41.9683265), (-91.68561400000002, 41.9684187), (-91.6856737, 41.9684646), (-91.6857729, 41.9685424), (-91.68584469999999, 41.9685952), (-91.68593919999999, 41.968668), (-91.6860137, 41.9686685), (-91.6861357, 41.9686695), (-91.6862517, 41.968672), (-91.6863838, 41.968673), (-91.6864985, 41.9686745), (-91.6865327, 41.9686431), (-91.6865602, 41.9686161), (-91.6866869, 41.9686166), (-91.6867801, 41.9686166), (-91.6867962, 41.9677456)]   pro  (-91.6867962, 41.9677456)    33792.513058  157481.450065    191273.963123       0.176671      4.660247                                                                                                                                                        [324, 326, 336, 352, 369, 389, 390, 395, 404, 402, 402, 403, 409, 398, 382, 369, 356, 348, 335, 324, 319, 318]           409           318    365.909091 -91.686796  41.967746       Iowa  United States\n"
     ]
    }
   ],
   "source": [
    "### Print the DataFrame to a string for reference\n",
    "\n",
    "df_cleaned_str = df_cleaned.head().to_string()\n",
    "print(df_cleaned_str)\n",
    "\n",
    "# Output the string to a txt file for reference\n",
    "with open('data/df_cleaned_str.txt', 'w') as f:\n",
    "    f.write(df_cleaned_str)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Orienting the map to the home plate location ####\n",
    "\n",
    "### Find the center of the field\n",
    "def calculate_centroid(coords):\n",
    "    x_coords = [coord[0] for coord in coords]\n",
    "    y_coords = [coord[1] for coord in coords]\n",
    "    centroid_x = sum(x_coords) / len(coords)\n",
    "    centroid_y = sum(y_coords) / len(coords)\n",
    "    return (centroid_x, centroid_y)\n",
    "\n",
    "\n",
    "## Find the bearing between the home plate and the center of the field\n",
    "import math\n",
    "\n",
    "def calculate_bearing(point1, point2):\n",
    "    lat1, lon1 = math.radians(point1[1]), math.radians(point1[0])\n",
    "    lat2, lon2 = math.radians(point2[1]), math.radians(point2[0])\n",
    "\n",
    "    d_lon = lon2 - lon1\n",
    "\n",
    "    x = math.cos(lat2) * math.sin(d_lon)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(d_lon)\n",
    "\n",
    "    bearing = math.degrees(math.atan2(x, y))\n",
    "    bearing = (bearing + 360) % 360  # Normalize the bearing to the range [0, 360)\n",
    "\n",
    "    return bearing\n",
    "\n",
    "### Function to classify direction in laymans terms North, South, East, West, ect\n",
    "def degrees_to_cardinal_direction(degrees):\n",
    "    directions = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW', 'N']\n",
    "    index = round(degrees / 45)\n",
    "    return directions[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the centroid of the outfield fence coordinates for each row\n",
    "df_cleaned['fop_centroid'] = df_cleaned['fop'].apply(lambda coords: calculate_centroid(coords[1:]))\n",
    "\n",
    "# Calculate the bearing between home plate and the centroid for each row\n",
    "df_cleaned['field_orientation'] = df_cleaned.apply(lambda row: calculate_bearing(row['home_plate'], row['fop_centroid']), axis=1)\n",
    "\n",
    "# Convert the bearing to a cardinal direction\n",
    "df_cleaned['field_cardinal_direction'] = df_cleaned['field_orientation'].apply(degrees_to_cardinal_direction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rename the Cleaned Dataframe back to the default name\n",
    "df = df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### In the fop, foul and home_plate columns, the coordinates are in the format (longitude, latitude).\n",
    "# ### This is the opposite of the format that is used in Google Maps, so we need to reverse the order of the coordinates.\n",
    "# df['fop'] = df['fop'].apply(lambda coords: [(coord[1], coord[0]) for coord in coords])\n",
    "# df['foul'] = df['foul'].apply(lambda coords: [(coord[1], coord[0]) for coord in coords])\n",
    "# df['home_plate'] = df['home_plate'].apply(lambda coord: (coord[1], coord[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_22336\\944074035.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hs_df['match'] = hs_df['park_name'].apply(lambda x: get_best_match(x, mhsaa_df['school_name'], threshold))\n",
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_22336\\944074035.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hs_df[col] = hs_df['match'].apply(lambda x: mhsaa_df.loc[mhsaa_df['school_name'] == x, col].iloc[0] if not mhsaa_df.loc[mhsaa_df['school_name'] == x, col].empty else None)\n",
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_22336\\944074035.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hs_df[col] = hs_df['match'].apply(lambda x: mhsaa_df.loc[mhsaa_df['school_name'] == x, col].iloc[0] if not mhsaa_df.loc[mhsaa_df['school_name'] == x, col].empty else None)\n",
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_22336\\944074035.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hs_df[col] = hs_df['match'].apply(lambda x: mhsaa_df.loc[mhsaa_df['school_name'] == x, col].iloc[0] if not mhsaa_df.loc[mhsaa_df['school_name'] == x, col].empty else None)\n",
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_22336\\944074035.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hs_df[col] = hs_df['match'].apply(lambda x: mhsaa_df.loc[mhsaa_df['school_name'] == x, col].iloc[0] if not mhsaa_df.loc[mhsaa_df['school_name'] == x, col].empty else None)\n"
     ]
    }
   ],
   "source": [
    "########## TURNED OFF FOR NOW ###########\n",
    "#****** Don't need school info. want to test new fields\n",
    "\n",
    "#### MATCHING HIGH SCHOOL NAMES TO THE MHSAA DATA #####\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# Read the enrollment table from MHSAA website - 2022 enrollment\n",
    "mhsaa_df = pd.read_csv('data/school_info/mhsaa_enrolment_2022.csv')\n",
    "\n",
    "# Select just the high school level\n",
    "hs_df = df[df['level'] == 'high_school']\n",
    "other_df = df[df['level'] != 'high_school']\n",
    "\n",
    "# Set the threshold for the fuzzy match\n",
    "threshold = 90\n",
    "\n",
    "# Define a function to get the best fuzzy match with the threshold\n",
    "def get_best_match(field_name, school_names, threshold):\n",
    "    best_match = process.extractOne(field_name, school_names, scorer=fuzz.token_set_ratio)\n",
    "    if best_match[1] >= threshold:\n",
    "        return best_match[0]\n",
    "    return None\n",
    "\n",
    "# Apply the function to the 'field' column and store the result in a new 'match' column\n",
    "hs_df['match'] = hs_df['park_name'].apply(lambda x: get_best_match(x, mhsaa_df['school_name'], threshold))\n",
    "\n",
    "# #### This destroys a bunch of the data, at least every high school outside of michigan\n",
    "# # Drop rows with no match\n",
    "# # hs_df = hs_df.dropna(subset=['match'])\n",
    "\n",
    "# Lookup the hs_df['match'] in the mhsaa_df and return the columns: 'school_id', 'school_name', 'students', 'division'\n",
    "columns_to_extract = ['school_id', 'school_name', 'students', 'division']\n",
    "for col in columns_to_extract:\n",
    "    hs_df[col] = hs_df['match'].apply(lambda x: mhsaa_df.loc[mhsaa_df['school_name'] == x, col].iloc[0] if not mhsaa_df.loc[mhsaa_df['school_name'] == x, col].empty else None)\n",
    "\n",
    "# Merge the hs_df and other_df back together\n",
    "merged_df = pd.concat([hs_df, other_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = df\n",
    "merged_df.head(10)\n",
    "\n",
    "# # Save the merged_df DataFrame as a JSON file\n",
    "merged_df.to_json('data/default_updated_output.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['park_name', 'foul', 'fop', 'level', 'home_plate', 'foul_area_sqft',\n",
      "       'fop_area_sqft', 'field_area_sqft', 'foul_area_per', 'fair_to_foul',\n",
      "       'distances', 'max_distance', 'min_distance', 'avg_distance', 'lng',\n",
      "       'lat', 'state', 'country', 'fop_centroid', 'field_orientation',\n",
      "       'field_cardinal_direction', 'match', 'school_id', 'school_name',\n",
      "       'students', 'division'],\n",
      "      dtype='object')\n",
      "0    SW Christian HS Fort Worth - high school\n",
      "1                             Soo High School\n",
      "2                   Frankfort-Elberta Area HS\n",
      "3                                 Ironwood HS\n",
      "4                                  Morenci HS\n",
      "5                                     Peck HS\n",
      "6                         North Farmington HS\n",
      "7                      Walled Lake Western HS\n",
      "8                   Petersburg Summerfield HS\n",
      "9                           Mount Pleasant HS\n"
     ]
    }
   ],
   "source": [
    "column_list = merged_df.columns\n",
    "print(column_list)\n",
    "\n",
    "## Print a list of ten high school field names to test mascot lookup\n",
    "print(merged_df[merged_df['level'] == 'high_school']['park_name'].head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Code to create report on the json file structure to be used as a reference later\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# from collections import defaultdict\n",
    "# from builtins import list, dict\n",
    "\n",
    "# # Load the JSON data from the file\n",
    "# with open('data/updated_output_data.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# # Analyze the JSON data\n",
    "# def analyze_structure(data, prefix=''):\n",
    "#     structure = defaultdict(set)\n",
    "    \n",
    "#     if isinstance(data, dict):\n",
    "#         for key, value in data.items():\n",
    "#             new_prefix = f'{prefix}.{key}' if prefix else key\n",
    "#             structure[new_prefix].add(type(value))\n",
    "#             structure.update(analyze_structure(value, new_prefix))\n",
    "#     elif isinstance(data, list):\n",
    "#         for item in data:\n",
    "#             structure.update(analyze_structure(item, prefix))\n",
    "    \n",
    "#     return structure\n",
    "\n",
    "# # Generate the report\n",
    "# structure = analyze_structure(data)\n",
    "# descriptions = {\n",
    "#     'field_name': 'The name of the baseball field.',\n",
    "#     'foul': 'A list of coordinates representing the foul territory of the field. (lat, lon)',\n",
    "#     'fop': 'A list of coordinates representing the fair territory of the field. (lat, lon)',\n",
    "#     'level': 'The level of the field, e.g., high_school, college, etc.',\n",
    "#     'home_plate': 'A list of coordinates representing the home plate location on the field. (lat, lon)',\n",
    "#     'foul_area_sqft': 'The total area of the foul territory in square feet.',\n",
    "#     'fop_area_sqft': 'The total area of the fair territory in square feet.',\n",
    "#     'distances': 'A list of distances from home plate to the outfield fence at the vertices of the wall.',\n",
    "#     'max_distance': 'The maximum distance from home plate to the outfield fence.',\n",
    "#     'min_distance': 'The minimum distance from home plate to the outfield fence.',\n",
    "#     'avg_distance': 'The average distance from home plate to the outfield fence.',\n",
    "#     'fop_centroid': 'A list of coordinates representing the centroid of the fair territory.',\n",
    "#     'field_orientation': \"The angle (in degrees) of the field's orientation, with 0 degrees being North.\",\n",
    "#     'field_cardinal_direction': \"The cardinal direction abbreviation (N, S, E, W, NE, NW, SE, SW) representing the field's orientation.\",\n",
    "#     'match': 'The matched school name found using fuzzy matching.',\n",
    "#     'school_id': 'The unique identifier of the matched school.',\n",
    "#     'school_name': 'The name of the matched school.',\n",
    "#     'students': 'The number of students enrolled in the matched school.',\n",
    "#     'division': 'The athletic division the matched school belongs to.'\n",
    "# }\n",
    "\n",
    "# # Replace <filename> with your desired filename without the extension\n",
    "# filename = \"output_data\"\n",
    "\n",
    "# def get_sample_value(data, key):\n",
    "#     for item in data:\n",
    "#         if key in item and item[key] is not None:\n",
    "#             return item[key]\n",
    "#     return None\n",
    "\n",
    "# # Generate the report with sample values\n",
    "# report = pd.DataFrame([(key, ', '.join([t.__name__ for t in types]), descriptions.get(key, ''), get_sample_value(data, key)) \n",
    "#                        for key, types in structure.items()],\n",
    "#                       columns=['Key', 'Data Types', 'Description', 'Sample Value'])\n",
    "\n",
    "# # Save the report as a CSV file\n",
    "# report.to_csv(f\"{filename}_report.csv\", index=False)\n",
    "\n",
    "# print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/default_updated_output.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to process the data, counting the orientations and filtering by level.\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_data(data, level_filter=None):\n",
    "    count_by_orientation = defaultdict(int)\n",
    "    \n",
    "    for record in data:\n",
    "        if level_filter is None or record['level'] == level_filter:\n",
    "            orientation = round(record['field_orientation'])\n",
    "            count_by_orientation[orientation] += 1\n",
    "\n",
    "    return count_by_orientation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polar_chart(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 2 * np.pi / num_bins\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "\n",
    "    ax.set_facecolor('#808080')\n",
    "    ###\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "    \n",
    "    # # Set dark background\n",
    "    # ax.set_facecolor('#2b2b2b')\n",
    "    plt.gca().set_rlabel_position(22.5)\n",
    "    ax.set_ylim(-10, 100)  # Adjust based on max count\n",
    "\n",
    "    bars = ax.bar(bin_edges[:-1], bin_counts, width=bin_width, bottom=-20)\n",
    "    \n",
    "\n",
    "\n",
    "    # Use custom colors and opacity\n",
    "    for r, bar in zip(bin_counts, bars):\n",
    "        bar.set_facecolor(plt.cm.viridis(r / max(bin_counts)))\n",
    "        # bar.set_facecolor(plt.cm.plasma(r / max(bin_counts)))\n",
    "        bar.set_alpha(0.8)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UPDATED GPT CODE - LATE NIGHT FRIDAY\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_polar_chart(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 2 * np.pi / num_bins\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "\n",
    "    ax.set_facecolor('#808080')\n",
    "    ###\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "    \n",
    "    # # Set dark background\n",
    "    ax.set_facecolor('#2b2b2b')\n",
    "    plt.gca().set_rlabel_position(22.5)\n",
    "    ax.set_ylim(-20, 130)  # Adjust based on max count\n",
    "\n",
    "    # Add bars for negative values\n",
    "    zero_height_bars = ax.bar(bin_edges[:-1], np.abs(ax.get_ylim()[0]), width=bin_width, bottom=0.0, color='k', alpha=0.3)\n",
    "\n",
    "    bars = ax.bar(bin_edges[:-1], bin_counts, width=bin_width, bottom=0)\n",
    "    \n",
    "    # Use custom colors and opacity\n",
    "    for r, bar in zip(bin_counts, bars):\n",
    "        bar.set_facecolor(plt.cm.viridis(r / max(bin_counts)))\n",
    "        # bar.set_facecolor(plt.cm.plasma(r / max(bin_counts)))\n",
    "        bar.set_alpha(0.8)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_polar_chart(data, num_bins=50, level_filter=None)\n",
    "\n",
    "\n",
    "#### GOAL\n",
    "## fill the center portion of the plot to create a heat map of the field orientations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_polar_chart(data, num_bins=180, level_filter=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HEATMAP CODE\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_heatmap(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "\n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "\n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "\n",
    "    # Reshape histogram data into a 2D array\n",
    "    heatmap_data = np.tile(bin_counts, (num_bins, 1))\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Create heatmap\n",
    "    plt.imshow(heatmap_data, cmap='viridis', aspect='auto', interpolation='nearest', origin='lower')\n",
    "    plt.colorbar(label='Counts')\n",
    "\n",
    "    # Set x-axis ticks and labels\n",
    "    plt.xticks(np.arange(0, num_bins, num_bins // 6), np.arange(0, 361, 60))\n",
    "    plt.xlabel('Orientation (degrees)')\n",
    "\n",
    "    # Set y-axis ticks and labels (assuming equal radial divisions)\n",
    "    max_radius_label = 'Max Radius'\n",
    "    plt.yticks(np.arange(0, num_bins, num_bins // 6), [0, 1, 2, 3, 4, max_radius_label])\n",
    "    plt.ylabel('Radial Division')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# create_heatmap(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "create_heatmap(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### EXAMPLE# CODE FROM MATPLOTLIB GALLERY\n",
    "# =======================\n",
    "# Pie chart on polar axis\n",
    "# =======================\n",
    "\n",
    "# Demo of bar plot on a polar axis.\n",
    "# \"\"\"\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # Compute pie slices\n",
    "# N = 20\n",
    "# theta = np.linspace(0.0, 2 * np.pi, N, endpoint=False)\n",
    "# radii = 10 * np.random.rand(N)\n",
    "# width = np.pi / 4 * np.random.rand(N)\n",
    "\n",
    "# ax = plt.subplot(111, projection='polar')\n",
    "# bars = ax.bar(theta, radii, width=width, bottom=0.0)\n",
    "\n",
    "# # Use custom colors and opacity\n",
    "# for r, bar in zip(radii, bars):\n",
    "#     bar.set_facecolor(plt.cm.viridis(r / 10.))\n",
    "#     bar.set_alpha(0.5)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot a histogram of the field orientations for all levels\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_histogram(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 360.0, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 360 / num_bins\n",
    "\n",
    "    # Plot the histogram\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(bin_edges[:-1], bin_counts, width=bin_width, edgecolor='black')\n",
    "    ax.set_xlabel('Field Orientation (degrees)')\n",
    "    ax.set_ylabel('Number of Fields')\n",
    "    ax.set_title('Field Orientation Histogram')\n",
    "\n",
    "    # Set the major tick locations\n",
    "    major_tick_locations = [45, 135, 225, 315]\n",
    "    plt.xticks(major_tick_locations, major_tick_locations)\n",
    "\n",
    "\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_histogram(data, num_bins=36, level_filter=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END WORK BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['field_cardinal_direction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Rename the dataframe back to the default name\n",
    "df = merged_df\n",
    "\n",
    "# Set the plot size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.scatter(df_cleaned['min_distance'], df_cleaned['max_distance'], alpha=0.8)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Min Distance (feet)')\n",
    "plt.ylabel('Max Distance (feet)')\n",
    "plt.title('Scatter Plot of Min and Max Distances to Outfield Fence')\n",
    "\n",
    "# Display the plot in the Jupyter Notebook\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CODE TO MAKE A LIST OF OUTLIERS #####\n",
    "\n",
    "# Filter the DataFrame for fields with min distances below 100 feet\n",
    "outliers = df[df['min_distance'] < 100]\n",
    "\n",
    "# Display the outlier fields in the Jupyter Notebook\n",
    "print(outliers[['park_name', 'min_distance', 'max_distance']])\n",
    "\n",
    "# Save the outlier fields to a CSV file\n",
    "outliers.to_csv('outlier_fields.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
