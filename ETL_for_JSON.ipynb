{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dependencies\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is for extracting the data from the original kml file and outputing a JSON that will be easy to use with Google Maps API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD BLOCK###\n",
    "#### Load data from kml file exported from Google Earth\n",
    "\n",
    "file_path = ('data/kml/ballparks.kml') # file path to kml file\n",
    "\n",
    "with open(file_path) as file:\n",
    "\n",
    "    xml_data = file.read()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize soup variables for parsing file\n",
    "soup = BeautifulSoup(xml_data, 'xml')\n",
    "folders = soup.Document.Folder\n",
    "list = soup.Document.Folder.find_all('Folder')\n",
    "\n",
    "## Create a dataframe to hold the data parsed from xml\n",
    "df = pd.DataFrame(columns=['field', 'foul', 'fop'])\n",
    "\n",
    "failed = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing folder: Progressive Field - Cleveland - MLB. Error message: list index out of range\n",
      "Failed to process 1 folders: Progressive Field - Cleveland - MLB\n"
     ]
    }
   ],
   "source": [
    "#### EXTRACTION BLOCK ####\n",
    "#### Extract data from kml file\n",
    "\n",
    "# Create an empty list to store the rows to append to the DataFrame\n",
    "rows = []\n",
    "\n",
    "# Loop through the folders and extract the data\n",
    "for folder in list:\n",
    "    try:\n",
    "        field_name = folder.find('name').text\n",
    "        foul = folder.find_all('coordinates')[0].text\n",
    "        fop = folder.find_all('coordinates')[1].text\n",
    "\n",
    "        row = {\n",
    "            'field': field_name,\n",
    "            'foul': foul,\n",
    "            'fop': fop\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Add name of folder to a list of failed folders\n",
    "        failed.append(folder.find('name').text)\n",
    "        print(f\"Error processing folder: {folder.find('name').text}. Error message: {str(e)}\")\n",
    "\n",
    "# Convert the list of rows to a DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Print a list of failed folders\n",
    "print(f\"Failed to process {len(failed)} folders: {', '.join(failed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary that maps level indicators to levels and size factors\n",
    "level_dict = {\n",
    "    'International': 'international',\n",
    "    'Major Leagues': 'mlb', \n",
    "    'Professional': 'pro', \n",
    "    'College': 'college', \n",
    "    'High School': 'high_school',\n",
    "    'Youth': 'youth',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original DataFrame\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Remove new line and space characters from coordinates\n",
    "df_cleaned = df_cleaned.replace(r'\\n','', regex=True) \n",
    "df_cleaned = df_cleaned.replace(r'\\t','', regex=True) \n",
    "\n",
    "# Drop any duplicate rows\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['field'], keep='first')\n",
    "\n",
    "# Drop any rows with empty fields\n",
    "df_cleaned = df_cleaned[(df_cleaned != 0).all(1)]\n",
    "\n",
    "# Define the regex patterns for each level\n",
    "re_mlb = re.compile(r'mlb', re.IGNORECASE)\n",
    "re_pro = re.compile(r'pro|semi[-\\s]*pro', re.IGNORECASE)\n",
    "re_college = re.compile(r'college', re.IGNORECASE)\n",
    "re_high_school = re.compile(r'high school|hs', re.IGNORECASE)  # Include the abbreviation 'hs'\n",
    "re_youth = re.compile(r'youth', re.IGNORECASE)\n",
    "re_muni = re.compile(r'muni', re.IGNORECASE)\n",
    "re_international = re.compile(r'international', re.IGNORECASE)\n",
    "\n",
    "# Define a function to classify the fields based on the regex patterns\n",
    "def classify_field(field_name):\n",
    "    if re_mlb.search(field_name):\n",
    "        return 'Major League'\n",
    "    elif re_pro.search(field_name):\n",
    "        return 'Professional'\n",
    "    elif re_college.search(field_name):\n",
    "        return 'College'\n",
    "    elif re_high_school.search(field_name):\n",
    "        return 'High School'\n",
    "    elif re_youth.search(field_name):\n",
    "        return 'Youth'\n",
    "    elif re_muni.search(field_name):\n",
    "        return 'State / County / Municipal'\n",
    "    elif re_international.search(field_name):\n",
    "        return 'International'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Apply the classify_field function to the 'field' column\n",
    "df_cleaned['level'] = df_cleaned['field'].apply(classify_field)\n",
    "\n",
    "# Clean up the 'field' column by removing the level indicator and any trailing '-' characters\n",
    "level_regex = r'\\s*(%s)\\s*' % '|'.join(re.escape(level) for level in level_dict.values())\n",
    "df_cleaned['field'] = df_cleaned['field'].str.replace(level_regex, '', regex=True, flags=re.IGNORECASE)\n",
    "df_cleaned['field'] = df_cleaned['field'].str.replace(r'-\\s*$', '', regex=True)\n",
    "\n",
    "# Rename field column to park_name to avoid confusion down the line\n",
    "df_cleaned = df_cleaned.rename(columns={'field': 'park_name'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park_name</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Day Air Ballpark - Dayton Dragons</td>\n",
       "      <td>-84.18557,39.7642091,0 -84.18494440000001,39.7...</td>\n",
       "      <td>-84.18557,39.7642091,0 -84.18460640000001,39.7...</td>\n",
       "      <td>Professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Parkview Field - Fort Wayne TinCaps</td>\n",
       "      <td>-85.14299819999999,41.0741133,0 -85.1432207999...</td>\n",
       "      <td>-85.14299819999999,41.0741133,0 -85.1418776999...</td>\n",
       "      <td>Professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic Park- Lake County Captains</td>\n",
       "      <td>-81.43620009999999,41.6407581,0 -81.4362007999...</td>\n",
       "      <td>-81.43620009999999,41.6407581,0 -81.4350347,41...</td>\n",
       "      <td>Professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABC Supply Stadium - Beloit Sky Carp</td>\n",
       "      <td>-89.0406651,42.4971349,0 -89.0408006,42.498079...</td>\n",
       "      <td>-89.0406651,42.4971349,0 -89.03947290000001,42...</td>\n",
       "      <td>Professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Veterans Memorial Stadium - Cedar Rapids Kernels</td>\n",
       "      <td>-91.6867962,41.9677456,0 -91.68678009999999,41...</td>\n",
       "      <td>-91.6867962,41.9677456,0 -91.6856006,41.967735...</td>\n",
       "      <td>Professional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           park_name  \\\n",
       "0                 Day Air Ballpark - Dayton Dragons    \n",
       "1               Parkview Field - Fort Wayne TinCaps    \n",
       "2                Classic Park- Lake County Captains    \n",
       "3              ABC Supply Stadium - Beloit Sky Carp    \n",
       "4  Veterans Memorial Stadium - Cedar Rapids Kernels    \n",
       "\n",
       "                                                foul  \\\n",
       "0  -84.18557,39.7642091,0 -84.18494440000001,39.7...   \n",
       "1  -85.14299819999999,41.0741133,0 -85.1432207999...   \n",
       "2  -81.43620009999999,41.6407581,0 -81.4362007999...   \n",
       "3  -89.0406651,42.4971349,0 -89.0408006,42.498079...   \n",
       "4  -91.6867962,41.9677456,0 -91.68678009999999,41...   \n",
       "\n",
       "                                                 fop         level  \n",
       "0  -84.18557,39.7642091,0 -84.18460640000001,39.7...  Professional  \n",
       "1  -85.14299819999999,41.0741133,0 -85.1418776999...  Professional  \n",
       "2  -81.43620009999999,41.6407581,0 -81.4350347,41...  Professional  \n",
       "3  -89.0406651,42.4971349,0 -89.03947290000001,42...  Professional  \n",
       "4  -91.6867962,41.9677456,0 -91.6856006,41.967735...  Professional  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()\n",
    "\n",
    "## print a list of all the values in the level column\n",
    "# print(df_cleaned['level'].unique())\n",
    "\n",
    "## Print the two the headers and two rows of data to a txt file for reference\n",
    "\n",
    "\n",
    "# with open('data/rows.txt', 'w') as f:\n",
    "#     f.write(df_cleaned.iloc[0].to_string())\n",
    "#     f.write(df_cleaned.iloc[1].to_string())\n",
    "#     f.write(df_cleaned.iloc[14].to_string())\n",
    "    \n",
    "\n",
    "# print(df_cleaned.iloc[15].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Clean up polygon data and create a new home_plate column\n",
    "\n",
    "def parse_coordinates(coord_string):\n",
    "    coords = coord_string.split()\n",
    "    parsed_coords = [tuple(map(float, coord.split(',')[:2])) for coord in coords]\n",
    "    return parsed_coords\n",
    "\n",
    "# Create a new column for the home_plate location using the first set of coordinates in the 'fop' column\n",
    "df_cleaned['home_plate'] = df_cleaned['fop'].apply(lambda x: parse_coordinates(x)[0])\n",
    "\n",
    "# Apply the parse_coordinates function to the 'foul' and 'fop' columns\n",
    "df_cleaned['foul'] = df_cleaned['foul'].apply(parse_coordinates)\n",
    "df_cleaned['fop'] = df_cleaned['fop'].apply(parse_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doesn't seem to be returning useful data - will need to revisit\n",
    "\n",
    "# def determine_direction(coordinates):\n",
    "#     num_points = len(coordinates)\n",
    "#     if num_points < 2:\n",
    "#         return \"Not enough points\"\n",
    "\n",
    "#     # Get the latitude (y-coordinate) values\n",
    "#     latitudes = [point[1] for point in coordinates]\n",
    "\n",
    "#     # Check the change in latitude values\n",
    "#     increasing_latitudes = all(latitudes[i] <= latitudes[i+1] for i in range(num_points-1))\n",
    "#     decreasing_latitudes = all(latitudes[i] >= latitudes[i+1] for i in range(num_points-1))\n",
    "\n",
    "#     if increasing_latitudes:\n",
    "#         return \"Left Field\"\n",
    "#     elif decreasing_latitudes:\n",
    "#         return \"Right Field\"\n",
    "#     else:\n",
    "#         return \"Collinear\"\n",
    "\n",
    "# # Apply the determine_direction function to the 'fop' and 'foul' columns\n",
    "# df_cleaned['fop_direction'] = df_cleaned['fop'].apply(determine_direction)\n",
    "# df_cleaned['foul_direction'] = df_cleaned['foul'].apply(determine_direction)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park_name</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>level</th>\n",
       "      <th>home_plate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Day Air Ballpark - Dayton Dragons</td>\n",
       "      <td>[(-84.18557, 39.7642091), (-84.1849444, 39.764...</td>\n",
       "      <td>[(-84.18557, 39.7642091), (-84.1846064, 39.763...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>(-84.18557, 39.7642091)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Parkview Field - Fort Wayne TinCaps</td>\n",
       "      <td>[(-85.1429982, 41.0741133), (-85.1432208, 41.0...</td>\n",
       "      <td>[(-85.1429982, 41.0741133), (-85.1418777, 41.0...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>(-85.1429982, 41.0741133)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic Park- Lake County Captains</td>\n",
       "      <td>[(-81.4362001, 41.6407581), (-81.4362008, 41.6...</td>\n",
       "      <td>[(-81.4362001, 41.6407581), (-81.4350347, 41.6...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>(-81.4362001, 41.6407581)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABC Supply Stadium - Beloit Sky Carp</td>\n",
       "      <td>[(-89.0406651, 42.4971349), (-89.0408006, 42.4...</td>\n",
       "      <td>[(-89.0406651, 42.4971349), (-89.0394729, 42.4...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>(-89.0406651, 42.4971349)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Veterans Memorial Stadium - Cedar Rapids Kernels</td>\n",
       "      <td>[(-91.6867962, 41.9677456), (-91.6867801, 41.9...</td>\n",
       "      <td>[(-91.6867962, 41.9677456), (-91.6856006, 41.9...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>(-91.6867962, 41.9677456)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           park_name  \\\n",
       "0                 Day Air Ballpark - Dayton Dragons    \n",
       "1               Parkview Field - Fort Wayne TinCaps    \n",
       "2                Classic Park- Lake County Captains    \n",
       "3              ABC Supply Stadium - Beloit Sky Carp    \n",
       "4  Veterans Memorial Stadium - Cedar Rapids Kernels    \n",
       "\n",
       "                                                foul  \\\n",
       "0  [(-84.18557, 39.7642091), (-84.1849444, 39.764...   \n",
       "1  [(-85.1429982, 41.0741133), (-85.1432208, 41.0...   \n",
       "2  [(-81.4362001, 41.6407581), (-81.4362008, 41.6...   \n",
       "3  [(-89.0406651, 42.4971349), (-89.0408006, 42.4...   \n",
       "4  [(-91.6867962, 41.9677456), (-91.6867801, 41.9...   \n",
       "\n",
       "                                                 fop         level  \\\n",
       "0  [(-84.18557, 39.7642091), (-84.1846064, 39.763...  Professional   \n",
       "1  [(-85.1429982, 41.0741133), (-85.1418777, 41.0...  Professional   \n",
       "2  [(-81.4362001, 41.6407581), (-81.4350347, 41.6...  Professional   \n",
       "3  [(-89.0406651, 42.4971349), (-89.0394729, 42.4...  Professional   \n",
       "4  [(-91.6867962, 41.9677456), (-91.6856006, 41.9...  Professional   \n",
       "\n",
       "                  home_plate  \n",
       "0    (-84.18557, 39.7642091)  \n",
       "1  (-85.1429982, 41.0741133)  \n",
       "2  (-81.4362001, 41.6407581)  \n",
       "3  (-89.0406651, 42.4971349)  \n",
       "4  (-91.6867962, 41.9677456)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()\n",
    "\n",
    "## Value counts for the fop_direction column\n",
    "df_cleaned['level'].value_counts()\n",
    "\n",
    "## Value counts for the foul_direction column\n",
    "# df_cleaned['foul_direction'].value_counts()\n",
    "\n",
    "df_cleaned.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate Areas of Foul and FOP\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "import pyproj\n",
    "\n",
    "\n",
    "#### Functions to calculate the area of a polygon\n",
    "def utm_zone_number(lat, lon):\n",
    "    return int((lon + 180) / 6) + 1\n",
    "\n",
    "def utm_epsg_code(lat, lon):\n",
    "    zone_number = utm_zone_number(lat, lon)\n",
    "    if lat >= 0:\n",
    "        return 32600 + zone_number\n",
    "    else:\n",
    "        return 32700 + zone_number\n",
    "\n",
    "def calculate_area(coords):\n",
    "    # Create a Polygon object from the coordinates\n",
    "    polygon = Polygon(coords)\n",
    "\n",
    "    # Get the appropriate UTM EPSG code\n",
    "    epsg_code = utm_epsg_code(*coords[0])\n",
    "\n",
    "    # Create a transformer for converting coordinates to UTM\n",
    "    transformer = pyproj.Transformer.from_crs(\n",
    "        pyproj.CRS(\"EPSG:4326\"),  # WGS 84 (latitude and longitude)\n",
    "        pyproj.CRS(f\"EPSG:{epsg_code}\"),  # UTM zone\n",
    "        always_xy=True\n",
    "    )\n",
    "\n",
    "    # Convert the coordinates to UTM\n",
    "    coords_utm = [transformer.transform(*coord) for coord in coords]\n",
    "\n",
    "    # Create a Polygon object from the UTM coordinates\n",
    "    polygon_utm = Polygon(coords_utm)\n",
    "\n",
    "    # Calculate the area in square meters\n",
    "    area_sqm = polygon_utm.area\n",
    "\n",
    "    # Convert the area to square feet (1 square meter = 10.764 square feet)\n",
    "    area_sqft = area_sqm * 10.764\n",
    "\n",
    "    return area_sqft\n",
    "\n",
    "\n",
    "### Call Function and add to dataframe\n",
    "df_cleaned['foul_area_sqft'] = df_cleaned['foul'].apply(calculate_area)\n",
    "df_cleaned['fop_area_sqft'] = df_cleaned['fop'].apply(calculate_area)\n",
    "\n",
    "## Calculate the total area of the field and the ratio of foul area to field area\n",
    "df_cleaned['field_area_sqft'] = df_cleaned['foul_area_sqft'] + df_cleaned['fop_area_sqft']\n",
    "## Percentage foul area\n",
    "df_cleaned['foul_area_per'] = df_cleaned['foul_area_sqft'] / df_cleaned['field_area_sqft']\n",
    "## Fair to Foul Ratio\n",
    "df_cleaned['fair_to_foul'] = df_cleaned['fop_area_sqft'] / df_cleaned['foul_area_sqft']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Justin\\anaconda3\\lib\\site-packages\\geopy\\point.py:472: UserWarning: Latitude normalization has been prohibited in the newer versions of geopy, because the normalized value happened to be on a different pole, which is probably not what was meant. If you pass coordinates as positional args, please make sure that the order is (latitude, longitude) or (y, x) in Cartesian terms.\n",
      "  return cls(*args)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Latitude must be in the [-90; 90] range.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Justin\\Desktop\\Project\\BB_parks\\ETL_for_JSON.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m distances\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Calculate distances for each row using the 'fop' column\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m df_cleaned[\u001b[39m'\u001b[39m\u001b[39mdistances\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_cleaned\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m row: calculate_distances(row[\u001b[39m'\u001b[39;49m\u001b[39mhome_plate\u001b[39;49m\u001b[39m'\u001b[39;49m], row[\u001b[39m'\u001b[39;49m\u001b[39mfop\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msafe_max\u001b[39m(distances):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(distances) \u001b[39mif\u001b[39;00m distances \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:8839\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   8828\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m   8830\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m   8831\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   8832\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8837\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m   8838\u001b[0m )\n\u001b[1;32m-> 8839\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:727\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    725\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 727\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:851\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    853\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:867\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    865\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    866\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[0;32m    868\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    869\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    870\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    871\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\Justin\\Desktop\\Project\\BB_parks\\ETL_for_JSON.ipynb Cell 12\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m distances\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Calculate distances for each row using the 'fop' column\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m df_cleaned[\u001b[39m'\u001b[39m\u001b[39mdistances\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_cleaned\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: calculate_distances(row[\u001b[39m'\u001b[39;49m\u001b[39mhome_plate\u001b[39;49m\u001b[39m'\u001b[39;49m], row[\u001b[39m'\u001b[39;49m\u001b[39mfop\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msafe_max\u001b[39m(distances):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(distances) \u001b[39mif\u001b[39;00m distances \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Justin\\Desktop\\Project\\BB_parks\\ETL_for_JSON.ipynb Cell 12\u001b[0m in \u001b[0;36mcalculate_distances\u001b[1;34m(home_plate, outfield_coords, num_points)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     interval_length \u001b[39m=\u001b[39m line_length \u001b[39m/\u001b[39m (num_points \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     points_on_line \u001b[39m=\u001b[39m [line\u001b[39m.\u001b[39minterpolate(i \u001b[39m*\u001b[39m interval_length) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_points)]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39mround\u001b[39m(great_circle(home_plate_lat_lon, (point\u001b[39m.\u001b[39my, point\u001b[39m.\u001b[39mx))\u001b[39m.\u001b[39mfeet)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mfor\u001b[39;00m point \u001b[39min\u001b[39;00m points_on_line\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mreturn\u001b[39;00m distances\n",
      "\u001b[1;32mc:\\Users\\Justin\\Desktop\\Project\\BB_parks\\ETL_for_JSON.ipynb Cell 12\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     interval_length \u001b[39m=\u001b[39m line_length \u001b[39m/\u001b[39m (num_points \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     points_on_line \u001b[39m=\u001b[39m [line\u001b[39m.\u001b[39minterpolate(i \u001b[39m*\u001b[39m interval_length) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_points)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39mround\u001b[39m(great_circle(home_plate_lat_lon, (point\u001b[39m.\u001b[39;49my, point\u001b[39m.\u001b[39;49mx))\u001b[39m.\u001b[39mfeet)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mfor\u001b[39;00m point \u001b[39min\u001b[39;00m points_on_line\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/ETL_for_JSON.ipynb#Y103sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mreturn\u001b[39;00m distances\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\geopy\\distance.py:461\u001b[0m, in \u001b[0;36mgreat_circle.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    460\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mRADIUS \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mradius\u001b[39m\u001b[39m'\u001b[39m, EARTH_RADIUS)\n\u001b[1;32m--> 461\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\geopy\\distance.py:276\u001b[0m, in \u001b[0;36mDistance.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    275\u001b[0m     \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m util\u001b[39m.\u001b[39mpairwise(args):\n\u001b[1;32m--> 276\u001b[0m         kilometers \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmeasure(a, b)\n\u001b[0;32m    278\u001b[0m kilometers \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m units\u001b[39m.\u001b[39mkilometers(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kilometers \u001b[39m=\u001b[39m kilometers\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\geopy\\distance.py:464\u001b[0m, in \u001b[0;36mgreat_circle.measure\u001b[1;34m(self, a, b)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmeasure\u001b[39m(\u001b[39mself\u001b[39m, a, b):\n\u001b[1;32m--> 464\u001b[0m     a, b \u001b[39m=\u001b[39m Point(a), Point(b)\n\u001b[0;32m    465\u001b[0m     _ensure_same_altitude(a, b)\n\u001b[0;32m    467\u001b[0m     lat1, lng1 \u001b[39m=\u001b[39m radians(degrees\u001b[39m=\u001b[39ma\u001b[39m.\u001b[39mlatitude), radians(degrees\u001b[39m=\u001b[39ma\u001b[39m.\u001b[39mlongitude)\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\geopy\\point.py:175\u001b[0m, in \u001b[0;36mPoint.__new__\u001b[1;34m(cls, latitude, longitude, altitude)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    172\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mFailed to create Point instance from \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg,)\n\u001b[0;32m    173\u001b[0m             )\n\u001b[0;32m    174\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_sequence(seq)\n\u001b[0;32m    177\u001b[0m \u001b[39mif\u001b[39;00m single_arg:\n\u001b[0;32m    178\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    179\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mA single number has been passed to the Point \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    180\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mconstructor. This is probably a mistake, because \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mto get rid of this error.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    185\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\geopy\\point.py:472\u001b[0m, in \u001b[0;36mPoint.from_sequence\u001b[1;34m(cls, seq)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    470\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mWhen creating a Point from sequence, it \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    471\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mmust not have more than 3 items.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 472\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\geopy\\point.py:188\u001b[0m, in \u001b[0;36mPoint.__new__\u001b[1;34m(cls, latitude, longitude, altitude)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mif\u001b[39;00m single_arg:\n\u001b[0;32m    178\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    179\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mA single number has been passed to the Point \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    180\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mconstructor. This is probably a mistake, because \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mto get rid of this error.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    185\u001b[0m     )\n\u001b[0;32m    187\u001b[0m latitude, longitude, altitude \u001b[39m=\u001b[39m \\\n\u001b[1;32m--> 188\u001b[0m     _normalize_coordinates(latitude, longitude, altitude)\n\u001b[0;32m    190\u001b[0m \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m)\n\u001b[0;32m    191\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatitude \u001b[39m=\u001b[39m latitude\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\geopy\\point.py:74\u001b[0m, in \u001b[0;36m_normalize_coordinates\u001b[1;34m(latitude, longitude, altitude)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mabs\u001b[39m(latitude) \u001b[39m>\u001b[39m \u001b[39m90\u001b[39m:\n\u001b[0;32m     67\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m'\u001b[39m\u001b[39mLatitude normalization has been prohibited in the newer \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     68\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mversions of geopy, because the normalized value happened \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     69\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mto be on a different pole, which is probably not what was \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39m(latitude, longitude) or (y, x) in Cartesian terms.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     73\u001b[0m                   \u001b[39mUserWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mLatitude must be in the [-90; 90] range.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mabs\u001b[39m(longitude) \u001b[39m>\u001b[39m \u001b[39m180\u001b[39m:\n\u001b[0;32m     77\u001b[0m     \u001b[39m# Longitude normalization is pretty straightforward and doesn't seem\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[39m# to be error-prone, so there's nothing to complain about.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     longitude \u001b[39m=\u001b[39m _normalize_angle(longitude, \u001b[39m180.0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Latitude must be in the [-90; 90] range."
     ]
    }
   ],
   "source": [
    "from geopy.distance import great_circle\n",
    "from shapely.geometry import LineString, Point\n",
    "\n",
    "def create_line_segments(outfield_coords):\n",
    "    outfield_coords_no_home_plate = outfield_coords[1:-1]  # Exclude first and last coordinates (home plate)\n",
    "    outfield_coords_reversed = [(coord[1], coord[0]) for coord in outfield_coords_no_home_plate]\n",
    "    return [LineString([outfield_coords_reversed[i], outfield_coords_reversed[i+1]]) for i in range(len(outfield_coords_reversed) - 1)]\n",
    "\n",
    "def sample_points_on_line(line, num_points):\n",
    "    line_length = line.length\n",
    "    interval_length = line_length / (num_points - 1)\n",
    "    return [line.interpolate(i * interval_length) for i in range(num_points)]\n",
    "\n",
    "\n",
    "def calculate_distances(home_plate, outfield_coords, num_points=30):\n",
    "    home_plate_lat_lon = (home_plate[1], home_plate[0])\n",
    "    line_segments = create_line_segments(outfield_coords)\n",
    "    distances = []\n",
    "    for line in line_segments:\n",
    "        line_length = line.length\n",
    "        interval_length = line_length / (num_points - 1)\n",
    "        points_on_line = [line.interpolate(i * interval_length) for i in range(num_points)]\n",
    "        distances += [\n",
    "            round(great_circle(home_plate_lat_lon, (point.y, point.x)).feet)\n",
    "            for point in points_on_line\n",
    "        ]\n",
    "    return distances\n",
    "\n",
    "# Calculate distances for each row using the 'fop' column\n",
    "df_cleaned['distances'] = df_cleaned.apply(lambda row: calculate_distances(row['home_plate'], row['fop']), axis=1)\n",
    "\n",
    "def safe_max(distances):\n",
    "    return max(distances) if distances else None\n",
    "\n",
    "def safe_min(distances):\n",
    "    return min(distances) if distances else None\n",
    "\n",
    "def safe_avg(distances):\n",
    "    return sum(distances) / len(distances) if distances else None\n",
    "\n",
    "def safe_median(distances):\n",
    "    return statistics.median(distances) if distances else None\n",
    "\n",
    "# Use the 'distances' column to compute max_distance, min_distance, avg_distance, and median_distance\n",
    "df_cleaned['max_distance'] = df_cleaned['distances'].apply(safe_max)\n",
    "df_cleaned['min_distance'] = df_cleaned['distances'].apply(safe_min)\n",
    "df_cleaned['avg_distance'] = df_cleaned['distances'].apply(safe_avg)\n",
    "df_cleaned['median_distance'] = df_cleaned['distances'].apply(safe_median)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park_name</th>\n",
       "      <th>foul</th>\n",
       "      <th>fop</th>\n",
       "      <th>level</th>\n",
       "      <th>home_plate</th>\n",
       "      <th>foul_area_sqft</th>\n",
       "      <th>fop_area_sqft</th>\n",
       "      <th>field_area_sqft</th>\n",
       "      <th>foul_area_per</th>\n",
       "      <th>fair_to_foul</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Day Air Ballpark - Dayton Dragons</td>\n",
       "      <td>[(-84.18557, 39.7642091), (-84.1849444, 39.764...</td>\n",
       "      <td>[(-84.18557, 39.7642091), (-84.1846064, 39.763...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>(-84.18557, 39.7642091)</td>\n",
       "      <td>35842.340679</td>\n",
       "      <td>179080.935713</td>\n",
       "      <td>214923.276392</td>\n",
       "      <td>0.166768</td>\n",
       "      <td>4.996352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Parkview Field - Fort Wayne TinCaps</td>\n",
       "      <td>[(-85.1429982, 41.0741133), (-85.1432208, 41.0...</td>\n",
       "      <td>[(-85.1429982, 41.0741133), (-85.1418777, 41.0...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>(-85.1429982, 41.0741133)</td>\n",
       "      <td>30974.660210</td>\n",
       "      <td>174006.866640</td>\n",
       "      <td>204981.526850</td>\n",
       "      <td>0.151110</td>\n",
       "      <td>5.617717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic Park- Lake County Captains</td>\n",
       "      <td>[(-81.4362001, 41.6407581), (-81.4362008, 41.6...</td>\n",
       "      <td>[(-81.4362001, 41.6407581), (-81.4350347, 41.6...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>(-81.4362001, 41.6407581)</td>\n",
       "      <td>48647.683251</td>\n",
       "      <td>171063.165183</td>\n",
       "      <td>219710.848434</td>\n",
       "      <td>0.221417</td>\n",
       "      <td>3.516368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABC Supply Stadium - Beloit Sky Carp</td>\n",
       "      <td>[(-89.0406651, 42.4971349), (-89.0408006, 42.4...</td>\n",
       "      <td>[(-89.0406651, 42.4971349), (-89.0394729, 42.4...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>(-89.0406651, 42.4971349)</td>\n",
       "      <td>30336.095693</td>\n",
       "      <td>151369.981351</td>\n",
       "      <td>181706.077044</td>\n",
       "      <td>0.166951</td>\n",
       "      <td>4.989765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Veterans Memorial Stadium - Cedar Rapids Kernels</td>\n",
       "      <td>[(-91.6867962, 41.9677456), (-91.6867801, 41.9...</td>\n",
       "      <td>[(-91.6867962, 41.9677456), (-91.6856006, 41.9...</td>\n",
       "      <td>Professional</td>\n",
       "      <td>(-91.6867962, 41.9677456)</td>\n",
       "      <td>33792.513058</td>\n",
       "      <td>157481.450065</td>\n",
       "      <td>191273.963123</td>\n",
       "      <td>0.176671</td>\n",
       "      <td>4.660247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           park_name  \\\n",
       "0                 Day Air Ballpark - Dayton Dragons    \n",
       "1               Parkview Field - Fort Wayne TinCaps    \n",
       "2                Classic Park- Lake County Captains    \n",
       "3              ABC Supply Stadium - Beloit Sky Carp    \n",
       "4  Veterans Memorial Stadium - Cedar Rapids Kernels    \n",
       "\n",
       "                                                foul  \\\n",
       "0  [(-84.18557, 39.7642091), (-84.1849444, 39.764...   \n",
       "1  [(-85.1429982, 41.0741133), (-85.1432208, 41.0...   \n",
       "2  [(-81.4362001, 41.6407581), (-81.4362008, 41.6...   \n",
       "3  [(-89.0406651, 42.4971349), (-89.0408006, 42.4...   \n",
       "4  [(-91.6867962, 41.9677456), (-91.6867801, 41.9...   \n",
       "\n",
       "                                                 fop         level  \\\n",
       "0  [(-84.18557, 39.7642091), (-84.1846064, 39.763...  Professional   \n",
       "1  [(-85.1429982, 41.0741133), (-85.1418777, 41.0...  Professional   \n",
       "2  [(-81.4362001, 41.6407581), (-81.4350347, 41.6...  Professional   \n",
       "3  [(-89.0406651, 42.4971349), (-89.0394729, 42.4...  Professional   \n",
       "4  [(-91.6867962, 41.9677456), (-91.6856006, 41.9...  Professional   \n",
       "\n",
       "                  home_plate  foul_area_sqft  fop_area_sqft  field_area_sqft  \\\n",
       "0    (-84.18557, 39.7642091)    35842.340679  179080.935713    214923.276392   \n",
       "1  (-85.1429982, 41.0741133)    30974.660210  174006.866640    204981.526850   \n",
       "2  (-81.4362001, 41.6407581)    48647.683251  171063.165183    219710.848434   \n",
       "3  (-89.0406651, 42.4971349)    30336.095693  151369.981351    181706.077044   \n",
       "4  (-91.6867962, 41.9677456)    33792.513058  157481.450065    191273.963123   \n",
       "\n",
       "   foul_area_per  fair_to_foul  \n",
       "0       0.166768      4.996352  \n",
       "1       0.151110      5.617717  \n",
       "2       0.221417      3.516368  \n",
       "3       0.166951      4.989765  \n",
       "4       0.176671      4.660247  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Figure out how long the distance list is for each row\n",
    "df_cleaned['num_distances'] = df_cleaned['distances'].apply(len)\n",
    "\n",
    "## Print the value counts for the 'num_distances' column\n",
    "df_cleaned['num_distances'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### SIMPLE DISTANCE CALCULATION USING GEOPY ####\n",
    "\n",
    "# ## Returns a list of distances from home plate to each outfield coordinate\n",
    "# from geopy.distance import great_circle\n",
    "\n",
    "# def calculate_distances(home_plate, outfield_coords):\n",
    "#     def is_same_point(point1, point2, tolerance=1e-6):\n",
    "#         return abs(point1[0] - point2[0]) < tolerance and abs(point1[1] - point2[1]) < tolerance\n",
    "\n",
    "#     home_plate_lat_lon = (home_plate[1], home_plate[0])\n",
    "#     distances = [\n",
    "#         round(great_circle(home_plate_lat_lon, (coord[1], coord[0])).feet)\n",
    "#         for coord in outfield_coords\n",
    "#         if not is_same_point(home_plate, coord)\n",
    "#     ]\n",
    "#     return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Run Distance Functions and add to dataframe\n",
    "\n",
    "# # Calculate distances for each row\n",
    "# df_cleaned['distances'] = df_cleaned.apply(lambda row: calculate_distances(row['home_plate'], row['fop']), axis=1)\n",
    "\n",
    "# # Calculate max, min, and average distances for each row\n",
    "# df_cleaned['max_distance'] = df_cleaned['distances'].apply(max)\n",
    "# df_cleaned['min_distance'] = df_cleaned['distances'].apply(min)\n",
    "# df_cleaned['avg_distance'] = df_cleaned['distances'].apply(lambda distances: sum(distances) / len(distances))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculate Ranks for each field\n",
    "### Grouped by level\n",
    "\n",
    "def rank_fields(df):\n",
    "    # Calculate the rank for each category\n",
    "    df['max_distance_rank'] = df['max_distance'].rank(ascending=False, method='min')\n",
    "    df['min_distance_rank'] = df['min_distance'].rank(ascending=False, method='min')\n",
    "    df['avg_distance_rank'] = df['avg_distance'].rank(ascending=False, method='min')\n",
    "    df['field_area_rank'] = df['field_area_sqft'].rank(ascending=False, method='min')\n",
    "    df['ratio_rank'] = df['fair_to_foul'].rank(ascending=False, method='min')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Group the DataFrame by level and apply the rank_fields function to each group\n",
    "df_ranked = df_cleaned.groupby('level').apply(rank_fields)\n",
    "\n",
    "# Reset the index to get the original DataFrame structure\n",
    "df_ranked.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename df bnack to df_cleaned\n",
    "df_cleaned = df_ranked\n",
    "\n",
    "## Show samples of the data from each level\n",
    "\n",
    "df_ranked[df_ranked['level'] == 'high_school'].head(10)\n",
    "\n",
    "# df_ranked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not sure if this is useful - will need to revisit\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def calculate_orientation(coordinates):\n",
    "#     num_points = len(coordinates)\n",
    "#     if num_points < 3:\n",
    "#         return \"Not enough points\"\n",
    "\n",
    "#     signed_area = 0\n",
    "\n",
    "#     for i in range(num_points):\n",
    "#         x1, y1 = coordinates[i]\n",
    "#         x2, y2 = coordinates[(i + 1) % num_points]\n",
    "#         signed_area += (x2 - x1) * (y2 + y1)\n",
    "\n",
    "#     if signed_area > 0:\n",
    "#         return \"Counterclockwise\"\n",
    "#     elif signed_area < 0:\n",
    "#         return \"Clockwise\"\n",
    "#     else:\n",
    "#         return \"Collinear\"\n",
    "\n",
    "# # Apply the calculate_orientation function to the 'fop' and 'foul' columns\n",
    "# df_cleaned['fop_orientation'] = df_cleaned['fop'].apply(calculate_orientation)\n",
    "# df_cleaned['foul_orientation'] = df_cleaned['foul'].apply(calculate_orientation)\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# print(df_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Show value counts od orientation columns\n",
    "# print(df_cleaned['fop_orientation'].value_counts())\n",
    "# print(df_cleaned['foul_orientation'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Print the DataFrame to a string for reference\n",
    "\n",
    "# df_cleaned_str = df_cleaned.head().to_string()\n",
    "# print(df_cleaned_str)\n",
    "\n",
    "# # Output the string to a txt file for reference\n",
    "# with open('data/df_cleaned_str.txt', 'w') as f:\n",
    "#     f.write(df_cleaned_str)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Orienting the map to the home plate location ####\n",
    "\n",
    "### Find the center of the field\n",
    "def calculate_centroid(coords):\n",
    "    x_coords = [coord[0] for coord in coords]\n",
    "    y_coords = [coord[1] for coord in coords]\n",
    "    centroid_x = sum(x_coords) / len(coords)\n",
    "    centroid_y = sum(y_coords) / len(coords)\n",
    "    return (centroid_x, centroid_y)\n",
    "\n",
    "\n",
    "## Find the bearing between the home plate and the center of the field\n",
    "import math\n",
    "\n",
    "def calculate_bearing(point1, point2):\n",
    "    lat1, lon1 = math.radians(point1[1]), math.radians(point1[0])\n",
    "    lat2, lon2 = math.radians(point2[1]), math.radians(point2[0])\n",
    "\n",
    "    d_lon = lon2 - lon1\n",
    "\n",
    "    x = math.cos(lat2) * math.sin(d_lon)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(d_lon)\n",
    "\n",
    "    bearing = math.degrees(math.atan2(x, y))\n",
    "    bearing = (bearing + 360) % 360  # Normalize the bearing to the range [0, 360)\n",
    "\n",
    "    return bearing\n",
    "\n",
    "### Function to classify direction in laymans terms North, South, East, West, ect\n",
    "def degrees_to_cardinal_direction(degrees):\n",
    "    directions = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW', 'N']\n",
    "    index = round(degrees / 45)\n",
    "    return directions[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the centroid of the outfield fence coordinates for each row\n",
    "df_cleaned['fop_centroid'] = df_cleaned['fop'].apply(lambda coords: calculate_centroid(coords[1:]))\n",
    "\n",
    "# Calculate the bearing between home plate and the centroid for each row\n",
    "df_cleaned['field_orientation'] = df_cleaned.apply(lambda row: calculate_bearing(row['home_plate'], row['fop_centroid']), axis=1)\n",
    "\n",
    "# Convert the bearing to a cardinal direction\n",
    "df_cleaned['field_cardinal_direction'] = df_cleaned['field_orientation'].apply(degrees_to_cardinal_direction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rename the Cleaned Dataframe back to the default name\n",
    "# df = df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### In the fop, foul and home_plate columns, the coordinates are in the format (longitude, latitude).\n",
    "# ### This is the opposite of the format that is used in Google Maps, so we need to reverse the order of the coordinates.\n",
    "# df['fop'] = df['fop'].apply(lambda coords: [(coord[1], coord[0]) for coord in coords])\n",
    "# df['foul'] = df['foul'].apply(lambda coords: [(coord[1], coord[0]) for coord in coords])\n",
    "# df['home_plate'] = df['home_plate'].apply(lambda coord: (coord[1], coord[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get Geolocation of each field based on home plate coordinates and return state and country\n",
    "### This block takes a long time to run - will need to revisit\n",
    "## up to ten minutes\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "from tqdm import tqdm\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"baseball_field_locator\")\n",
    "\n",
    "# Function to get location information\n",
    "def get_location_info(lng, lat):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lng), timeout=10)\n",
    "        state = location.raw['address'].get('state', None)\n",
    "        country = location.raw['address'].get('country', None)\n",
    "        return state, country\n",
    "    except GeocoderTimedOut:\n",
    "        print(f\"GeocoderTimedOut error for coordinates: ({lng}, {lat})\")\n",
    "        return None, None\n",
    "    except GeocoderServiceError:\n",
    "        print(f\"GeocoderServiceError for coordinates: ({lng}, {lat})\")\n",
    "        return None, None\n",
    "\n",
    "# Extract the first coordinate for each field\n",
    "df_cleaned['lng'], df_cleaned['lat'] = zip(*df_cleaned['home_plate'].apply(lambda x: x))\n",
    "\n",
    "# Wrap the DataFrame apply function with tqdm for progress indication\n",
    "tqdm.pandas(desc=\"Processing coordinates\")\n",
    "\n",
    "# Get state and country information for each field\n",
    "df_cleaned[['state', 'country']] = df_cleaned.progress_apply(lambda row: get_location_info(row['lng'], row['lat']), axis=1, result_type='expand')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the Cleaned Dataframe back to the default name\n",
    "df = df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a dataframe of just high school fields in Michigan\n",
    "hs_df = df[df['level'] == 'High School']\n",
    "\n",
    "### Create a dataframe of all other fields\n",
    "other_df = df[df['level'] != 'High School']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "def find_best_match(school_name, choices, score_cutoff=80):\n",
    "    best_match = process.extractOne(school_name, choices, scorer=fuzz.token_sort_ratio, score_cutoff=score_cutoff)\n",
    "    if best_match:\n",
    "        return best_match[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Read CSV files\n",
    "mhsaa_df = pd.read_csv('data/school_info/mhsaa_enrolment_2022.csv')\n",
    "name_color_df = pd.read_csv('data\\school_info\\mhsaa_school_nickname_color_2020.csv')\n",
    "\n",
    "# Get the list of park names from hs_df\n",
    "park_names = hs_df['park_name'].tolist()\n",
    "\n",
    "# Apply find_best_match function to create a new column 'best_match' in mhsaa_df\n",
    "mhsaa_df['best_match'] = mhsaa_df['school_name'].apply(find_best_match, choices=park_names, score_cutoff=90)\n",
    "\n",
    "## Pull the school_id, school_name, students, and division columns from mhsaa_df and add to hs_df\n",
    "columns_to_extract = ['school_id', 'school_name', 'students', 'division']\n",
    "for col in columns_to_extract:\n",
    "    hs_df[col] = hs_df['park_name'].apply(lambda x: mhsaa_df.loc[mhsaa_df['best_match'] == x, col].iloc[0] if not mhsaa_df.loc[mhsaa_df['best_match'] == x, col].empty else None)\n",
    "\n",
    "# Apply find_best_match function to create a new column 'best_match' in name_color_df\n",
    "name_color_df['best_match'] = name_color_df['School'].apply(find_best_match, choices=park_names, score_cutoff=80)\n",
    "\n",
    "## Pull the data from the name_color_df and add to hs_df (Nickname,Color1,Color2,Color3,Color4)\n",
    "columns_to_extract = ['Nickname', 'Color1', 'Color2', 'Color3', 'Color4']\n",
    "for col in columns_to_extract:\n",
    "    hs_df[col] = hs_df['park_name'].apply(lambda x: name_color_df.loc[name_color_df['best_match'] == x, col].iloc[0] if not name_color_df.loc[name_color_df['best_match'] == x, col].empty else None)\n",
    "\n",
    "# Drop the 'best_match' columns\n",
    "# hs_df.drop(columns=['best_match'], inplace=True)\n",
    "\n",
    "## Take a look at the new hs_df\n",
    "hs_df.head()\n",
    "\n",
    "\n",
    "\n",
    "# # Lookup the mhsaa_df['best_match'] in hs_df and return the columns: 'school_id', 'school_name', 'students', 'division'\n",
    "# columns_to_extract = ['school_id', 'school_name', 'students', 'division']\n",
    "# for col in columns_to_extract:\n",
    "#     mhsaa_df[col] = mhsaa_df['best_match'].apply(lambda x: hs_df.loc[hs_df['park_name'] == x, col].iloc[0] if not hs_df.loc[hs_df['park_name'] == x, col].empty else None)\n",
    "\n",
    "# # Apply find_best_match function to create a new column 'best_match' in name_color_df\n",
    "# name_color_df['best_match'] = name_color_df['School'].apply(find_best_match, choices=park_names, score_cutoff=80)\n",
    "\n",
    "# # Merge hs_df with mhsaa_df and name_color_df on the 'park_name' and 'best_match' columns\n",
    "# hs_df = hs_df.merge(mhsaa_df, left_on='park_name', right_on='best_match', how='left', suffixes=('', '_from_mhsaa'))\n",
    "# hs_df = hs_df.merge(name_color_df, left_on='park_name', right_on='best_match', how='left', suffixes=('', '_from_name_color'))\n",
    "\n",
    "# # Drop the 'best_match' columns\n",
    "# hs_df.drop(columns=['best_match_x', 'best_match_y'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### examin the output\n",
    "\n",
    "hs_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ORIGINAL MATCHING CODE ####\n",
    "\n",
    "# ########## TURNED OFF FOR NOW ###########\n",
    "# #****** Don't need school info. want to test new fields\n",
    "\n",
    "# #### MATCHING HIGH SCHOOL NAMES TO THE MHSAA DATA #####\n",
    "# import pandas as pd\n",
    "# from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# # Read the enrollment table from MHSAA website - 2022 enrollment\n",
    "# mhsaa_df = pd.read_csv('data/school_info/mhsaa_enrolment_2022.csv')\n",
    "\n",
    "# # Select just the high school level\n",
    "# hs_df = df[df['level'] == 'high_school']\n",
    "# other_df = df[df['level'] != 'high_school']\n",
    "\n",
    "# # Set the threshold for the fuzzy match\n",
    "# threshold = 90\n",
    "\n",
    "# # Define a function to get the best fuzzy match with the threshold\n",
    "# def get_best_match(field_name, school_names, threshold):\n",
    "#     best_match = process.extractOne(field_name, school_names, scorer=fuzz.token_set_ratio)\n",
    "#     if best_match[1] >= threshold:\n",
    "#         return best_match[0]\n",
    "#     return None\n",
    "\n",
    "# # Apply the function to the 'field' column and store the result in a new 'match' column\n",
    "# hs_df['match'] = hs_df['park_name'].apply(lambda x: get_best_match(x, mhsaa_df['school_name'], threshold))\n",
    "\n",
    "# # #### This destroys a bunch of the data, at least every high school outside of michigan\n",
    "# # # Drop rows with no match\n",
    "# # # hs_df = hs_df.dropna(subset=['match'])\n",
    "\n",
    "# # Lookup the hs_df['match'] in the mhsaa_df and return the columns: 'school_id', 'school_name', 'students', 'division'\n",
    "# columns_to_extract = ['school_id', 'school_name', 'students', 'division']\n",
    "# for col in columns_to_extract:\n",
    "#     hs_df[col] = hs_df['match'].apply(lambda x: mhsaa_df.loc[mhsaa_df['school_name'] == x, col].iloc[0] if not mhsaa_df.loc[mhsaa_df['school_name'] == x, col].empty else None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge the two dataframes back to a single one (using append)\n",
    "\n",
    "# Merge the hs_df and other_df back together\n",
    "merged_df = pd.concat([hs_df, other_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = df\n",
    "merged_df.head(10)\n",
    "\n",
    "# # Save the merged_df DataFrame as a JSON file\n",
    "merged_df.to_json('data/default_updated_output.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = merged_df.columns\n",
    "print(column_list)\n",
    "\n",
    "# ## Print a list of ten high school field names to test mascot lookup\n",
    "# print(merged_df[merged_df['level'] == 'high_school']['park_name'].head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Code to create report on the json file structure to be used as a reference later\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# from collections import defaultdict\n",
    "# from builtins import list, dict\n",
    "\n",
    "# # Load the JSON data from the file\n",
    "# with open('data/updated_output_data.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# # Analyze the JSON data\n",
    "# def analyze_structure(data, prefix=''):\n",
    "#     structure = defaultdict(set)\n",
    "    \n",
    "#     if isinstance(data, dict):\n",
    "#         for key, value in data.items():\n",
    "#             new_prefix = f'{prefix}.{key}' if prefix else key\n",
    "#             structure[new_prefix].add(type(value))\n",
    "#             structure.update(analyze_structure(value, new_prefix))\n",
    "#     elif isinstance(data, list):\n",
    "#         for item in data:\n",
    "#             structure.update(analyze_structure(item, prefix))\n",
    "    \n",
    "#     return structure\n",
    "\n",
    "# # Generate the report\n",
    "# structure = analyze_structure(data)\n",
    "# descriptions = {\n",
    "#     'field_name': 'The name of the baseball field.',\n",
    "#     'foul': 'A list of coordinates representing the foul territory of the field. (lat, lon)',\n",
    "#     'fop': 'A list of coordinates representing the fair territory of the field. (lat, lon)',\n",
    "#     'level': 'The level of the field, e.g., high_school, college, etc.',\n",
    "#     'home_plate': 'A list of coordinates representing the home plate location on the field. (lat, lon)',\n",
    "#     'foul_area_sqft': 'The total area of the foul territory in square feet.',\n",
    "#     'fop_area_sqft': 'The total area of the fair territory in square feet.',\n",
    "#     'distances': 'A list of distances from home plate to the outfield fence at the vertices of the wall.',\n",
    "#     'max_distance': 'The maximum distance from home plate to the outfield fence.',\n",
    "#     'min_distance': 'The minimum distance from home plate to the outfield fence.',\n",
    "#     'avg_distance': 'The average distance from home plate to the outfield fence.',\n",
    "#     'fop_centroid': 'A list of coordinates representing the centroid of the fair territory.',\n",
    "#     'field_orientation': \"The angle (in degrees) of the field's orientation, with 0 degrees being North.\",\n",
    "#     'field_cardinal_direction': \"The cardinal direction abbreviation (N, S, E, W, NE, NW, SE, SW) representing the field's orientation.\",\n",
    "#     'match': 'The matched school name found using fuzzy matching.',\n",
    "#     'school_id': 'The unique identifier of the matched school.',\n",
    "#     'school_name': 'The name of the matched school.',\n",
    "#     'students': 'The number of students enrolled in the matched school.',\n",
    "#     'division': 'The athletic division the matched school belongs to.'\n",
    "# }\n",
    "\n",
    "# # Replace <filename> with your desired filename without the extension\n",
    "# filename = \"output_data\"\n",
    "\n",
    "# def get_sample_value(data, key):\n",
    "#     for item in data:\n",
    "#         if key in item and item[key] is not None:\n",
    "#             return item[key]\n",
    "#     return None\n",
    "\n",
    "# # Generate the report with sample values\n",
    "# report = pd.DataFrame([(key, ', '.join([t.__name__ for t in types]), descriptions.get(key, ''), get_sample_value(data, key)) \n",
    "#                        for key, types in structure.items()],\n",
    "#                       columns=['Key', 'Data Types', 'Description', 'Sample Value'])\n",
    "\n",
    "# # Save the report as a CSV file\n",
    "# report.to_csv(f\"{filename}_report.csv\", index=False)\n",
    "\n",
    "# print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Code to create report on some interesting stats like total fields, total area, ect\n",
    "\n",
    "# ## Create a function to measure the total distance of the outside of each polygon\n",
    "# from shapely.geometry import Polygon\n",
    "# import pyproj\n",
    "\n",
    "# def calculate_perimeter(coords):\n",
    "\n",
    "#     perimeter = 0\n",
    "\n",
    "#     for i in range(len(coords)):\n",
    "#         x1, y1 = coords[i]\n",
    "#         x2, y2 = coords[(i + 1) % len(coords)]\n",
    "#         perimeter += math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "#     return perimeter\n",
    "\n",
    "# # Get a sum of all of the perimeters\n",
    "# total_perimeter_fop = df['fop'].apply(calculate_perimeter).sum()\n",
    "# total_perimeter_foul = df['foul'].apply(calculate_perimeter).sum()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/default_updated_output.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to process the data, counting the orientations and filtering by level.\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_data(data, level_filter=None):\n",
    "    count_by_orientation = defaultdict(int)\n",
    "    \n",
    "    for record in data:\n",
    "        if level_filter is None or record['level'] == level_filter:\n",
    "            orientation = round(record['field_orientation'])\n",
    "            count_by_orientation[orientation] += 1\n",
    "\n",
    "    return count_by_orientation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polar_chart(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 2 * np.pi / num_bins\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "\n",
    "    ax.set_facecolor('#808080')\n",
    "    ###\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "    \n",
    "    # # Set dark background\n",
    "    # ax.set_facecolor('#2b2b2b')\n",
    "    plt.gca().set_rlabel_position(22.5)\n",
    "    ax.set_ylim(-10, 100)  # Adjust based on max count\n",
    "\n",
    "    bars = ax.bar(bin_edges[:-1], bin_counts, width=bin_width, bottom=-20)\n",
    "    \n",
    "\n",
    "\n",
    "    # Use custom colors and opacity\n",
    "    for r, bar in zip(bin_counts, bars):\n",
    "        bar.set_facecolor(plt.cm.viridis(r / max(bin_counts)))\n",
    "        # bar.set_facecolor(plt.cm.plasma(r / max(bin_counts)))\n",
    "        bar.set_alpha(0.8)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UPDATED GPT CODE - LATE NIGHT FRIDAY\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_polar_chart(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 2 * np.pi / num_bins\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "\n",
    "    ax.set_facecolor('#808080')\n",
    "    ###\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "    \n",
    "    # # Set dark background\n",
    "    ax.set_facecolor('#2b2b2b')\n",
    "    plt.gca().set_rlabel_position(22.5)\n",
    "    ax.set_ylim(-20, 130)  # Adjust based on max count\n",
    "\n",
    "    # Add bars for negative values\n",
    "    zero_height_bars = ax.bar(bin_edges[:-1], np.abs(ax.get_ylim()[0]), width=bin_width, bottom=0.0, color='k', alpha=0.3)\n",
    "\n",
    "    bars = ax.bar(bin_edges[:-1], bin_counts, width=bin_width, bottom=0)\n",
    "    \n",
    "    # Use custom colors and opacity\n",
    "    for r, bar in zip(bin_counts, bars):\n",
    "        bar.set_facecolor(plt.cm.viridis(r / max(bin_counts)))\n",
    "        # bar.set_facecolor(plt.cm.plasma(r / max(bin_counts)))\n",
    "        bar.set_alpha(0.8)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_polar_chart(data, num_bins=50, level_filter=None)\n",
    "\n",
    "\n",
    "#### GOAL\n",
    "## fill the center portion of the plot to create a heat map of the field orientations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_polar_chart(data, num_bins=60, level_filter=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### HEATMAP CODE\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def create_heatmap(data, num_bins=36, level_filter=None):\n",
    "#     count_by_orientation = process_data(data, level_filter)\n",
    "\n",
    "#     # Compute the histogram\n",
    "#     bin_edges = np.linspace(0.0, 2 * np.pi, num_bins + 1)\n",
    "#     bin_counts = np.zeros(num_bins)\n",
    "\n",
    "#     for orientation, count in count_by_orientation.items():\n",
    "#         idx = int(orientation / (360 / num_bins))\n",
    "#         if idx == num_bins:\n",
    "#             idx = 0\n",
    "#         bin_counts[idx] += count\n",
    "\n",
    "#     # Reshape histogram data into a 2D array\n",
    "#     heatmap_data = np.tile(bin_counts, (num_bins, 1))\n",
    "\n",
    "#     # Set plot size\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "\n",
    "#     # Create heatmap\n",
    "#     plt.imshow(heatmap_data, cmap='viridis', aspect='auto', interpolation='nearest', origin='lower')\n",
    "#     plt.colorbar(label='Counts')\n",
    "\n",
    "#     # Set x-axis ticks and labels\n",
    "#     plt.xticks(np.arange(0, num_bins, num_bins // 6), np.arange(0, 361, 60))\n",
    "#     plt.xlabel('Orientation (degrees)')\n",
    "\n",
    "#     # Set y-axis ticks and labels (assuming equal radial divisions)\n",
    "#     max_radius_label = 'Max Radius'\n",
    "#     plt.yticks(np.arange(0, num_bins, num_bins // 6), [0, 1, 2, 3, 4, max_radius_label])\n",
    "#     plt.ylabel('Radial Division')\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# # Example usage:\n",
    "# # create_heatmap(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# create_heatmap(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot a histogram of the field orientations for all levels\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_histogram(data, num_bins=36, level_filter=None):\n",
    "    count_by_orientation = process_data(data, level_filter)\n",
    "    \n",
    "    # Compute the histogram\n",
    "    bin_edges = np.linspace(0.0, 360.0, num_bins + 1)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for orientation, count in count_by_orientation.items():\n",
    "        idx = int(orientation / (360 / num_bins))\n",
    "        if idx == num_bins:\n",
    "            idx = 0\n",
    "        bin_counts[idx] += count\n",
    "    \n",
    "    bin_width = 360 / num_bins\n",
    "\n",
    "    # Plot the histogram\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(bin_edges[:-1], bin_counts, width=bin_width, edgecolor='black')\n",
    "    ax.set_xlabel('Field Orientation (degrees)')\n",
    "    ax.set_ylabel('Number of Fields')\n",
    "    ax.set_title('Field Orientation Histogram')\n",
    "\n",
    "    # Set the major tick locations\n",
    "    major_tick_locations = [45, 135, 225, 315]\n",
    "    plt.xticks(major_tick_locations, major_tick_locations)\n",
    "\n",
    "\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_histogram(data, num_bins=36, level_filter=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END WORK BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['field_cardinal_direction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Rename the dataframe back to the default name\n",
    "df = merged_df\n",
    "\n",
    "# Set the plot size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.scatter(df_cleaned['min_distance'], df_cleaned['max_distance'], alpha=0.8)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Min Distance (feet)')\n",
    "plt.ylabel('Max Distance (feet)')\n",
    "plt.title('Scatter Plot of Min and Max Distances to Outfield Fence')\n",
    "\n",
    "# Display the plot in the Jupyter Notebook\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CODE TO MAKE A LIST OF OUTLIERS #####\n",
    "\n",
    "# Filter the DataFrame for fields with min distances below 100 feet\n",
    "outliers = df[df['min_distance'] < 100]\n",
    "\n",
    "# Display the outlier fields in the Jupyter Notebook\n",
    "print(outliers[['park_name', 'min_distance', 'max_distance']])\n",
    "\n",
    "# Save the outlier fields to a CSV file\n",
    "outliers.to_csv('outlier_fields.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
