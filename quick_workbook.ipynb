{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the district html\n",
    "\n",
    "#### Goal: get dataframe that includes district games and times"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## each region runs 1-10 with 8-10 being the state semi final and final\n",
    "\n",
    "## Base url\n",
    "\n",
    "'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/'\n",
    "\n",
    "then groupNumber\n",
    "\n",
    "'/SportSeasonId/424201/Classification/'\n",
    "\n",
    "divsionNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1-1': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/1/SportSeasonId/424201/Classification/1', '1-2': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/1/SportSeasonId/424201/Classification/2', '1-3': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/1/SportSeasonId/424201/Classification/3', '1-4': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/1/SportSeasonId/424201/Classification/4', '2-1': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/2/SportSeasonId/424201/Classification/1', '2-2': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/2/SportSeasonId/424201/Classification/2', '2-3': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/2/SportSeasonId/424201/Classification/3', '2-4': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/2/SportSeasonId/424201/Classification/4', '3-1': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/3/SportSeasonId/424201/Classification/1', '3-2': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/3/SportSeasonId/424201/Classification/2', '3-3': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/3/SportSeasonId/424201/Classification/3', '3-4': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/3/SportSeasonId/424201/Classification/4', '4-1': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/4/SportSeasonId/424201/Classification/1', '4-2': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/4/SportSeasonId/424201/Classification/2', '4-3': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/4/SportSeasonId/424201/Classification/3', '4-4': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/4/SportSeasonId/424201/Classification/4', '5-1': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/5/SportSeasonId/424201/Classification/1', '5-2': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/5/SportSeasonId/424201/Classification/2', '5-3': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/5/SportSeasonId/424201/Classification/3', '5-4': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/5/SportSeasonId/424201/Classification/4', '6-1': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/6/SportSeasonId/424201/Classification/1', '6-2': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/6/SportSeasonId/424201/Classification/2', '6-3': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/6/SportSeasonId/424201/Classification/3', '6-4': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/6/SportSeasonId/424201/Classification/4', '7-1': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/7/SportSeasonId/424201/Classification/1', '7-2': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/7/SportSeasonId/424201/Classification/2', '7-3': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/7/SportSeasonId/424201/Classification/3', '7-4': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/7/SportSeasonId/424201/Classification/4', '8-1': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/8/SportSeasonId/424201/Classification/1', '8-2': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/8/SportSeasonId/424201/Classification/2', '8-3': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/8/SportSeasonId/424201/Classification/3', '8-4': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/8/SportSeasonId/424201/Classification/4'}\n"
     ]
    }
   ],
   "source": [
    "## Create List of urls to parse\n",
    "\n",
    "base = 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/'\n",
    "base2 = '/SportSeasonId/424201/Classification/'\n",
    "\n",
    "bracket = []\n",
    "division = []\n",
    "\n",
    "## Create a list of urls to parse\n",
    "for i in range(1, 9):\n",
    "    for j in range(1, 5):\n",
    "        bracket.append(base + str(i) + base2 + str(j))\n",
    "        division.append(str(i) + '-' + str(j))\n",
    "\n",
    "## Create a dictionary of urls and divisions\n",
    "bracket_dict = dict(zip(division, bracket))\n",
    "\n",
    "print(bracket_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REFACTOR ALL IN ONE FUNCTION\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='scraping.log', level=logging.INFO)\n",
    "\n",
    "def scrape_bracket(url):\n",
    "    # Request the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all divs containing game info\n",
    "    games = soup.find_all('div', class_='contestboxhover')\n",
    "\n",
    "    # Initialize an empty list to store game data\n",
    "    data = []\n",
    "\n",
    "    # Iterate over each game\n",
    "    for i, game in enumerate(games):\n",
    "        try:\n",
    "            round_district = game.find('div', class_='contesttitle').text\n",
    "            date = game.find('span', class_='contestdate').text\n",
    "            time = game.find('span', class_='contesttime').text\n",
    "            location = game.find('span', class_='contestlocation').text\n",
    "\n",
    "            team1_record = game.find('div', class_='line1hov').text.rsplit(' ', 1)\n",
    "            team1, record1 = team1_record if len(team1_record) == 2 else (team1_record[0], None)\n",
    "\n",
    "            team2_record = game.find('div', class_='line2hov').text.rsplit(' ', 1)\n",
    "            team2, record2 = team2_record if len(team2_record) == 2 else (team2_record[0], None)\n",
    "\n",
    "            more_info_link = game.find('a', class_='moreinfo')['href']\n",
    "\n",
    "            data.append([round_district, date, time, location, team1, record1, team2, record2, more_info_link])\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f'Error processing game #{i}: {e}')\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Round', 'Date', 'Time', 'Location', 'Team1', 'Record1', 'Team2', 'Record2', 'MoreInfo'])\n",
    "\n",
    "    df = df.replace('\\n', ' ', regex=True)\n",
    "    df = df.replace(' +', ' ', regex=True)\n",
    "    df = df.replace('\\r', ' ', regex=True)\n",
    "    df = df.replace(' +', ' ', regex=True)\n",
    "\n",
    "    df['Location'] = df['Location'].str.replace(' - MAP', '')\n",
    "\n",
    "    df['District'] = df['Round'].str[-3:]\n",
    "    # Task the first 8 characters of the Round column\n",
    "    df['Round'] = df['Round'].str[:8]\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "# Usage example:\n",
    "# df = scrape_bracket('https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/classification/1/sportseasonid/424201/bracketgroup/1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Division'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Division'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Justin\\Desktop\\Project\\BB_parks\\quick_workbook.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y160sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Iterate over the dictionary\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y160sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m division, url \u001b[39min\u001b[39;00m bracket_dict\u001b[39m.\u001b[39mitems():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y160sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Call the function and store the DataFrame in the list\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y160sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     df \u001b[39m=\u001b[39m scrape_bracket(url)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y160sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Add a new column 'Division' with the value from the dictionary\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y160sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mDivision\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m division\n",
      "\u001b[1;32mc:\\Users\\Justin\\Desktop\\Project\\BB_parks\\quick_workbook.ipynb Cell 5\u001b[0m in \u001b[0;36mscrape_bracket\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y160sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Task the first 8 characters of the Round column\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y160sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mRound\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mRound\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr[:\u001b[39m8\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y160sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mDivision\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mDivision\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mstr[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y160sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Division'"
     ]
    }
   ],
   "source": [
    "dfs = []  # Initialize an empty list to store DataFrames\n",
    "\n",
    "# Iterate over the dictionary\n",
    "for division, url in bracket_dict.items():\n",
    "    # Call the function and store the DataFrame in the list\n",
    "    df = scrape_bracket(url)\n",
    "    \n",
    "    # Add a new column 'Division' with the last character of the division\n",
    "    df['Division'] = division[-1]\n",
    "    \n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames into a single one\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.info()\n",
    "final_df.describe()\n",
    "# final_df.head()\n",
    "# final_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE TO CSV\n",
    "\n",
    "final_df.to_csv('TEMP/mhsaa_brackets.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='scraping.log', level=logging.INFO)\n",
    "\n",
    "url = 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/classification/1/sportseasonid/424201/bracketgroup/1'\n",
    "\n",
    "# Request the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all divs containing game info\n",
    "games = soup.find_all('div', class_='contestboxhover')\n",
    "\n",
    "# Initialize an empty list to store game data\n",
    "data = []\n",
    "\n",
    "# Iterate over each game\n",
    "for i, game in enumerate(games):\n",
    "    try:\n",
    "        round_district = game.find('div', class_='contesttitle').text\n",
    "        date = game.find('span', class_='contestdate').text\n",
    "        time = game.find('span', class_='contesttime').text\n",
    "        location = game.find('span', class_='contestlocation').text\n",
    "        \n",
    "        # Split on the last space in the string\n",
    "        team1_record = game.find('div', class_='line1hov').text.rsplit(' ', 1)\n",
    "        team1, record1 = team1_record if len(team1_record) == 2 else (team1_record[0], None)\n",
    "        \n",
    "        team2_record = game.find('div', class_='line2hov').text.rsplit(' ', 1)\n",
    "        team2, record2 = team2_record if len(team2_record) == 2 else (team2_record[0], None)\n",
    "        \n",
    "        more_info_link = game.find('a', class_='moreinfo')['href']\n",
    "        \n",
    "        # Add the game info to our data list\n",
    "        data.append([round_district, date, time, location, team1, record1, team2, record2, more_info_link])\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f'Error processing game #{i}: {e}')\n",
    "        continue\n",
    "\n",
    "# Convert the data list to a pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=['Round', 'Date', 'Time', 'Location', 'Team1', 'Record1', 'Team2', 'Record2', 'MoreInfo'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "\n",
    "## Clean up the data\n",
    "\n",
    "# Get ride of newlines and extra spaces'\n",
    "df = df.replace('\\n', ' ', regex=True)\n",
    "df = df.replace(' +', ' ', regex=True)\n",
    "df = df.replace('\\r', ' ', regex=True)\n",
    "## Get rid of extra spaces\n",
    "df = df.replace(' +', ' ', regex=True)\n",
    "\n",
    "# Get rid of ' - MAP' in the location column\n",
    "df['Location'] = df['Location'].str.replace(' - MAP', '')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the district number from the Round column\n",
    "## get the last three characters of the string\n",
    "df['District'] = df['Round'].str[-3:]\n",
    "## Get rid of the last three characters of the string\n",
    "df['Round'] = df['Round'].str[:-15]\n",
    "\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Of Work Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('data/fields_cleaned.csv')\n",
    "\n",
    "## Read json into a dataframe\n",
    "df = pd.read_json('data/default_updated_output.json')\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_fields(data):\n",
    "    # Define weights for each parameter\n",
    "    weights = {\n",
    "        'max_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'min_distance': 1,  # positive weight since shorter fences favor hitters\n",
    "        'avg_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'median_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'field_area_sqft': -1,  # negative weight since larger fields favor pitchers\n",
    "        'fair_to_foul': -1,  # negative weight since larger ratio (more foul territory) favors pitchers\n",
    "    }\n",
    "\n",
    "    # Standardize features (subtract mean and divide by standard deviation)\n",
    "    standardized_data = data.copy()\n",
    "    for column in weights.keys():\n",
    "        standardized_data[column] = (standardized_data[column] - standardized_data[column].mean()) / standardized_data[column].std()\n",
    "\n",
    "    # Calculate score for each field\n",
    "    standardized_data['score'] = standardized_data.apply(lambda row: sum(row[param] * weight for param, weight in weights.items()), axis=1)\n",
    "\n",
    "    # Rank fields based on score (higher scores are more hitter-friendly)\n",
    "    ranked_fields = standardized_data.sort_values('score', ascending=False)\n",
    "\n",
    "    return ranked_fields\n",
    "\n",
    "# Suppose 'df' is your DataFrame containing the field data\n",
    "ranked_fields = rank_fields(df)\n",
    "print(ranked_fields[['park_name', 'score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ranked_fields.to_csv('data/ALL_ranked_fields.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import googlemaps\n",
    "import pandas as pd\n",
    "\n",
    "gmaps = googlemaps.Client(key='AIzaSyA_BhlTupRdBPBhRptQuR6pYorMVYQnRMA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USING KEYWORDS\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_place_names(coord, client):\n",
    "    \"\"\"\n",
    "    Get place names using Google Places API.\n",
    "    \n",
    "    Parameters:\n",
    "    coord: str - Coordinates in the format (lng, lat).\n",
    "    client: googlemaps.Client - Google Maps client.\n",
    "    \n",
    "    Returns:\n",
    "    list: Three closest place names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse coordinates\n",
    "        lng, lat = map(float, coord.strip('()').split(', '))\n",
    "        # Swap longitude and latitude for Google API\n",
    "        places_result_1 = client.places_nearby(location=(lat, lng), radius=1000, keyword='baseball field')\n",
    "        places_result_2 = client.places_nearby(location=(lat, lng), radius=1000, keyword='school')\n",
    "        places_result_3 = client.places_nearby(location=(lat, lng), radius=1000, type='park')\n",
    "        places_result = places_result_1['results'] + places_result_2['results'] + places_result_3['results']\n",
    "        place_names = [place['name'] for place in places_result[:5]]\n",
    "        return place_names\n",
    "    except:\n",
    "        # Return NaN if there is an error\n",
    "        return [np.nan]*5\n",
    "\n",
    "\n",
    "# Use tqdm to create progress bar\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply the function to each coordinate in 'home_plate'\n",
    "df['place_names'] = df['home_plate'].progress_apply(lambda coord: get_place_names(coord, gmaps))\n",
    "\n",
    "# Split 'place_names' into five separate columns\n",
    "df[['Place 1', 'Place 2', 'Place 3', 'Place 4', 'Place 5']] = pd.DataFrame(df['place_names'].to_list(), index=df.index)\n",
    "\n",
    "# Create new dataframe 'place_test_df' with the necessary columns\n",
    "place_test_df = df[['park_name', 'Place 1', 'Place 2', 'Place 3', 'Place 4', 'Place 5']]\n",
    "place_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output Csv for manual cleaning\n",
    "\n",
    "place_test_df.to_csv('data/mhsaa_places.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## FIRST TRY - working, returned OK results\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def get_place_names(coord, client):\n",
    "#     \"\"\"\n",
    "#     Get place names using Google Places API.\n",
    "    \n",
    "#     Parameters:\n",
    "#     coord: str - Coordinates in the format (lng, lat).\n",
    "#     client: googlemaps.Client - Google Maps client.\n",
    "    \n",
    "#     Returns:\n",
    "#     list: Three closest place names.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Parse coordinates\n",
    "#         lng, lat = map(float, coord.strip('()').split(', '))\n",
    "#         # Swap longitude and latitude for Google API\n",
    "#         places_result = client.places_nearby(location=(lat, lng), radius=100)\n",
    "#         place_names = [place['name'] for place in places_result['results'][:3]]\n",
    "#         return place_names\n",
    "#     except:\n",
    "#         # Return NaN if there is an error\n",
    "#         return [np.nan, np.nan, np.nan]\n",
    "\n",
    "# # Use tqdm to create progress bar\n",
    "# tqdm.pandas()\n",
    "\n",
    "# # Apply the function to each coordinate in 'home_plate'\n",
    "# df['place_names'] = df['home_plate'].progress_apply(lambda coord: get_place_names(coord, gmaps))\n",
    "\n",
    "# # Split 'place_names' into three separate columns\n",
    "# df[['Place 1', 'Place 2', 'Place 3']] = pd.DataFrame(df['place_names'].to_list(), index=df.index)\n",
    "\n",
    "# # Create new dataframe 'place_test_df' with the necessary columns\n",
    "# place_test_df = df[['park_name', 'Place 1', 'Place 2', 'Place 3']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_test_df.info()\n",
    "place_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the dataframe to a csv file\n",
    "\n",
    "place_test_df.to_csv('data/place_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW ALL IN ONE DISTRICT REGIONAL COMBO BLOCK\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# read the district host table\n",
    "d_path = 'data/2023_district_hosts.csv'\n",
    "\n",
    "# read regional table\n",
    "r_path = 'data/2023_regional_hosts.csv'\n",
    "\n",
    "# read the district host table\n",
    "d_host = pd.read_csv(d_path)\n",
    "\n",
    "# read regional table\n",
    "r_host = pd.read_csv(r_path)\n",
    "\n",
    "# Display summary of the district host table\n",
    "print(d_host.info())\n",
    "\n",
    "# Display summary of the regional host table\n",
    "print(r_host.info())\n",
    "\n",
    "# Lowercase the 'team' and 'park_name' columns to improve the match rate\n",
    "d_host['team'] = d_host['team'].str.lower()\n",
    "r_host['park_name'] = r_host['park_name'].str.lower()\n",
    "\n",
    "def fuzzy_merge(df_1, df_2, key1, key2, threshold=90):\n",
    "    \"\"\"\n",
    "    :param df_1: the left table to join\n",
    "    :param df_2: the right table to join\n",
    "    :param key1: key column of the left table\n",
    "    :param key2: key column of the right table\n",
    "    :param threshold: how close the matches should be to return a match, based on Levenshtein distance\n",
    "    :return: dataframe with both tables joined on fuzzy match\n",
    "    \"\"\"\n",
    "    s = df_2[key2].dropna().tolist()\n",
    "\n",
    "    m = df_1[key1].dropna().apply(lambda x: process.extractOne(x, s, scorer=fuzz.ratio))  \n",
    "    df_1['matches'] = m\n",
    "\n",
    "    m2 = df_1['matches'].apply(lambda x: x[0] if x[1] >= threshold else np.nan)\n",
    "    df_1['matches'] = m2\n",
    "\n",
    "    merged = df_1.merge(df_2.rename(columns={key2: 'matches'}), on='matches', how='outer')\n",
    "\n",
    "    # Remove the 'matches' column and return the merged dataframe\n",
    "    merged = merged.drop(columns=['matches'])\n",
    "    return merged\n",
    "\n",
    "# Execute the function\n",
    "merged_df = fuzzy_merge(d_host, r_host, 'team', 'park_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Rename some colums for clarity and drop unnecessary columns\n",
    "\n",
    "merged_df = merged_df.rename(columns={'division_x' : 'district_div', 'division_y' : 'regional_div'})\n",
    "\n",
    "merged_df = merged_df.drop(columns=['color4', 'Unnamed: 3', 'score'])\n",
    "\n",
    "merged_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save the merged dataframe to a csv file\n",
    "\n",
    "merged_df.to_csv('data/2023_all_hosts.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge the district host and the regional host into a all_host.csv table\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read the district host table\n",
    "d_path = 'data/2023_district_hosts.csv'\n",
    "\n",
    "\n",
    "# read rional table\n",
    "r_path = 'data/2023_regional_hosts.csv'\n",
    "\n",
    "## read the district host table\n",
    "d_host = pd.read_csv(d_path)\n",
    "\n",
    "## read rional table\n",
    "r_host = pd.read_csv(r_path)\n",
    "\n",
    "## Display summary og the district host table\n",
    "\n",
    "print(d_host.info())\n",
    "\n",
    "## Display summary og the regional host table\n",
    "\n",
    "print(r_host.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Lowercase the 'team' and 'park_name' columns to improve the match rate\n",
    "d_host['team'] = d_host['team'].str.lower()\n",
    "r_host['park_name'] = r_host['park_name'].str.lower()\n",
    "\n",
    "def fuzzy_merge(df_1, df_2, key1, key2, threshold=90):\n",
    "    \"\"\"\n",
    "    :param df_1: the left table to join\n",
    "    :param df_2: the right table to join\n",
    "    :param key1: key column of the left table\n",
    "    :param key2: key column of the right table\n",
    "    :param threshold: how close the matches should be to return a match, based on Levenshtein distance\n",
    "    :return: dataframe with both tables joined on fuzzy match\n",
    "    \"\"\"\n",
    "    s = df_2[key2].tolist()\n",
    "\n",
    "    m = df_1[key1].apply(lambda x: process.extract(x, s, limit=1))  \n",
    "    df_1['matches'] = m\n",
    "\n",
    "    m2 = df_1['matches'].apply(lambda x: x[0][0] if x[0][1] >= threshold else '')\n",
    "    df_1['matches'] = m2\n",
    "\n",
    "    merged = df_1.merge(df_2.rename(columns={key2: 'matches'}), on='matches', how='outer')\n",
    "\n",
    "    # Remove the 'matches' column and return the merged dataframe\n",
    "    merged = merged.drop(columns=['matches'])\n",
    "    return merged\n",
    "\n",
    "# Execute the function\n",
    "merged_df = fuzzy_merge(d_host, r_host, 'team', 'park_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output merged table to csv\n",
    "\n",
    "merged_df.to_csv('data/2023_all_hosts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a JSON with all the ditrict assignments\n",
    "## Include Tema nickname\n",
    "## Output a dictionary with district number as key, host team and a list of the rest of the teams\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Build the file path using os.path.join\n",
    "file_path = os.path.join('data', '2023_team_info.csv')\n",
    "\n",
    "\n",
    "# teaminfo_path = 'data\\2023_team_info.csv'\n",
    "\n",
    "## load into df\n",
    "\n",
    "teaminfo_df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### combine the team name and the nickname to make a new column\n",
    "\n",
    "teaminfo_df['team_name'] = teaminfo_df['team'] + ' ' + teaminfo_df['nickname']\n",
    "\n",
    "teaminfo_df.head()\n",
    "\n",
    "## Create a column with a search term string\n",
    "## This will be used to search for the team in the TBA API\n",
    "## Just want school name + 'high school' + 'michigan' \n",
    "teaminfo_df['search'] = teaminfo_df['team'] + ' high school michigan'\n",
    "\n",
    "## Create a json that is grouped by district and contains a list of all the teams names in that district\n",
    "\n",
    "## Create a list of all the districts\n",
    "\n",
    "district_list = teaminfo_df['district'].unique().tolist()\n",
    "\n",
    "## Create a dictionary with the district as the key and the value is a list of all the teams in that district with the search field\n",
    "\n",
    "district_dict = {}\n",
    "\n",
    "for district in district_list:\n",
    "    district_dict[district] = teaminfo_df[teaminfo_df['district'] == district]['team_name'].tolist()\n",
    "    district_dict[district].append(teaminfo_df[teaminfo_df['district'] == district]['search'].tolist())\n",
    "    \n",
    "\n",
    "## sort by district number\n",
    "\n",
    "district_dict = dict(sorted(district_dict.items()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to a json called district_dict.json\n",
    "\n",
    "with open('district_dict.json', 'w') as fp:\n",
    "    json.dump(district_dict, fp)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Build the file path using os.path.join\n",
    "file_path = os.path.join('data', 'html', 'mhsaa', 'data', 'tourney_2023.json')\n",
    "\n",
    "load = json.load(open(file_path))\n",
    "# Convert to df\n",
    "\n",
    "df = pd.DataFrame(load)\n",
    "df.info()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## list of column names\n",
    "\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Look at the Oxford High School data\n",
    "\n",
    "oxford = df[df['park_name'] == 'Oxford HS']\n",
    "oxford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "\n",
    "# Load HTML\n",
    "html_path = 'data/districts_2023.html'\n",
    "with open(html_path, 'r') as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Parse HTML\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Extract data\n",
    "divisions = soup.find_all('div', class_='keep-together')\n",
    "\n",
    "data = []\n",
    "for division in divisions:\n",
    "    division_number = division.find('span', {'data-bind': 'text:Division'}).text\n",
    "    tournament_name = division.find('span', {'data-bind': 'text:TournamentName'}).text\n",
    "    host = division.find('span', {'data-bind': 'text:Host'}).text\n",
    "    teams = [a.text for a in division.find_all('a')[1:]]\n",
    "\n",
    "    for team in teams:\n",
    "        data.append({\n",
    "            'team': team,\n",
    "            'division': division_number,\n",
    "            'district': int(re.sub(r'\\D+', '', tournament_name)),\n",
    "            'host': host,\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df_by_district = pd.DataFrame(data)\n",
    "\n",
    "# Load the csv that contains nickname info\n",
    "df_nickname = pd.read_csv('data/school_info/mhsaa_school_nickname_color_2020.csv')\n",
    "df_nickname.columns = df_nickname.columns.str.lower()\n",
    "\n",
    "# Fuzzy match team names\n",
    "matches = df_by_district['team'].apply(lambda x: process.extractOne(x, df_nickname['school'], scorer=fuzz.ratio))\n",
    "\n",
    "df_by_district['match_name'] = [i[0] for i in matches]\n",
    "df_by_district['score'] = [i[1] for i in matches]\n",
    "\n",
    "# Merge df_by_district and df_nickname on the common columns generated by fuzzy matching\n",
    "final_df = pd.merge(df_by_district, df_nickname, left_on='match_name', right_on='school', how='inner')\n",
    "\n",
    "# Select only the columns you're interested in\n",
    "final_df = final_df[['team', 'division', 'district', 'host', 'nickname', 'color1', 'color2', 'color3', 'color4', 'score']]\n",
    "\n",
    "# Display the final dataframe\n",
    "# print(final_df)\n",
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output dataframe to new file called 2023_team_info.csv\n",
    "\n",
    "## Team info for 2023 output file\n",
    "# drop host column\n",
    "teams_df = final_df.drop(columns=['host'])\n",
    "\n",
    "# Path: quick_workbook.ipynb\n",
    "teams_df.to_csv('data/2023_team_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with matching host and team\n",
    "matching_rows = final_df[final_df['host'] == final_df['team']]\n",
    "\n",
    "# Sort the filtered rows by district number\n",
    "sorted_rows = matching_rows.sort_values(by='district')\n",
    "\n",
    "# Reset the index of the sorted rows\n",
    "sorted_rows.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the sorted dataframe\n",
    "print(sorted_rows)\n",
    "\n",
    "# Save the sorted dataframe to a CSV file\n",
    "sorted_rows.to_csv('data/2023_district_hosts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(20)\n",
    "\n",
    "## Show the lowest scores in the dataframe\n",
    "\n",
    "final_df.sort_values(by='score').head(20)\n",
    "\n",
    "## Show the distro of scores\n",
    "\n",
    "# final_df['score'].hist()\n",
    "\n",
    "## Show numberical counts of scores in incriments of 5\n",
    "\n",
    "# final_df['score'].value_counts(bins=range(0, 101, 5))\n",
    "\n",
    "# Number of match scores under 90\n",
    "\n",
    "len(final_df[final_df['score'] < 90])\n",
    "\n",
    "# Number of match scores under 80\n",
    "\n",
    "len(final_df[final_df['score'] < 80])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Block opperation above replaces the functionality of the following blocks of beta code\n",
    "\n",
    "## Create a table with School info (Name, division, district assignment - from the district_2023 html on MHSAA site)\n",
    "\n",
    "### Then merge that into the info from the table I have with School Nickname and colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load entire district tree from local html file\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_path = 'data\\districts_2023.html'\n",
    "\n",
    "with open(html_path, 'r') as f:\n",
    "    html = f.read()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ALL IN ONE TRY ####\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "html_doc = html\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Create an empty DataFrame to store the data\n",
    "df = pd.DataFrame(columns=['Division', 'Tournament Name', 'Host', 'Location', 'Teams'])\n",
    "\n",
    "# Find all 'div' tags with class 'keep-together'\n",
    "divisions = soup.find_all('div', class_='keep-together')\n",
    "\n",
    "for division in divisions:\n",
    "    division_number = division.find('span', {'data-bind': 'text:Division'}).text\n",
    "    tournament_name = division.find('span', {'data-bind': 'text:TournamentName'}).text\n",
    "    host = division.find('span', {'data-bind': 'text:Host'}).text\n",
    "    location = division.find('a', {'data-bind': 'text: Title, attr: {href: LocationUrl}'}).text\n",
    "    \n",
    "    # Find all the team names, skipping the first 'a' tag which is the location\n",
    "    teams = [a.text for a in division.find_all('a')[1:]]\n",
    "    # # Remove the host team from the list\n",
    "    # if host in teams:\n",
    "    #     teams.remove(host)\n",
    "    \n",
    "    # Add data to the DataFrame\n",
    "    df = df.append({\n",
    "        'Division': division_number, \n",
    "        'Tournament Name': tournament_name, \n",
    "        'Host': host, \n",
    "        'Location': location, \n",
    "        'Teams': teams}, \n",
    "        ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get a DF with a row for every team\n",
    "\n",
    "df_temp = df.explode('Teams')\n",
    "\n",
    "df_temp.info()\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up the new team DF\n",
    "\n",
    "## Drop The non numberical characters from 'Tournament Name' and Rename to 'District'\n",
    "\n",
    "df_temp['District'] = df_temp['Tournament Name'].str.replace(r'\\D+', '')\n",
    "\n",
    "df_temp['District'] = df_temp['District'].astype(int)\n",
    "\n",
    "# Drop the 'Tournament Name' column\n",
    "df_temp.drop('Tournament Name', axis=1, inplace=True)\n",
    "\n",
    "## Drop Host and Location\n",
    "\n",
    "df_temp.drop(['Host', 'Location'], axis=1, inplace=True)\n",
    "\n",
    "# Rename Teams to Team\n",
    "df_temp.rename(columns={'Teams': 'Team'}, inplace=True)\n",
    "\n",
    "# Remove capitalization from column names\n",
    "df_temp.columns = df_temp.columns.str.lower()\n",
    "\n",
    "\n",
    "## Move team name to first column\n",
    "\n",
    "cols = df_temp.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_temp = df_temp[cols]\n",
    "\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rename df\n",
    "\n",
    "df_by_district = df_temp\n",
    "\n",
    "## Load the csv that contains nickname ect info\n",
    "\n",
    "df_nickname = pd.read_csv('data\\school_info\\mhsaa_school_nickname_color_2020.csv')\n",
    "\n",
    "## Remove capitalization from column names\n",
    "df_nickname.columns = df_nickname.columns.str.lower()\n",
    "\n",
    "df_nickname.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### match and merge the dataframes based on Team name and School name\n",
    "\n",
    "### Use fuzzy match to match team names\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "def match_name(name, list_names, min_score=0):\n",
    "    # -1 score incase we don't get any matches\n",
    "    max_score = -1\n",
    "    # Returning empty name for no match as well\n",
    "    max_name = \"\"\n",
    "    # Iternating over all names in the other\n",
    "    for name2 in list_names:\n",
    "        #Finding fuzzy match score\n",
    "        score = fuzz.ratio(name, name2)\n",
    "        # Checking if we are above our threshold and have a better score\n",
    "        if (score > min_score) & (score > max_score):\n",
    "            max_name = name2\n",
    "            max_score = score\n",
    "    return (max_name, max_score)\n",
    "\n",
    "# List for dicts for easy dataframe creation\n",
    "dict_list = []\n",
    "# iterating over our players without salaries found above\n",
    "for name in df_by_district.team:\n",
    "    # Use our method to find best match, we can set a threshold here\n",
    "    match = match_name(name, df_nickname.school, 75)\n",
    "    \n",
    "    # New dict for storing data\n",
    "    dict_ = {}\n",
    "    dict_.update({\"team_name\" : name})\n",
    "    dict_.update({\"match_name\" : match[0]})\n",
    "    dict_.update({\"score\" : match[1]})\n",
    "    dict_list.append(dict_)\n",
    "    \n",
    "merge_table = pd.DataFrame(dict_list)\n",
    "# Display results\n",
    "# print(merge_table)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do the table merges\n",
    "\n",
    "df_by_district = df_by_district.merge(merge_table, left_on='team', right_on='team_name', how='left')\n",
    "df_nickname = df_nickname.merge(merge_table, left_on='school', right_on='match_name', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_by_district and df_nickname on the common columns generated by fuzzy matching\n",
    "final_df = pd.merge(df_by_district, df_nickname, left_on='team', right_on='match_name', how='inner')\n",
    "\n",
    "# Select only the columns you're interested in\n",
    "final_df = final_df[['team', 'division', 'district', 'nickname', 'color1', 'color2', 'color3', 'color4', 'score']]\n",
    "\n",
    "\n",
    "# Display the final dataframe\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv to check\n",
    "\n",
    "df.to_csv('data\\district_2023_team_and_host.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End 2023 Team info creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a dictionary of all the teams seperated by division level\n",
    "\n",
    "# Create an empty dictionary to store the data\n",
    "divisions_dict = {}\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "html_doc = html\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Create an empty DataFrame to store the data\n",
    "df = pd.DataFrame(columns=['Division', 'Tournament Name', 'Host', 'Location', 'Teams'])\n",
    "\n",
    "# Find all 'div' tags with class 'keep-together'\n",
    "divisions = soup.find_all('div', class_='keep-together')\n",
    "\n",
    "for division in divisions:\n",
    "    division_number = division.find('span', {'data-bind': 'text:Division'}).text\n",
    "    tournament_name = division.find('span', {'data-bind': 'text:TournamentName'}).text\n",
    "    host = division.find('span', {'data-bind': 'text:Host'}).text\n",
    "    location = division.find('a', {'data-bind': 'text: Title, attr: {href: LocationUrl}'}).text\n",
    "    teams = [team.text for team in division.find_all('span', {'data-bind': 'highlightedText: { text: TeamName, highlight: $parents[1].Search, css: \"highlight\" }'})]\n",
    "    \n",
    "    # Add data to the DataFrame\n",
    "    df = df.append({\n",
    "        'Division': division_number, \n",
    "        'Tournament Name': tournament_name, \n",
    "        'Host': host, \n",
    "        'Location': location, \n",
    "        'Teams': teams}, \n",
    "        ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## File Paths\n",
    "# Path to mhsaa tables to merge\n",
    "\n",
    "enrollment_path = 'data\\school_info\\mhsaa_enrolment_2022.csv'\n",
    "name_color_path = 'data\\school_info\\mhsaa_school_nickname_color_2020.csv'\n",
    "\n",
    "df_enrol = pd.read_csv(enrollment_path)\n",
    "df_name_color = pd.read_csv(name_color_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5-9-23\n",
    "\n",
    "## Code to scrape 2023 MHSAA Tourny Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace this with the plain text containing the tournament information\n",
    "# read a text file into the variable text\n",
    "\n",
    "text = open('2023_districts_raw.txt', 'r').read()\n",
    "\n",
    "# Split the text into sections for each division\n",
    "sections = text.split('Division ')\n",
    "\n",
    "# Remove the first empty string\n",
    "sections.pop(0)\n",
    "\n",
    "# Initialize empty lists for each column in the dataframe\n",
    "divisions = []\n",
    "districts = []\n",
    "hosts = []\n",
    "locations = []\n",
    "\n",
    "# Loop through the sections and extract the relevant information\n",
    "for section in sections:\n",
    "    lines = section.split('\\n')\n",
    "    division = 'Division ' + lines[0]\n",
    "    for line in lines[1:]:\n",
    "        if 'Baseball District' in line:\n",
    "            district = line\n",
    "        elif 'Host:' in line:\n",
    "            host = line.split(': ')[1]\n",
    "        elif 'Location:' in line:\n",
    "            location = line.split(': ')[1]\n",
    "        elif line != '':\n",
    "            # Skip any blank lines\n",
    "            districts.append(district)\n",
    "            divisions.append(division)\n",
    "            hosts.append(host)\n",
    "            locations.append(location)\n",
    "\n",
    "# Create a dataframe to store the extracted information\n",
    "df = pd.DataFrame({'Division': divisions, 'District': districts, 'Host': hosts, 'Location': locations})\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DF came back as a ton of duplicates. I need to clean it up.\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Reindex the dataframe\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# output the dataframe to a csv file\n",
    "df.to_csv('data/2023_district_hosts.csv', index=False)\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regional Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### URLS of pages with retional data\n",
    "\n",
    "urls = {'Division 1': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/9/Classification/1/SportSeasonId/424201',\n",
    "        'Division 2': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/9/Classification/2/SportSeasonId/424201',\n",
    "        'Division 3': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/9/Classification/3/SportSeasonId/424201',\n",
    "        'Division 4': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/9/Classification/4/SportSeasonId/424201'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URLs for each division\n",
    "urls = {'Division 1': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/9/Classification/1/SportSeasonId/424201',\n",
    "        'Division 2': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/9/Classification/2/SportSeasonId/424201',\n",
    "        'Division 3': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/9/Classification/3/SportSeasonId/424201',\n",
    "        'Division 4': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/9/Classification/4/SportSeasonId/424201'}\n",
    "\n",
    "# Initialize empty lists for each column in the dataframe\n",
    "divisions = []\n",
    "locations = []\n",
    "links = []\n",
    "\n",
    "# Loop through each division URL in the dictionary\n",
    "for division, url in urls.items():\n",
    "    try:\n",
    "        # Send a GET request to the URL and parse the HTML content\n",
    "        page = requests.get(url)\n",
    "        tree = html.fromstring(page.content)\n",
    "\n",
    "        # Find all the contest location spans using XPath\n",
    "        location_spans = tree.xpath('//span[@class=\"contestlocation\"]')\n",
    "\n",
    "        # Loop through the location spans and extract the relevant information\n",
    "        for location_span in location_spans:\n",
    "            # Extract the location and link from the contest location span\n",
    "            location = location_span.xpath('text()')[0].strip()\n",
    "            link = location_span.xpath('a/@href')[0]\n",
    "            # Append the information to the respective lists\n",
    "            divisions.append(division)\n",
    "            locations.append(location)\n",
    "            links.append(link)\n",
    "    except:\n",
    "        print(f'Error: Failed to retrieve data for {division}')\n",
    "\n",
    "# Create a dataframe to store the extracted information\n",
    "df = pd.DataFrame({'Division': divisions, 'Location': locations, 'Link': links})\n",
    "\n",
    "# Reset the index of the dataframe\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up the dataframe\n",
    "\n",
    "## Drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df.head(30)\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "\n",
    "### Output as a csv\n",
    "## Might want to go back and adjust code to try to store which specific games are at each location\n",
    "## regional has (semis and finals) then there is a quarterfinals round\n",
    "\n",
    "df.to_csv('data/2023_regional_hosts.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Project\n",
    "\n",
    "### Create a json with just the fields in michigan and try to integrate a column that marks the appropriate fields as host of districts and regionals\n",
    "\n",
    "The text of the locations in the playoff csvs is not going to match the field names all that well. it might be worth trying to identify them from the map location - will have to go back to districts and extract map locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try to get all the google maps link from the districts page\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## Read local file\n",
    "path = 'districts_2023.html'\n",
    "html = open(path, 'r').read()\n",
    "\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Find all the tournament divs\n",
    "tournaments = soup.select('div.keep-together')\n",
    "\n",
    "# Initialize lists to store data\n",
    "district_numbers = []\n",
    "hosts = []\n",
    "locations = []\n",
    "\n",
    "# Extract data for each tournament\n",
    "for tournament in tournaments:\n",
    "    district_number = tournament.find('span', {'data-bind': 'text:Division'}).text\n",
    "    host = tournament.find('span', {'data-bind': 'text:Host'}).text\n",
    "    location = tournament.find('a', {'target': '_blank'}).get('href')\n",
    "    \n",
    "\n",
    "    district_numbers.append(district_number)\n",
    "    hosts.append(host)\n",
    "    locations.append(location)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {'Division': district_numbers, 'Host': hosts, 'Location': locations}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# add another column the is the index number of the row + 1\n",
    "df['District'] = df.index + 1\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "df.to_csv('district_tournaments.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to match up locations from the scraped district and regional and link them to a field in my json data\n",
    "\n",
    "## Stragegy: The district csv contains a field that has a link to a google maps search. Loop through all of those and return the lat and longitude coordinates then match the coordinates to the nearest home plate coordinate in the json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import googlemaps\n",
    "\n",
    "## paths\n",
    "\n",
    "local_json = 'data\\michigan_fields.json'\n",
    "\n",
    "district_csv = 'district_tournaments.csv'\n",
    "\n",
    "regional_csv = 'data\\2023_regional_hosts.csv'\n",
    "\n",
    "# Replace this with your own API key\n",
    "api_key = \"AIzaSyA_BhlTupRdBPBhRptQuR6pYorMVYQnRMA\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Data\n",
    "df = pd.read_csv(district_csv)\n",
    "\n",
    "# df.head()\n",
    "\n",
    "df.info()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean up the location column to remove first portion and just leave the address remaining\n",
    "\n",
    "\n",
    "\n",
    "# Remove the unwanted portion of the string in the 'Location' column\n",
    "prefix = \"http://maps.google.com/maps?q=\"\n",
    "\n",
    "# Check if the location is a string before applying lstrip\n",
    "df['Location'] = df['Location'].apply(lambda x: x.lstrip(prefix) if isinstance(x, str) else x)\n",
    "\n",
    "# Print the cleaned DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USES GOOGLE CODE\n",
    "import pandas as pd\n",
    "import googlemaps\n",
    "\n",
    "# Replace 'your_api_key' with your actual Google Maps API key\n",
    "api_key = 'AIzaSyA_BhlTupRdBPBhRptQuR6pYorMVYQnRMA'\n",
    "gmaps = googlemaps.Client(key=api_key)\n",
    "\n",
    "# # Create a DataFrame from your data (use your actual DataFrame here)\n",
    "# data = {\n",
    "#     \"Division\": [1, 1, 1],\n",
    "#     \"Host\": [\"Marquette\", \"Midland Dow\", \"Muskegon Mona Shores\"],\n",
    "#     \"Location\": [\n",
    "#         \"North Marquette Fields, Marquette, MI\",\n",
    "#         \"H H Dow High School - Baseball, 3901 N. Saginaw Rd. Midland, MI\",\n",
    "#         \"Mona Shores Baseball Field, 1121 W. Seminole Rd. Muskegon, MI\",\n",
    "#     ],\n",
    "#     \"District\": [1, 2, 3],\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# Function to get the coordinates for a given address\n",
    "def get_coordinates(address):\n",
    "    geocode_result = gmaps.geocode(address)\n",
    "    if geocode_result:\n",
    "        lat = geocode_result[0][\"geometry\"][\"location\"][\"lat\"]\n",
    "        lng = geocode_result[0][\"geometry\"][\"location\"][\"lng\"]\n",
    "        return (lat, lng)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to the 'Location' column and store the coordinates in a new column\n",
    "df[\"Coordinates\"] = df[\"Location\"].apply(get_coordinates)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check Output\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google code worked OK - returned coords for 126 of 128\n",
    "\n",
    "### Below I am going to try to match up those coordinates to the michigan fields jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### set up paths and load data(copied from above)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "# import googlemaps\n",
    "\n",
    "## paths\n",
    "\n",
    "local_json = 'data\\michigan_fields.json'\n",
    "\n",
    "district_csv = 'district_tournaments.csv'\n",
    "\n",
    "regional_csv = 'data\\2023_regional_hosts.csv'\n",
    "\n",
    "# Replace this with your own API key\n",
    "api_key = \"AIzaSyA_BhlTupRdBPBhRptQuR6pYorMVYQnRMA\"\n",
    "\n",
    "# load Data\n",
    "df = pd.read_csv(district_csv)\n",
    "\n",
    "## Load MI fields data from json file\n",
    "\n",
    "\n",
    "# Read the JSON file\n",
    "with open(local_json) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame from the JSON data\n",
    "mi_df = pd.DataFrame(data)\n",
    "\n",
    "mi_df.head()\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try a different approach to match the fields\n",
    "## Use the Host name to find 3 matches from the mi_df\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# Assuming you have the two dataframes df and mi_df\n",
    "\n",
    "def find_closest_park_names(host, n_closest=3):\n",
    "    closest_park_names = process.extract(host, mi_df[\"park_name\"], limit=n_closest, scorer=fuzz.token_sort_ratio)\n",
    "    return [name for name, score, index in closest_park_names]\n",
    "\n",
    "# Apply the function to the 'Host' column and store the results in new columns\n",
    "df[[\"closest_park_1\", \"closest_park_2\", \"closest_park_3\"]] = df[\"Host\"].apply(find_closest_park_names).apply(pd.Series)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTION TO FIND NEAREST FIELD TO DISTRICT TOURNAMENT LOCATION\n",
    "\n",
    "import math\n",
    "\n",
    "## Define a function to calculate the Haversine distance between two points\n",
    "def haversine_distance(coord1, coord2):\n",
    "    # Convert latitude and longitude to radians\n",
    "    lat1, lon1 = map(math.radians, coord1)\n",
    "    lat2, lon2 = map(math.radians, coord2)\n",
    "\n",
    "    # Calculate the differences between latitudes and longitudes\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # Calculate the Haversine distance\n",
    "    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    r = 6371  # Radius of the Earth in km\n",
    "\n",
    "    return c * r\n",
    "\n",
    "def find_closest_parks(coord, n_closest=3):\n",
    "    if coord is None:\n",
    "        return [\"Unknown\"] * n_closest\n",
    "\n",
    "    mi_df[\"distance\"] = mi_df[\"home_plate\"].apply(lambda x: haversine_distance(coord, (x[1], x[0])))\n",
    "    closest_park_indices = mi_df[\"distance\"].nsmallest(n_closest).index\n",
    "    return mi_df.loc[closest_park_indices, \"park_name\"].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Make sure 'home_plate' in mi_df has coordinates in the format (lat, lng)\n",
    "mi_df[\"home_plate\"] = mi_df[\"home_plate\"].apply(lambda x: (x[0], x[1]))\n",
    "\n",
    "# Create a new column 'closest_park' in df\n",
    "df[[\"closest_park_1\", \"closest_park_2\", \"closest_park_3\"]] = df[\"Coordinates\"].apply(find_closest_parks).apply(pd.Series)\n",
    "\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output this matching as a csv so I can manulauly check it\n",
    "\n",
    "df.to_csv('district_fields_text_match.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of 5-9-23 Work for now. output csv file with possible matches for the district fields\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with outlier fields \n",
    "\n",
    "## Start 59/23 Night\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependencies and Setup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "\n",
    "# Load files\n",
    "out_df = pd.read_csv('outlier_fields.csv')\n",
    "\n",
    "out_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_out = df_out.copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you already have the out_df DataFrame\n",
    "# Creating an empty HP2 column\n",
    "out_df['HP2'] = None\n",
    "\n",
    "# Loop through the DataFrame rows and populate the HP2 column with repeated points\n",
    "for idx, row in out_df.iterrows():\n",
    "    fop_list = row['fop']\n",
    "    repeated_points = [point for point in set(fop_list) if fop_list.count(point) > 1]\n",
    "    if len(repeated_points) > 0:\n",
    "        out_df.loc[idx, 'HP2'] = str(repeated_points[0])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "out_df.head(20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "url = 'https://my.mhsaa.com/Sports/Baseball/Districts'  # Replace this with the URL of the webpage you want to scrape\n",
    "page = requests.get(url)\n",
    "tree = html.fromstring(page.content)\n",
    "\n",
    "# Find the game location using the XPath\n",
    "location = tree.xpath('/html/body/form/div[5]/div[2]/div/div/div[2]/div[1]/div/div/div/div/div/div/div/div[2]/div[2]/div[1]/div[2]/div[1]/a')[0]\n",
    "\n",
    "# Extract the relevant information\n",
    "name = location.text.strip()\n",
    "link = location.get('href')\n",
    "address = link.split('=')[1].strip()\n",
    "\n",
    "# Create a dataframe to store the scraped data\n",
    "df = pd.DataFrame({'Field_name': [name], 'Location': [address], 'Link': [link]})\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Get latitude and longitude for each location\n",
    "df[\"Coordinates\"] = df[\"Location\"].apply(get_latitude_longitude)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_name_color.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Match the school name to School and merge the dataframes into a single object\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "def find_best_match(school_name, choices, score_cutoff=70):\n",
    "    best_match = process.extractOne(school_name, choices, scorer=fuzz.token_sort_ratio, score_cutoff=score_cutoff)\n",
    "    if best_match:\n",
    "        return best_match[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Get the list of school names from df_name_color\n",
    "school_names = df_name_color['School'].tolist()\n",
    "\n",
    "# Apply find_best_match function to create a new column 'best_match' in df_enrol\n",
    "df_enrol['best_match'] = df_enrol['school_name'].apply(find_best_match, choices=school_names, score_cutoff=80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'School' column in df_name_color to 'best_match'\n",
    "df_name_color = df_name_color.rename(columns={'School': 'best_match'})\n",
    "\n",
    "# Merge the dataframes on the 'best_match' column\n",
    "df_merged = df_enrol.merge(df_name_color, on='best_match', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_enrol.columns)\n",
    "\n",
    "print(df_name_color.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - get icons sorted out\n",
    "# -*** DONE*** get the level assigner sorted out in the etl\n",
    "# - add filter based on level to map\n",
    "# - implement the search box places from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
