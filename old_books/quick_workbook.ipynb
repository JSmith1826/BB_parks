{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the district html\n",
    "\n",
    "#### Goal: get dataframe that includes district games and times"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## each region runs 1-10 with 8-10 being the state semi final and final\n",
    "\n",
    "## Base url\n",
    "\n",
    "'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/'\n",
    "\n",
    "then groupNumber\n",
    "\n",
    "'/SportSeasonId/424201/Classification/'\n",
    "\n",
    "divsionNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## open the match_df and change the names back to Title case so it can be used with the other dataframes\n",
    "\n",
    "match_df = pd.read_csv('../BB_CLEAN/data/match_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ProperName', 'PopularName', 'ProperName_Score', 'PopularName_Score',\n",
       "       'MHSAA_SchoolId', 'SchoolInfo_Index', 'School_Name', 'City'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ProperName', 'PopularName', 'School_Name']\n",
    "\n",
    "## Title case the names in the columns\n",
    "for col in cols:\n",
    "    match_df[col] = match_df[col].str.title()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace 'High School' in ProperName with 'HS' to match the other dataframes\n",
    "\n",
    "match_df['ProperName'] = match_df['ProperName'].str.replace('High School', 'HS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProperName</th>\n",
       "      <th>PopularName</th>\n",
       "      <th>ProperName_Score</th>\n",
       "      <th>PopularName_Score</th>\n",
       "      <th>MHSAA_SchoolId</th>\n",
       "      <th>SchoolInfo_Index</th>\n",
       "      <th>School_Name</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brighton HS</td>\n",
       "      <td>Brighton</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>3834</td>\n",
       "      <td>143</td>\n",
       "      <td>Brighton High School</td>\n",
       "      <td>BRIGHTON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Roseville HS</td>\n",
       "      <td>Roseville</td>\n",
       "      <td>100</td>\n",
       "      <td>53</td>\n",
       "      <td>3837</td>\n",
       "      <td>1057</td>\n",
       "      <td>Roseville High School</td>\n",
       "      <td>ROSEVILLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shepherd HS</td>\n",
       "      <td>Shepherd</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>3838</td>\n",
       "      <td>1086</td>\n",
       "      <td>Shepherd High School</td>\n",
       "      <td>SHEPHERD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Corunna HS</td>\n",
       "      <td>Corunna</td>\n",
       "      <td>100</td>\n",
       "      <td>46</td>\n",
       "      <td>3844</td>\n",
       "      <td>300</td>\n",
       "      <td>Corunna High School</td>\n",
       "      <td>CORUNNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arts Academy In The Woods</td>\n",
       "      <td>Fraser Arts Academy</td>\n",
       "      <td>100</td>\n",
       "      <td>68</td>\n",
       "      <td>3845</td>\n",
       "      <td>51</td>\n",
       "      <td>Arts Academy In The Woods</td>\n",
       "      <td>Fraser</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ProperName          PopularName  ProperName_Score  \\\n",
       "0                Brighton HS             Brighton               100   \n",
       "1               Roseville HS            Roseville               100   \n",
       "2                Shepherd HS             Shepherd               100   \n",
       "3                 Corunna HS              Corunna               100   \n",
       "4  Arts Academy In The Woods  Fraser Arts Academy               100   \n",
       "\n",
       "   PopularName_Score  MHSAA_SchoolId  SchoolInfo_Index  \\\n",
       "0                 50            3834               143   \n",
       "1                 53            3837              1057   \n",
       "2                 50            3838              1086   \n",
       "3                 46            3844               300   \n",
       "4                 68            3845                51   \n",
       "\n",
       "                 School_Name       City  \n",
       "0       Brighton High School   BRIGHTON  \n",
       "1      Roseville High School  ROSEVILLE  \n",
       "2       Shepherd High School   SHEPHERD  \n",
       "3        Corunna High School    CORUNNA  \n",
       "4  Arts Academy In The Woods     Fraser  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Save the dataframe\n",
    "match_df.to_csv('../BB_CLEAN/data/match_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProperName</th>\n",
       "      <th>PopularName</th>\n",
       "      <th>ProperName_Score</th>\n",
       "      <th>PopularName_Score</th>\n",
       "      <th>MHSAA_SchoolId</th>\n",
       "      <th>SchoolInfo_Index</th>\n",
       "      <th>School_Name</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brighton HS</td>\n",
       "      <td>Brighton</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>3834</td>\n",
       "      <td>143</td>\n",
       "      <td>Brighton High School</td>\n",
       "      <td>BRIGHTON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Roseville HS</td>\n",
       "      <td>Roseville</td>\n",
       "      <td>100</td>\n",
       "      <td>53</td>\n",
       "      <td>3837</td>\n",
       "      <td>1057</td>\n",
       "      <td>Roseville High School</td>\n",
       "      <td>ROSEVILLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shepherd HS</td>\n",
       "      <td>Shepherd</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>3838</td>\n",
       "      <td>1086</td>\n",
       "      <td>Shepherd High School</td>\n",
       "      <td>SHEPHERD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Corunna HS</td>\n",
       "      <td>Corunna</td>\n",
       "      <td>100</td>\n",
       "      <td>46</td>\n",
       "      <td>3844</td>\n",
       "      <td>300</td>\n",
       "      <td>Corunna High School</td>\n",
       "      <td>CORUNNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arts Academy In The Woods</td>\n",
       "      <td>Fraser Arts Academy</td>\n",
       "      <td>100</td>\n",
       "      <td>68</td>\n",
       "      <td>3845</td>\n",
       "      <td>51</td>\n",
       "      <td>Arts Academy In The Woods</td>\n",
       "      <td>Fraser</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ProperName          PopularName  ProperName_Score  \\\n",
       "0                Brighton HS             Brighton               100   \n",
       "1               Roseville HS            Roseville               100   \n",
       "2                Shepherd HS             Shepherd               100   \n",
       "3                 Corunna HS              Corunna               100   \n",
       "4  Arts Academy In The Woods  Fraser Arts Academy               100   \n",
       "\n",
       "   PopularName_Score  MHSAA_SchoolId  SchoolInfo_Index  \\\n",
       "0                 50            3834               143   \n",
       "1                 53            3837              1057   \n",
       "2                 50            3838              1086   \n",
       "3                 46            3844               300   \n",
       "4                 68            3845                51   \n",
       "\n",
       "                 School_Name       City  \n",
       "0       Brighton High School   BRIGHTON  \n",
       "1      Roseville High School  ROSEVILLE  \n",
       "2       Shepherd High School   SHEPHERD  \n",
       "3        Corunna High School    CORUNNA  \n",
       "4  Arts Academy In The Woods     Fraser  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd;lfhasd;lkf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1314 entries, 0 to 1313\n",
      "Data columns (total 34 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   school_name                       1314 non-null   object \n",
      " 1   school_id                         1314 non-null   int64  \n",
      " 2   school_district                   1314 non-null   object \n",
      " 3   school_district_id                1314 non-null   int64  \n",
      " 4   total_students                    1314 non-null   float64\n",
      " 5   teachers                          1314 non-null   float64\n",
      " 6   student_teacher_ratio             1314 non-null   object \n",
      " 7   lunch_free_count                  1314 non-null   object \n",
      " 8   lunch_reduced_count               1314 non-null   object \n",
      " 9   lunch_total_count                 1314 non-null   object \n",
      " 10  9                                 974 non-null    float64\n",
      " 11  10                                972 non-null    float64\n",
      " 12  11                                970 non-null    float64\n",
      " 13  12                                838 non-null    float64\n",
      " 14  American Indian/Alaska Native     1234 non-null   float64\n",
      " 15  Asian                             1234 non-null   float64\n",
      " 16  Black                             1234 non-null   float64\n",
      " 17  Hispanic                          1234 non-null   float64\n",
      " 18  Native Hawaiian/Pacific Islander  1234 non-null   float64\n",
      " 19  White                             1234 non-null   float64\n",
      " 20  Two or MoreRaces                  1234 non-null   float64\n",
      " 21  Male                              1314 non-null   int64  \n",
      " 22  Female                            1314 non-null   int64  \n",
      " 23  KG                                298 non-null    float64\n",
      " 24  1                                 304 non-null    float64\n",
      " 25  2                                 306 non-null    float64\n",
      " 26  3                                 312 non-null    float64\n",
      " 27  4                                 329 non-null    float64\n",
      " 28  5                                 184 non-null    float64\n",
      " 29  6                                 179 non-null    float64\n",
      " 30  7                                 278 non-null    float64\n",
      " 31  8                                 327 non-null    float64\n",
      " 32  PK                                177 non-null    object \n",
      " 33  Ungraded                          81 non-null     float64\n",
      "dtypes: float64(23), int64(4), object(7)\n",
      "memory usage: 349.2+ KB\n"
     ]
    }
   ],
   "source": [
    "## Load full semi-raw public HS data\n",
    "\n",
    "path = 'public_school_demo_data.csv'\n",
    "\n",
    "df = pd.read_csv(path, encoding='latin-1')\n",
    "\n",
    "df.info()\n",
    "\n",
    "# df_public.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justin\\AppData\\Local\\Temp\\ipykernel_11104\\3280483564.py:4: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df['HS_ENROLL'] = df.iloc[:, 9:13].sum(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['school_name', 'school_id', 'school_district', 'school_district_id',\n",
       "       'total_students', 'teachers', 'student_teacher_ratio',\n",
       "       'lunch_free_count', 'lunch_reduced_count', 'lunch_total_count', '9',\n",
       "       '10', '11', '12', 'American Indian/Alaska Native', 'Asian', 'Black',\n",
       "       'Hispanic', 'Native Hawaiian/Pacific Islander', 'White',\n",
       "       'Two or MoreRaces', 'Male', 'Female', 'KG', '1', '2', '3', '4', '5',\n",
       "       '6', '7', '8', 'PK', 'Ungraded', 'HS_ENROLL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### clean the data to only the high school level (grades 9-12)\n",
    "\n",
    "## Add the values in the 9 10 11 12 columns to get HS_ENROLL by column position\n",
    "df['HS_ENROLL'] = df.iloc[:, 9:13].sum(axis=1)\n",
    "\n",
    "# drop any rows that have a 0 in the HS_ENROLL column\n",
    "df = df[df['HS_ENROLL'] != 0]\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape\n",
    "# df.info()\n",
    "\n",
    "# rename demo columns to match scchool master table\n",
    "\n",
    "name_list = ['demo_pct_American_Indian','demo_pct_Asian',\n",
    "             'demo_pct_Hispanic','demo_pct_Black',\n",
    "             'demo_pct_White','demo_pct_Pacific_Islander',\n",
    "             'demo_pct_Two_or_More']\n",
    "\n",
    "replace_list = ['American Indian/Alaska Native', 'Asian',\n",
    "                'Hispanic', 'Black',\n",
    "                'White', 'Native Hawaiian/Pacific Islander',\n",
    "                'Two or MoreRaces']\n",
    "\n",
    "# replace the coumn names\n",
    "df.rename(columns=dict(zip(replace_list, name_list)), inplace=True)\n",
    "\n",
    "## Multiply the demo_pct columns by 100 so they are in whole numbers to match the school master table\n",
    "df[name_list] = df[name_list].multiply(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate male / female percentage\n",
    "\n",
    "df['Male'] = df['Male'] / df['total_students']\n",
    "df['Female'] = df['Female'] / df['total_students']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename school_name to match other df\n",
    "df.rename(columns={'school_name':'School_Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 899 entries, 0 to 1313\n",
      "Data columns (total 35 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   School_Name                899 non-null    object \n",
      " 1   school_id                  899 non-null    int64  \n",
      " 2   school_district            899 non-null    object \n",
      " 3   school_district_id         899 non-null    int64  \n",
      " 4   total_students             899 non-null    float64\n",
      " 5   teachers                   899 non-null    float64\n",
      " 6   student_teacher_ratio      899 non-null    object \n",
      " 7   lunch_free_count           899 non-null    object \n",
      " 8   lunch_reduced_count        899 non-null    object \n",
      " 9   lunch_total_count          899 non-null    object \n",
      " 10  9                          886 non-null    float64\n",
      " 11  10                         883 non-null    float64\n",
      " 12  11                         872 non-null    float64\n",
      " 13  12                         753 non-null    float64\n",
      " 14  demo_pct_American_Indian   899 non-null    float64\n",
      " 15  demo_pct_Asian             899 non-null    float64\n",
      " 16  demo_pct_Black             899 non-null    float64\n",
      " 17  demo_pct_Hispanic          899 non-null    float64\n",
      " 18  demo_pct_Pacific_Islander  899 non-null    float64\n",
      " 19  demo_pct_White             899 non-null    float64\n",
      " 20  demo_pct_Two_or_More       899 non-null    float64\n",
      " 21  Male                       899 non-null    float64\n",
      " 22  Female                     899 non-null    float64\n",
      " 23  KG                         0 non-null      float64\n",
      " 24  1                          0 non-null      float64\n",
      " 25  2                          0 non-null      float64\n",
      " 26  3                          0 non-null      float64\n",
      " 27  4                          13 non-null     float64\n",
      " 28  5                          23 non-null     float64\n",
      " 29  6                          147 non-null    float64\n",
      " 30  7                          250 non-null    float64\n",
      " 31  8                          301 non-null    float64\n",
      " 32  PK                         17 non-null     object \n",
      " 33  Ungraded                   61 non-null     float64\n",
      " 34  HS_ENROLL                  899 non-null    float64\n",
      "dtypes: float64(26), int64(2), object(7)\n",
      "memory usage: 252.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['School Name', 'District', 'County Name', 'Street Address', 'City',\n",
       "       'State', 'ZIP', 'ZIP 4-digit', 'Phone', 'Locale Code', 'Locale',\n",
       "       'Charter', 'Magnet', 'Title I School', 'Title 1 School Wide',\n",
       "       'HS_ENROLL', 'Teachers', 'Student Teacher Ratio', 'Free Lunch',\n",
       "       'Reduced Lunch', 'Directly Certified', 'school_type', 'Coed?', 'Relig',\n",
       "       'Community_Type', 'Relig_subtype', 'School ID(private)',\n",
       "       'PSS_COUNTY_NO', 'County FIPS', 'demo_pct_American Indian',\n",
       "       'demo_pct_Asian', 'demo_pct_Hispanic', 'demo_pct_Black',\n",
       "       'demo_pct_White', 'demo_pct_Pacific Islander', 'demo_pct_Two_or_More',\n",
       "       'PSS_ASSOC_1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## open the school master table and merge the demographic columns into the master table\n",
    "\n",
    "path = 'TEMP\\clean_tables\\school_master_table.csv'\n",
    "\n",
    "master_df = pd.read_csv(path, encoding='latin-1')\n",
    "\n",
    "master_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change master school name column\n",
    "\n",
    "## replace an spaces in the master_df column names with underscores\n",
    "master_df.columns = master_df.columns.str.replace(' ', '_')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['School_Name', 'District', 'County_Name', 'Street_Address', 'City',\n",
       "       'State', 'ZIP', 'ZIP_4-digit', 'Phone', 'Locale_Code', 'Locale',\n",
       "       'Charter', 'Magnet', 'Title_I_School', 'Title_1_School_Wide',\n",
       "       'HS_ENROLL', 'Teachers', 'Student_Teacher_Ratio', 'Free_Lunch',\n",
       "       'Reduced_Lunch', 'Directly_Certified', 'school_type', 'Coed?', 'Relig',\n",
       "       'Community_Type', 'Relig_subtype', 'School_ID(private)',\n",
       "       'PSS_COUNTY_NO', 'County_FIPS', 'demo_pct_American_Indian',\n",
       "       'demo_pct_Asian', 'demo_pct_Hispanic', 'demo_pct_Black',\n",
       "       'demo_pct_White', 'demo_pct_Pacific_Islander', 'demo_pct_Two_or_More',\n",
       "       'PSS_ASSOC_1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge the demographic columns into the master table\n",
    "# match on school name and only send the columns we want to merge\n",
    "\n",
    "# First, merge the two dataframes on 'School Name'\n",
    "merged_df = pd.merge(master_df, df[['School_Name', 'demo_pct_American_Indian', 'demo_pct_Asian', \n",
    "                                    'demo_pct_Hispanic', 'demo_pct_Black', 'demo_pct_White', \n",
    "                                    'demo_pct_Pacific_Islander', 'demo_pct_Two_or_More']], \n",
    "                     on='School_Name', \n",
    "                     how='left', \n",
    "                     suffixes=('', '_y'))\n",
    "\n",
    "# Now, fill in the missing values in the original dataframe with the values from the new dataframe\n",
    "for column in ['demo_pct_American_Indian', 'demo_pct_Asian', 'demo_pct_Hispanic', 'demo_pct_Black', \n",
    "               'demo_pct_White', 'demo_pct_Pacific_Islander', 'demo_pct_Two_or_More']:\n",
    "    merged_df[column].fillna(merged_df[column + '_y'], inplace=True)\n",
    "\n",
    "# Finally, drop the duplicate columns\n",
    "merged_df.drop(columns=[column + '_y' for column in ['demo_pct_American_Indian', 'demo_pct_Asian', \n",
    "                                                    'demo_pct_Hispanic', 'demo_pct_Black', \n",
    "                                                    'demo_pct_White', 'demo_pct_Pacific_Islander', \n",
    "                                                    'demo_pct_Two_or_More']], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1506 entries, 0 to 1505\n",
      "Data columns (total 37 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   School_Name                1506 non-null   object \n",
      " 1   District                   1350 non-null   object \n",
      " 2   County_Name                1506 non-null   object \n",
      " 3   Street_Address             1506 non-null   object \n",
      " 4   City                       1506 non-null   object \n",
      " 5   State                      1506 non-null   object \n",
      " 6   ZIP                        1506 non-null   int64  \n",
      " 7   ZIP_4-digit                1204 non-null   float64\n",
      " 8   Phone                      1505 non-null   object \n",
      " 9   Locale_Code                1350 non-null   float64\n",
      " 10  Locale                     1506 non-null   object \n",
      " 11  Charter                    1506 non-null   object \n",
      " 12  Magnet                     1506 non-null   object \n",
      " 13  Title_I_School             1506 non-null   object \n",
      " 14  Title_1_School_Wide        1506 non-null   object \n",
      " 15  HS_ENROLL                  1506 non-null   float64\n",
      " 16  Teachers                   1350 non-null   float64\n",
      " 17  Student_Teacher_Ratio      1506 non-null   object \n",
      " 18  Free_Lunch                 1350 non-null   object \n",
      " 19  Reduced_Lunch              1350 non-null   object \n",
      " 20  Directly_Certified         1350 non-null   object \n",
      " 21  school_type                1506 non-null   object \n",
      " 22  Coed?                      1506 non-null   object \n",
      " 23  Relig                      1506 non-null   object \n",
      " 24  Community_Type             1506 non-null   object \n",
      " 25  Relig_subtype              1506 non-null   object \n",
      " 26  School_ID(private)         156 non-null    object \n",
      " 27  PSS_COUNTY_NO              156 non-null    float64\n",
      " 28  County_FIPS                156 non-null    float64\n",
      " 29  demo_pct_American_Indian   1082 non-null   float64\n",
      " 30  demo_pct_Asian             1083 non-null   float64\n",
      " 31  demo_pct_Hispanic          1082 non-null   float64\n",
      " 32  demo_pct_Black             1083 non-null   float64\n",
      " 33  demo_pct_White             1082 non-null   float64\n",
      " 34  demo_pct_Pacific_Islander  1083 non-null   float64\n",
      " 35  demo_pct_Two_or_More       1082 non-null   float64\n",
      " 36  PSS_ASSOC_1                156 non-null    object \n",
      "dtypes: float64(13), int64(1), object(23)\n",
      "memory usage: 447.1+ KB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save it to the clean tables directory but don't overwrite the original master table\n",
    "\n",
    "path = 'TEMP\\clean_tables\\school_master_table_NEW_POST_MERGE.csv'\n",
    "\n",
    "merged_df.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop columns\n",
    "\n",
    "col = ['PK','KG','1','2','3','4','5','6','7','8','9','10','11','12','Ungraded']\n",
    "\n",
    "df.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['School_Name', 'school_id', 'school_district', 'school_district_id',\n",
       "       'total_students', 'teachers', 'student_teacher_ratio',\n",
       "       'lunch_free_count', 'lunch_reduced_count', 'lunch_total_count',\n",
       "       'demo_pct_American_Indian', 'demo_pct_Asian', 'demo_pct_Black',\n",
       "       'demo_pct_Hispanic', 'demo_pct_Pacific_Islander', 'demo_pct_White',\n",
       "       'demo_pct_Two_or_More', 'Male', 'Female', 'HS_ENROLL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_public' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Justin\\Desktop\\Project\\BB_parks\\quick_workbook.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y211sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Rename columns to match private school data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y211sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Rename Students to HS_ENROLL\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y211sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df_public \u001b[39m=\u001b[39m df_public\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mStudents\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mHS_ENROLL\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y211sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Create and fill some columns that are from the private school data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y211sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m df_public \u001b[39m=\u001b[39m df_public\u001b[39m.\u001b[39massign(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y211sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mCoed\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mYes\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y211sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mRelig\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y211sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mCommunity_Type\u001b[39m\u001b[39m'\u001b[39m: df_public[\u001b[39m'\u001b[39m\u001b[39mLocale\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mstr[\u001b[39m0\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y211sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mRelig_subtype\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/Desktop/Project/BB_parks/quick_workbook.ipynb#Y211sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_public' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Rename columns to match private school data\n",
    "# Rename Students to HS_ENROLL\n",
    "\n",
    "df_public = df_public.rename(columns={'Students': 'HS_ENROLL'})\n",
    "\n",
    "# Create and fill some columns that are from the private school data\n",
    "df_public = df_public.assign(\n",
    "    **{'Coed': 'Yes',\n",
    "    'Relig': 'None',\n",
    "    'Community_Type': df_public['Locale'].str.split(':').str[0],\n",
    "    'Relig_subtype': 'None'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### END\n",
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create List of urls to parse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "\n",
    "base = 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/'\n",
    "base2 = '/SportSeasonId/424201/Classification/'\n",
    "\n",
    "bracket = []\n",
    "division = []\n",
    "\n",
    "## Create a list of urls to parse\n",
    "for i in range(1, 9):\n",
    "    for j in range(1, 5):\n",
    "        bracket.append(base + str(i) + base2 + str(j))\n",
    "        division.append(str(i) + '-' + str(j))\n",
    "\n",
    "## Create a dictionary of urls and divisions\n",
    "bracket_dict = dict(zip(division, bracket))\n",
    "\n",
    "# create a list of all the urls\n",
    "url_list = list(bracket_dict.values())\n",
    "\n",
    "print(url_list[0:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_bracket(url):\n",
    "    # Request the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all divs containing game info\n",
    "    games = soup.find_all('div', class_='contestboxhover')\n",
    "\n",
    "    # Find all li elements containing score info\n",
    "    scores_li = soup.find_all('li')\n",
    "\n",
    "    # Create a dictionary to store scores\n",
    "    scores_dict = {}\n",
    "    for score in scores_li:\n",
    "        # Check if the list item contains a score\n",
    "        if ',' in score.text:\n",
    "            try:\n",
    "                # Split teams and score\n",
    "                team_score1, team_score2 = score.text.split(\", \")\n",
    "                # Further split into team and score\n",
    "                team1, score1_str = team_score1.rsplit(' ', 1)\n",
    "                team2, score2_str = team_score2.rsplit(' ', 1)\n",
    "                # Convert scores to int\n",
    "                score1, score2 = int(score1_str), int(score2_str)\n",
    "                # Map teams to score in dictionary\n",
    "                scores_dict[(team1.strip(), team2.strip())] = (score1, score2)\n",
    "                print(f\"Processed score: {team1} {score1} vs {team2} {score2}\")  # Print the processed score for debugging\n",
    "            except ValueError:\n",
    "                print(f\"Failed to process score: {score.text}\")  # Print the score that failed to process\n",
    "                continue\n",
    "\n",
    "### Create a dataframe from the scores dictionary\n",
    "    scores_df = pd.DataFrame.from_dict(scores_dict, orient='index', columns=['Score1', 'Score2'])\n",
    "\n",
    "       # Initialize an empty list to store game data\n",
    "    data = []\n",
    "\n",
    "    # Iterate over each game\n",
    "    for game in games:\n",
    "        try:\n",
    "            round_elem = game.find('span', class_='contesttitle')\n",
    "            round = round_elem.text if round_elem else None\n",
    "\n",
    "            date_time_elem = game.find('span', class_='contestdate')\n",
    "            if date_time_elem:\n",
    "                date_time = date_time_elem.text.split()\n",
    "                date = ' '.join(date_time[:3])\n",
    "                time = ' '.join(date_time[3:])\n",
    "            else:\n",
    "                date, time = None, None\n",
    "\n",
    "            location_elem = game.find('span', class_='contestlocation')\n",
    "            location = location_elem.text if location_elem else None\n",
    "\n",
    "            teams = game.find_all('div', class_=['line1hov', 'line2hov'])\n",
    "            team1 = teams[0].text.strip() if teams else None\n",
    "            team2 = teams[1].text.strip() if teams else None\n",
    "\n",
    "            \n",
    "            print(f\"Processing game: {team1} vs {team2}\")  # Add this line to print the teams\n",
    "\n",
    "\n",
    "            score1, score2 = scores_dict.get((team1, team2), (None, None))\n",
    "\n",
    "            print(f\"Found scores: {score1} - {score2}\")  # Add this line to print the found scores\n",
    "\n",
    "            data.append([round, date, time, location, team1, score1, team2, score2])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error processing game: {e}')\n",
    "            continue\n",
    "\n",
    "    # Create a DataFrame from the games data\n",
    "    game_df = pd.DataFrame(data, columns=['Round', 'Date', 'Time', 'Location', 'Team1', 'Score1', 'Team2', 'Score2'])\n",
    "\n",
    "    \n",
    "    return game_df, scores_df\n",
    "\n",
    "\n",
    "url = \"https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/classification/3/sportseasonid/424201/bracketgroup/1\"\n",
    "games_df, scores_df = scrape_bracket(url)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = url_list # replace with your list of URLs\n",
    "game_dataframes = []\n",
    "score_dataframes = []\n",
    "\n",
    "for url in urls:\n",
    "    df_games, df_scores = scrape_bracket(url)\n",
    "    game_dataframes.append(df_games)\n",
    "    score_dataframes.append(df_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assign a name to the first column of the scores dataframe\n",
    "df_scores.index.name = 'Teams'\n",
    "## Split that column into two columns\n",
    "df_scores[['Team1', 'Team2']] = df_scores.index.to_series().apply(pd.Series)\n",
    "\n",
    "# Reindex the scores dataframe\n",
    "df_scores = df_scores.reindex(columns=['Team1', 'Score1', 'Team2', 'Score2'])\n",
    "\n",
    "df_scores.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save each dataframe to a csv so I can get a close look\n",
    "# df_games.to_csv('TEMP/df_games.csv')\n",
    "df_scores.to_csv('data/MHSAA/MHSAA_Scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the score to the bracket dataframe\n",
    "\n",
    "## open bracket dataframe\n",
    "df_bracket = pd.read_csv('data/MHSAA/mhsaa_brackets.csv')\n",
    "\n",
    "## Drop the Score columns if there are any\n",
    "if 'Score1' in df_bracket.columns:\n",
    "    df_bracket.drop(['Score1', 'Score2'], axis=1, inplace=True)\n",
    "    \n",
    "df_bracket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean the team names in both dfs to match\n",
    "# Convert to lower case and strip leading/trailing white spaces\n",
    "df_bracket['Team1'] = df_bracket['Team1'].fillna('').str.lower().str.strip()\n",
    "df_bracket['Team2'] = df_bracket['Team2'].fillna('').str.lower().str.strip()\n",
    "df_scores['Team1'] = df_scores['Team1'].fillna('').str.lower().str.strip()\n",
    "df_scores['Team2'] = df_scores['Team2'].fillna('').str.lower().str.strip()\n",
    "\n",
    "\n",
    "# Create a column in each df with the pair of teams in a game in alphabetical order to match\n",
    "# Create unique identifiers for each game\n",
    "df_bracket['team_pair'] = df_bracket.apply(lambda row: '_'.join(sorted([row['Team1'], row['Team2']])), axis=1)\n",
    "df_scores['team_pair'] = df_scores.apply(lambda row: '_'.join(sorted([row['Team1'], row['Team2']])), axis=1)\n",
    "\n",
    "# # make sure the District column is a string (for the javascript. it expects it that way for some reason)\n",
    "# df_bracket['District'] = df_bracket['District'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bracket.head()\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a fuzzy match on the team_pair columns to try to get better results\n",
    "# try to match the values of df_scores['team_pair'] to df_bracket['team_pair']\n",
    "# use those matches to copy the scores from df_scores to df_bracket\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Create a list of unique team pairs from the scores dataframe\n",
    "team_pairs = df_scores['team_pair'].unique().tolist()\n",
    "\n",
    "# Find a match in the bracket dataframe for each team pair\n",
    "for team_pair in team_pairs:\n",
    "    # Find the best match in the bracket dataframe for the team pair\n",
    "    match = process.extractOne(team_pair, df_bracket['team_pair'])\n",
    "    # If the match has a score of 100, it is a perfect match\n",
    "    if match[1] == 100:\n",
    "        # Get the index of the match\n",
    "        match_index = df_bracket[df_bracket['team_pair'] == match[0]].index[0]\n",
    "        # Copy the scores from the scores dataframe to the bracket dataframe\n",
    "        df_bracket.loc[match_index, 'Score1'] = df_scores[df_scores['team_pair'] == team_pair]['Score1'].values[0]\n",
    "        df_bracket.loc[match_index, 'Score2'] = df_scores[df_scores['team_pair'] == team_pair]['Score2'].values[0]\n",
    "    # If the match has a score of less than 100, it is not a perfect match\n",
    "    elif match[1] < 100:\n",
    "        # Get the index of the match\n",
    "        match_index = df_bracket[df_bracket['team_pair'] == match[0]].index[0]\n",
    "        # Copy the scores from the scores dataframe to the bracket dataframe\n",
    "        df_bracket.loc[match_index, 'Score1'] = df_scores[df_scores['team_pair'] == team_pair]['Score1'].values[0]\n",
    "        df_bracket.loc[match_index, 'Score2'] = df_scores[df_scores['team_pair'] == team_pair]['Score2'].values[0]\n",
    "    # If the match has a score of 0, there is no match\n",
    "    elif match[1] == 0:\n",
    "        print(f\"No match found for {team_pair}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scores.head(20)\n",
    "# df_bracket.head(20)\n",
    "\n",
    "# Check to see if there were any games that got a score\n",
    "df_bracket[df_bracket['Score1'].notnull() | df_bracket['Score2'].notnull()]\n",
    "\n",
    "# show a count of the number of games that got a score\n",
    "df_bracket[df_bracket['Score1'].notnull() | df_bracket['Score2'].notnull()].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bracket.info()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_bracket.copy()\n",
    "## Recapitalize the team names\n",
    "df_merge['Team1'] = df_merge['Team1'].str.title()\n",
    "df_merge['Team2'] = df_merge['Team2'].str.title()\n",
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the newly merged dataframe to a csv\n",
    "df_merge.to_csv('data/MHSAA/mhsaa_brackets.csv', index=False)\n",
    "\n",
    "# save to a json file\n",
    "df_merge.to_json('data/html/mhsaa/data/district_dict.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE TO CSV\n",
    "\n",
    "final_df.to_csv('TEMP/mhsaa_brackets.csv', index=False)\n",
    "\n",
    "### output a json\n",
    "final_df.to_json('data/html/mhsaa/data/district_dict.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.info()\n",
    "final_df.describe()\n",
    "final_df.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a csv and output a json\n",
    "df = pd.read_csv('../data/MHSAA/mhsaa_brackets.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Of Work Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('data/fields_cleaned.csv')\n",
    "\n",
    "## Read json into a dataframe\n",
    "df = pd.read_json('data/default_updated_output.json')\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_fields(data):\n",
    "    # Define weights for each parameter\n",
    "    weights = {\n",
    "        'max_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'min_distance': 1,  # positive weight since shorter fences favor hitters\n",
    "        'avg_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'median_distance': -1, # negative weight since longer fences favor pitchers\n",
    "        'field_area_sqft': -1,  # negative weight since larger fields favor pitchers\n",
    "        'fair_to_foul': -1,  # negative weight since larger ratio (more foul territory) favors pitchers\n",
    "    }\n",
    "\n",
    "    # Standardize features (subtract mean and divide by standard deviation)\n",
    "    standardized_data = data.copy()\n",
    "    for column in weights.keys():\n",
    "        standardized_data[column] = (standardized_data[column] - standardized_data[column].mean()) / standardized_data[column].std()\n",
    "\n",
    "    # Calculate score for each field\n",
    "    standardized_data['score'] = standardized_data.apply(lambda row: sum(row[param] * weight for param, weight in weights.items()), axis=1)\n",
    "\n",
    "    # Rank fields based on score (higher scores are more hitter-friendly)\n",
    "    ranked_fields = standardized_data.sort_values('score', ascending=False)\n",
    "\n",
    "    return ranked_fields\n",
    "\n",
    "# Suppose 'df' is your DataFrame containing the field data\n",
    "ranked_fields = rank_fields(df)\n",
    "print(ranked_fields[['park_name', 'score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ranked_fields.to_csv('data/ALL_ranked_fields.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import googlemaps\n",
    "import pandas as pd\n",
    "\n",
    "gmaps = googlemaps.Client(key='AIzaSyA_BhlTupRdBPBhRptQuR6pYorMVYQnRMA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USING KEYWORDS\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_place_names(coord, client):\n",
    "    \"\"\"\n",
    "    Get place names using Google Places API.\n",
    "    \n",
    "    Parameters:\n",
    "    coord: str - Coordinates in the format (lng, lat).\n",
    "    client: googlemaps.Client - Google Maps client.\n",
    "    \n",
    "    Returns:\n",
    "    list: Three closest place names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse coordinates\n",
    "        lng, lat = map(float, coord.strip('()').split(', '))\n",
    "        # Swap longitude and latitude for Google API\n",
    "        places_result_1 = client.places_nearby(location=(lat, lng), radius=1000, keyword='baseball field')\n",
    "        places_result_2 = client.places_nearby(location=(lat, lng), radius=1000, keyword='school')\n",
    "        places_result_3 = client.places_nearby(location=(lat, lng), radius=1000, type='park')\n",
    "        places_result = places_result_1['results'] + places_result_2['results'] + places_result_3['results']\n",
    "        place_names = [place['name'] for place in places_result[:5]]\n",
    "        return place_names\n",
    "    except:\n",
    "        # Return NaN if there is an error\n",
    "        return [np.nan]*5\n",
    "\n",
    "\n",
    "# Use tqdm to create progress bar\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply the function to each coordinate in 'home_plate'\n",
    "df['place_names'] = df['home_plate'].progress_apply(lambda coord: get_place_names(coord, gmaps))\n",
    "\n",
    "# Split 'place_names' into five separate columns\n",
    "df[['Place 1', 'Place 2', 'Place 3', 'Place 4', 'Place 5']] = pd.DataFrame(df['place_names'].to_list(), index=df.index)\n",
    "\n",
    "# Create new dataframe 'place_test_df' with the necessary columns\n",
    "place_test_df = df[['park_name', 'Place 1', 'Place 2', 'Place 3', 'Place 4', 'Place 5']]\n",
    "place_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output Csv for manual cleaning\n",
    "\n",
    "place_test_df.to_csv('data/mhsaa_places.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## FIRST TRY - working, returned OK results\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def get_place_names(coord, client):\n",
    "#     \"\"\"\n",
    "#     Get place names using Google Places API.\n",
    "    \n",
    "#     Parameters:\n",
    "#     coord: str - Coordinates in the format (lng, lat).\n",
    "#     client: googlemaps.Client - Google Maps client.\n",
    "    \n",
    "#     Returns:\n",
    "#     list: Three closest place names.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Parse coordinates\n",
    "#         lng, lat = map(float, coord.strip('()').split(', '))\n",
    "#         # Swap longitude and latitude for Google API\n",
    "#         places_result = client.places_nearby(location=(lat, lng), radius=100)\n",
    "#         place_names = [place['name'] for place in places_result['results'][:3]]\n",
    "#         return place_names\n",
    "#     except:\n",
    "#         # Return NaN if there is an error\n",
    "#         return [np.nan, np.nan, np.nan]\n",
    "\n",
    "# # Use tqdm to create progress bar\n",
    "# tqdm.pandas()\n",
    "\n",
    "# # Apply the function to each coordinate in 'home_plate'\n",
    "# df['place_names'] = df['home_plate'].progress_apply(lambda coord: get_place_names(coord, gmaps))\n",
    "\n",
    "# # Split 'place_names' into three separate columns\n",
    "# df[['Place 1', 'Place 2', 'Place 3']] = pd.DataFrame(df['place_names'].to_list(), index=df.index)\n",
    "\n",
    "# # Create new dataframe 'place_test_df' with the necessary columns\n",
    "# place_test_df = df[['park_name', 'Place 1', 'Place 2', 'Place 3']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_test_df.info()\n",
    "place_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the dataframe to a csv file\n",
    "\n",
    "place_test_df.to_csv('data/place_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW ALL IN ONE DISTRICT REGIONAL COMBO BLOCK\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# read the district host table\n",
    "d_path = 'data/2023_district_hosts.csv'\n",
    "\n",
    "# read regional table\n",
    "r_path = 'data/2023_regional_hosts.csv'\n",
    "\n",
    "# read the district host table\n",
    "d_host = pd.read_csv(d_path)\n",
    "\n",
    "# read regional table\n",
    "r_host = pd.read_csv(r_path)\n",
    "\n",
    "# Display summary of the district host table\n",
    "print(d_host.info())\n",
    "\n",
    "# Display summary of the regional host table\n",
    "print(r_host.info())\n",
    "\n",
    "# Lowercase the 'team' and 'park_name' columns to improve the match rate\n",
    "d_host['team'] = d_host['team'].str.lower()\n",
    "r_host['park_name'] = r_host['park_name'].str.lower()\n",
    "\n",
    "def fuzzy_merge(df_1, df_2, key1, key2, threshold=90):\n",
    "    \"\"\"\n",
    "    :param df_1: the left table to join\n",
    "    :param df_2: the right table to join\n",
    "    :param key1: key column of the left table\n",
    "    :param key2: key column of the right table\n",
    "    :param threshold: how close the matches should be to return a match, based on Levenshtein distance\n",
    "    :return: dataframe with both tables joined on fuzzy match\n",
    "    \"\"\"\n",
    "    s = df_2[key2].dropna().tolist()\n",
    "\n",
    "    m = df_1[key1].dropna().apply(lambda x: process.extractOne(x, s, scorer=fuzz.ratio))  \n",
    "    df_1['matches'] = m\n",
    "\n",
    "    m2 = df_1['matches'].apply(lambda x: x[0] if x[1] >= threshold else np.nan)\n",
    "    df_1['matches'] = m2\n",
    "\n",
    "    merged = df_1.merge(df_2.rename(columns={key2: 'matches'}), on='matches', how='outer')\n",
    "\n",
    "    # Remove the 'matches' column and return the merged dataframe\n",
    "    merged = merged.drop(columns=['matches'])\n",
    "    return merged\n",
    "\n",
    "# Execute the function\n",
    "merged_df = fuzzy_merge(d_host, r_host, 'team', 'park_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Rename some colums for clarity and drop unnecessary columns\n",
    "\n",
    "merged_df = merged_df.rename(columns={'division_x' : 'district_div', 'division_y' : 'regional_div'})\n",
    "\n",
    "merged_df = merged_df.drop(columns=['color4', 'Unnamed: 3', 'score'])\n",
    "\n",
    "merged_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save the merged dataframe to a csv file\n",
    "\n",
    "merged_df.to_csv('data/2023_all_hosts.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge the district host and the regional host into a all_host.csv table\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read the district host table\n",
    "d_path = 'data/2023_district_hosts.csv'\n",
    "\n",
    "\n",
    "# read rional table\n",
    "r_path = 'data/2023_regional_hosts.csv'\n",
    "\n",
    "## read the district host table\n",
    "d_host = pd.read_csv(d_path)\n",
    "\n",
    "## read rional table\n",
    "r_host = pd.read_csv(r_path)\n",
    "\n",
    "## Display summary og the district host table\n",
    "\n",
    "print(d_host.info())\n",
    "\n",
    "## Display summary og the regional host table\n",
    "\n",
    "print(r_host.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Lowercase the 'team' and 'park_name' columns to improve the match rate\n",
    "d_host['team'] = d_host['team'].str.lower()\n",
    "r_host['park_name'] = r_host['park_name'].str.lower()\n",
    "\n",
    "def fuzzy_merge(df_1, df_2, key1, key2, threshold=90):\n",
    "    \"\"\"\n",
    "    :param df_1: the left table to join\n",
    "    :param df_2: the right table to join\n",
    "    :param key1: key column of the left table\n",
    "    :param key2: key column of the right table\n",
    "    :param threshold: how close the matches should be to return a match, based on Levenshtein distance\n",
    "    :return: dataframe with both tables joined on fuzzy match\n",
    "    \"\"\"\n",
    "    s = df_2[key2].tolist()\n",
    "\n",
    "    m = df_1[key1].apply(lambda x: process.extract(x, s, limit=1))  \n",
    "    df_1['matches'] = m\n",
    "\n",
    "    m2 = df_1['matches'].apply(lambda x: x[0][0] if x[0][1] >= threshold else '')\n",
    "    df_1['matches'] = m2\n",
    "\n",
    "    merged = df_1.merge(df_2.rename(columns={key2: 'matches'}), on='matches', how='outer')\n",
    "\n",
    "    # Remove the 'matches' column and return the merged dataframe\n",
    "    merged = merged.drop(columns=['matches'])\n",
    "    return merged\n",
    "\n",
    "# Execute the function\n",
    "merged_df = fuzzy_merge(d_host, r_host, 'team', 'park_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output merged table to csv\n",
    "\n",
    "merged_df.to_csv('data/2023_all_hosts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a JSON with all the ditrict assignments\n",
    "## Include Tema nickname\n",
    "## Output a dictionary with district number as key, host team and a list of the rest of the teams\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Build the file path using os.path.join\n",
    "file_path = os.path.join('data', '2023_all_teams_nickname_colors.csv')\n",
    "\n",
    "\n",
    "# teaminfo_path = 'data\\2023_team_info.csv'\n",
    "\n",
    "## load into df\n",
    "\n",
    "teaminfo_df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### combine the team name and the nickname to make a new column\n",
    "\n",
    "teaminfo_df['team_name'] = teaminfo_df['team'] + ' ' + teaminfo_df['nickname']\n",
    "\n",
    "teaminfo_df.head()\n",
    "\n",
    "## Create a column with a search term string\n",
    "## This will be used to search for the team in the TBA API\n",
    "## Just want school name + 'high school' + 'michigan' \n",
    "# teaminfo_df['search'] = teaminfo_df['team'] + ' high school michigan'\n",
    "\n",
    "## Create a json that is grouped by district and contains a list of all the teams names in that district\n",
    "\n",
    "## Create a list of all the districts\n",
    "\n",
    "district_list = teaminfo_df['district'].unique().tolist()\n",
    "\n",
    "## Create a dictionary with the district as the key and the value is a list of all the teams in that district with the search field\n",
    "\n",
    "district_dict = {}\n",
    "\n",
    "for district in district_list:\n",
    "    district_dict[district] = teaminfo_df[teaminfo_df['district'] == district]['team_name'].tolist()\n",
    "    # district_dict[district].append(teaminfo_df[teaminfo_df['district'] == district]['search'].tolist())\n",
    "    \n",
    "\n",
    "## sort by district number\n",
    "\n",
    "district_dict = dict(sorted(district_dict.items()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to a json called district_dict.json\n",
    "\n",
    "with open('district_dict.json', 'w') as fp:\n",
    "    json.dump(district_dict, fp)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Build the file path using os.path.join\n",
    "file_path = os.path.join('data', 'html', 'mhsaa', 'data', 'tourney_2023.json')\n",
    "\n",
    "load = json.load(open(file_path))\n",
    "# Convert to df\n",
    "\n",
    "df = pd.DataFrame(load)\n",
    "df.info()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## list of column names\n",
    "\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Look at the Oxford High School data\n",
    "\n",
    "oxford = df[df['park_name'] == 'Oxford HS']\n",
    "oxford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "\n",
    "# Load HTML\n",
    "html_path = 'data/districts_2023.html'\n",
    "with open(html_path, 'r') as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Parse HTML\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Extract data\n",
    "divisions = soup.find_all('div', class_='keep-together')\n",
    "\n",
    "data = []\n",
    "for division in divisions:\n",
    "    division_number = division.find('span', {'data-bind': 'text:Division'}).text\n",
    "    tournament_name = division.find('span', {'data-bind': 'text:TournamentName'}).text\n",
    "    host = division.find('span', {'data-bind': 'text:Host'}).text\n",
    "    teams = [a.text for a in division.find_all('a')[1:]]\n",
    "\n",
    "    for team in teams:\n",
    "        data.append({\n",
    "            'team': team,\n",
    "            'division': division_number,\n",
    "            'district': int(re.sub(r'\\D+', '', tournament_name)),\n",
    "            'host': host,\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df_by_district = pd.DataFrame(data)\n",
    "\n",
    "# Load the csv that contains nickname info\n",
    "df_nickname = pd.read_csv('data/school_info/mhsaa_school_nickname_color_2020.csv')\n",
    "df_nickname.columns = df_nickname.columns.str.lower()\n",
    "\n",
    "# Fuzzy match team names\n",
    "matches = df_by_district['team'].apply(lambda x: process.extractOne(x, df_nickname['school'], scorer=fuzz.ratio))\n",
    "\n",
    "df_by_district['match_name'] = [i[0] for i in matches]\n",
    "df_by_district['score'] = [i[1] for i in matches]\n",
    "\n",
    "# Merge df_by_district and df_nickname on the common columns generated by fuzzy matching\n",
    "final_df = pd.merge(df_by_district, df_nickname, left_on='match_name', right_on='school', how='inner')\n",
    "\n",
    "# Select only the columns you're interested in\n",
    "final_df = final_df[['team', 'division', 'district', 'host', 'nickname', 'color1', 'color2', 'color3', 'color4', 'score']]\n",
    "\n",
    "# Display the final dataframe\n",
    "# print(final_df)\n",
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output dataframe to new file called 2023_team_info.csv\n",
    "\n",
    "## Team info for 2023 output file\n",
    "# drop host column\n",
    "teams_df = final_df.drop(columns=['host'])\n",
    "\n",
    "# Path: quick_workbook.ipynb\n",
    "teams_df.to_csv('data/2023_team_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with matching host and team\n",
    "matching_rows = final_df[final_df['host'] == final_df['team']]\n",
    "\n",
    "# Sort the filtered rows by district number\n",
    "sorted_rows = matching_rows.sort_values(by='district')\n",
    "\n",
    "# Reset the index of the sorted rows\n",
    "sorted_rows.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the sorted dataframe\n",
    "print(sorted_rows)\n",
    "\n",
    "# Save the sorted dataframe to a CSV file\n",
    "sorted_rows.to_csv('data/2023_district_hosts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(20)\n",
    "\n",
    "## Show the lowest scores in the dataframe\n",
    "\n",
    "final_df.sort_values(by='score').head(20)\n",
    "\n",
    "## Show the distro of scores\n",
    "\n",
    "# final_df['score'].hist()\n",
    "\n",
    "## Show numberical counts of scores in incriments of 5\n",
    "\n",
    "# final_df['score'].value_counts(bins=range(0, 101, 5))\n",
    "\n",
    "# Number of match scores under 90\n",
    "\n",
    "len(final_df[final_df['score'] < 90])\n",
    "\n",
    "# Number of match scores under 80\n",
    "\n",
    "len(final_df[final_df['score'] < 80])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Block opperation above replaces the functionality of the following blocks of beta code\n",
    "\n",
    "## Create a table with School info (Name, division, district assignment - from the district_2023 html on MHSAA site)\n",
    "\n",
    "### Then merge that into the info from the table I have with School Nickname and colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load entire district tree from local html file\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_path = 'data\\districts_2023.html'\n",
    "\n",
    "with open(html_path, 'r') as f:\n",
    "    html = f.read()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ALL IN ONE TRY ####\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "html_doc = html\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Create an empty DataFrame to store the data\n",
    "df = pd.DataFrame(columns=['Division', 'Tournament Name', 'Host', 'Location', 'Teams'])\n",
    "\n",
    "# Find all 'div' tags with class 'keep-together'\n",
    "divisions = soup.find_all('div', class_='keep-together')\n",
    "\n",
    "for division in divisions:\n",
    "    division_number = division.find('span', {'data-bind': 'text:Division'}).text\n",
    "    tournament_name = division.find('span', {'data-bind': 'text:TournamentName'}).text\n",
    "    host = division.find('span', {'data-bind': 'text:Host'}).text\n",
    "    location = division.find('a', {'data-bind': 'text: Title, attr: {href: LocationUrl}'}).text\n",
    "    \n",
    "    # Find all the team names, skipping the first 'a' tag which is the location\n",
    "    teams = [a.text for a in division.find_all('a')[1:]]\n",
    "    # # Remove the host team from the list\n",
    "    # if host in teams:\n",
    "    #     teams.remove(host)\n",
    "    \n",
    "    # Add data to the DataFrame\n",
    "    df = df.append({\n",
    "        'Division': division_number, \n",
    "        'Tournament Name': tournament_name, \n",
    "        'Host': host, \n",
    "        'Location': location, \n",
    "        'Teams': teams}, \n",
    "        ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get a DF with a row for every team\n",
    "\n",
    "df_temp = df.explode('Teams')\n",
    "\n",
    "df_temp.info()\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up the new team DF\n",
    "\n",
    "## Drop The non numberical characters from 'Tournament Name' and Rename to 'District'\n",
    "\n",
    "df_temp['District'] = df_temp['Tournament Name'].str.replace(r'\\D+', '')\n",
    "\n",
    "df_temp['District'] = df_temp['District'].astype(int)\n",
    "\n",
    "# Drop the 'Tournament Name' column\n",
    "df_temp.drop('Tournament Name', axis=1, inplace=True)\n",
    "\n",
    "## Drop Host and Location\n",
    "\n",
    "df_temp.drop(['Host', 'Location'], axis=1, inplace=True)\n",
    "\n",
    "# Rename Teams to Team\n",
    "df_temp.rename(columns={'Teams': 'Team'}, inplace=True)\n",
    "\n",
    "# Remove capitalization from column names\n",
    "df_temp.columns = df_temp.columns.str.lower()\n",
    "\n",
    "\n",
    "## Move team name to first column\n",
    "\n",
    "cols = df_temp.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_temp = df_temp[cols]\n",
    "\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rename df\n",
    "\n",
    "df_by_district = df_temp\n",
    "\n",
    "## Load the csv that contains nickname ect info\n",
    "\n",
    "df_nickname = pd.read_csv('data\\school_info\\mhsaa_school_nickname_color_2020.csv')\n",
    "\n",
    "## Remove capitalization from column names\n",
    "df_nickname.columns = df_nickname.columns.str.lower()\n",
    "\n",
    "df_nickname.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### match and merge the dataframes based on Team name and School name\n",
    "\n",
    "### Use fuzzy match to match team names\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "def match_name(name, list_names, min_score=0):\n",
    "    # -1 score incase we don't get any matches\n",
    "    max_score = -1\n",
    "    # Returning empty name for no match as well\n",
    "    max_name = \"\"\n",
    "    # Iternating over all names in the other\n",
    "    for name2 in list_names:\n",
    "        #Finding fuzzy match score\n",
    "        score = fuzz.ratio(name, name2)\n",
    "        # Checking if we are above our threshold and have a better score\n",
    "        if (score > min_score) & (score > max_score):\n",
    "            max_name = name2\n",
    "            max_score = score\n",
    "    return (max_name, max_score)\n",
    "\n",
    "# List for dicts for easy dataframe creation\n",
    "dict_list = []\n",
    "# iterating over our players without salaries found above\n",
    "for name in df_by_district.team:\n",
    "    # Use our method to find best match, we can set a threshold here\n",
    "    match = match_name(name, df_nickname.school, 75)\n",
    "    \n",
    "    # New dict for storing data\n",
    "    dict_ = {}\n",
    "    dict_.update({\"team_name\" : name})\n",
    "    dict_.update({\"match_name\" : match[0]})\n",
    "    dict_.update({\"score\" : match[1]})\n",
    "    dict_list.append(dict_)\n",
    "    \n",
    "merge_table = pd.DataFrame(dict_list)\n",
    "# Display results\n",
    "# print(merge_table)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do the table merges\n",
    "\n",
    "df_by_district = df_by_district.merge(merge_table, left_on='team', right_on='team_name', how='left')\n",
    "df_nickname = df_nickname.merge(merge_table, left_on='school', right_on='match_name', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_by_district and df_nickname on the common columns generated by fuzzy matching\n",
    "final_df = pd.merge(df_by_district, df_nickname, left_on='team', right_on='match_name', how='inner')\n",
    "\n",
    "# Select only the columns you're interested in\n",
    "final_df = final_df[['team', 'division', 'district', 'nickname', 'color1', 'color2', 'color3', 'color4', 'score']]\n",
    "\n",
    "\n",
    "# Display the final dataframe\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv to check\n",
    "\n",
    "df.to_csv('data\\district_2023_team_and_host.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End 2023 Team info creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a dictionary of all the teams seperated by division level\n",
    "\n",
    "# Create an empty dictionary to store the data\n",
    "divisions_dict = {}\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "html_doc = html\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Create an empty DataFrame to store the data\n",
    "df = pd.DataFrame(columns=['Division', 'Tournament Name', 'Host', 'Location', 'Teams'])\n",
    "\n",
    "# Find all 'div' tags with class 'keep-together'\n",
    "divisions = soup.find_all('div', class_='keep-together')\n",
    "\n",
    "for division in divisions:\n",
    "    division_number = division.find('span', {'data-bind': 'text:Division'}).text\n",
    "    tournament_name = division.find('span', {'data-bind': 'text:TournamentName'}).text\n",
    "    host = division.find('span', {'data-bind': 'text:Host'}).text\n",
    "    location = division.find('a', {'data-bind': 'text: Title, attr: {href: LocationUrl}'}).text\n",
    "    teams = [team.text for team in division.find_all('span', {'data-bind': 'highlightedText: { text: TeamName, highlight: $parents[1].Search, css: \"highlight\" }'})]\n",
    "    \n",
    "    # Add data to the DataFrame\n",
    "    df = df.append({\n",
    "        'Division': division_number, \n",
    "        'Tournament Name': tournament_name, \n",
    "        'Host': host, \n",
    "        'Location': location, \n",
    "        'Teams': teams}, \n",
    "        ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## File Paths\n",
    "# Path to mhsaa tables to merge\n",
    "\n",
    "enrollment_path = 'data\\school_info\\mhsaa_enrolment_2022.csv'\n",
    "name_color_path = 'data\\school_info\\mhsaa_school_nickname_color_2020.csv'\n",
    "\n",
    "df_enrol = pd.read_csv(enrollment_path)\n",
    "df_name_color = pd.read_csv(name_color_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5-9-23\n",
    "\n",
    "## Code to scrape 2023 MHSAA Tourny Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace this with the plain text containing the tournament information\n",
    "# read a text file into the variable text\n",
    "\n",
    "text = open('2023_districts_raw.txt', 'r').read()\n",
    "\n",
    "# Split the text into sections for each division\n",
    "sections = text.split('Division ')\n",
    "\n",
    "# Remove the first empty string\n",
    "sections.pop(0)\n",
    "\n",
    "# Initialize empty lists for each column in the dataframe\n",
    "divisions = []\n",
    "districts = []\n",
    "hosts = []\n",
    "locations = []\n",
    "\n",
    "# Loop through the sections and extract the relevant information\n",
    "for section in sections:\n",
    "    lines = section.split('\\n')\n",
    "    division = 'Division ' + lines[0]\n",
    "    for line in lines[1:]:\n",
    "        if 'Baseball District' in line:\n",
    "            district = line\n",
    "        elif 'Host:' in line:\n",
    "            host = line.split(': ')[1]\n",
    "        elif 'Location:' in line:\n",
    "            location = line.split(': ')[1]\n",
    "        elif line != '':\n",
    "            # Skip any blank lines\n",
    "            districts.append(district)\n",
    "            divisions.append(division)\n",
    "            hosts.append(host)\n",
    "            locations.append(location)\n",
    "\n",
    "# Create a dataframe to store the extracted information\n",
    "df = pd.DataFrame({'Division': divisions, 'District': districts, 'Host': hosts, 'Location': locations})\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DF came back as a ton of duplicates. I need to clean it up.\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Reindex the dataframe\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# output the dataframe to a csv file\n",
    "df.to_csv('data/2023_district_hosts.csv', index=False)\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regional Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### URLS of pages with retional data\n",
    "\n",
    "urls = {'Division 1': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/9/Classification/1/SportSeasonId/424201',\n",
    "        'Division 2': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/9/Classification/2/SportSeasonId/424201',\n",
    "        'Division 3': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/9/Classification/3/SportSeasonId/424201',\n",
    "        'Division 4': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/9/Classification/4/SportSeasonId/424201'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URLs for each division\n",
    "urls = {'Division 1': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/9/Classification/1/SportSeasonId/424201',\n",
    "        'Division 2': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/9/Classification/2/SportSeasonId/424201',\n",
    "        'Division 3': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/9/Classification/3/SportSeasonId/424201',\n",
    "        'Division 4': 'https://my.mhsaa.com/Sports/MHSAA-Tournament-Brackets/BracketGroup/9/Classification/4/SportSeasonId/424201'}\n",
    "\n",
    "# Initialize empty lists for each column in the dataframe\n",
    "divisions = []\n",
    "locations = []\n",
    "links = []\n",
    "\n",
    "# Loop through each division URL in the dictionary\n",
    "for division, url in urls.items():\n",
    "    try:\n",
    "        # Send a GET request to the URL and parse the HTML content\n",
    "        page = requests.get(url)\n",
    "        tree = html.fromstring(page.content)\n",
    "\n",
    "        # Find all the contest location spans using XPath\n",
    "        location_spans = tree.xpath('//span[@class=\"contestlocation\"]')\n",
    "\n",
    "        # Loop through the location spans and extract the relevant information\n",
    "        for location_span in location_spans:\n",
    "            # Extract the location and link from the contest location span\n",
    "            location = location_span.xpath('text()')[0].strip()\n",
    "            link = location_span.xpath('a/@href')[0]\n",
    "            # Append the information to the respective lists\n",
    "            divisions.append(division)\n",
    "            locations.append(location)\n",
    "            links.append(link)\n",
    "    except:\n",
    "        print(f'Error: Failed to retrieve data for {division}')\n",
    "\n",
    "# Create a dataframe to store the extracted information\n",
    "df = pd.DataFrame({'Division': divisions, 'Location': locations, 'Link': links})\n",
    "\n",
    "# Reset the index of the dataframe\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up the dataframe\n",
    "\n",
    "## Drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df.head(30)\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "\n",
    "### Output as a csv\n",
    "## Might want to go back and adjust code to try to store which specific games are at each location\n",
    "## regional has (semis and finals) then there is a quarterfinals round\n",
    "\n",
    "df.to_csv('data/2023_regional_hosts.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Project\n",
    "\n",
    "### Create a json with just the fields in michigan and try to integrate a column that marks the appropriate fields as host of districts and regionals\n",
    "\n",
    "The text of the locations in the playoff csvs is not going to match the field names all that well. it might be worth trying to identify them from the map location - will have to go back to districts and extract map locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try to get all the google maps link from the districts page\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## Read local file\n",
    "path = 'districts_2023.html'\n",
    "html = open(path, 'r').read()\n",
    "\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Find all the tournament divs\n",
    "tournaments = soup.select('div.keep-together')\n",
    "\n",
    "# Initialize lists to store data\n",
    "district_numbers = []\n",
    "hosts = []\n",
    "locations = []\n",
    "\n",
    "# Extract data for each tournament\n",
    "for tournament in tournaments:\n",
    "    district_number = tournament.find('span', {'data-bind': 'text:Division'}).text\n",
    "    host = tournament.find('span', {'data-bind': 'text:Host'}).text\n",
    "    location = tournament.find('a', {'target': '_blank'}).get('href')\n",
    "    \n",
    "\n",
    "    district_numbers.append(district_number)\n",
    "    hosts.append(host)\n",
    "    locations.append(location)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {'Division': district_numbers, 'Host': hosts, 'Location': locations}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# add another column the is the index number of the row + 1\n",
    "df['District'] = df.index + 1\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "df.to_csv('district_tournaments.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to match up locations from the scraped district and regional and link them to a field in my json data\n",
    "\n",
    "## Stragegy: The district csv contains a field that has a link to a google maps search. Loop through all of those and return the lat and longitude coordinates then match the coordinates to the nearest home plate coordinate in the json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import googlemaps\n",
    "\n",
    "## paths\n",
    "\n",
    "local_json = 'data\\michigan_fields.json'\n",
    "\n",
    "district_csv = 'district_tournaments.csv'\n",
    "\n",
    "regional_csv = 'data\\2023_regional_hosts.csv'\n",
    "\n",
    "# Replace this with your own API key\n",
    "api_key = \"AIzaSyA_BhlTupRdBPBhRptQuR6pYorMVYQnRMA\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Data\n",
    "df = pd.read_csv(district_csv)\n",
    "\n",
    "# df.head()\n",
    "\n",
    "df.info()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean up the location column to remove first portion and just leave the address remaining\n",
    "\n",
    "\n",
    "\n",
    "# Remove the unwanted portion of the string in the 'Location' column\n",
    "prefix = \"http://maps.google.com/maps?q=\"\n",
    "\n",
    "# Check if the location is a string before applying lstrip\n",
    "df['Location'] = df['Location'].apply(lambda x: x.lstrip(prefix) if isinstance(x, str) else x)\n",
    "\n",
    "# Print the cleaned DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USES GOOGLE CODE\n",
    "import pandas as pd\n",
    "import googlemaps\n",
    "\n",
    "# Replace 'your_api_key' with your actual Google Maps API key\n",
    "api_key = 'AIzaSyA_BhlTupRdBPBhRptQuR6pYorMVYQnRMA'\n",
    "gmaps = googlemaps.Client(key=api_key)\n",
    "\n",
    "# # Create a DataFrame from your data (use your actual DataFrame here)\n",
    "# data = {\n",
    "#     \"Division\": [1, 1, 1],\n",
    "#     \"Host\": [\"Marquette\", \"Midland Dow\", \"Muskegon Mona Shores\"],\n",
    "#     \"Location\": [\n",
    "#         \"North Marquette Fields, Marquette, MI\",\n",
    "#         \"H H Dow High School - Baseball, 3901 N. Saginaw Rd. Midland, MI\",\n",
    "#         \"Mona Shores Baseball Field, 1121 W. Seminole Rd. Muskegon, MI\",\n",
    "#     ],\n",
    "#     \"District\": [1, 2, 3],\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# Function to get the coordinates for a given address\n",
    "def get_coordinates(address):\n",
    "    geocode_result = gmaps.geocode(address)\n",
    "    if geocode_result:\n",
    "        lat = geocode_result[0][\"geometry\"][\"location\"][\"lat\"]\n",
    "        lng = geocode_result[0][\"geometry\"][\"location\"][\"lng\"]\n",
    "        return (lat, lng)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to the 'Location' column and store the coordinates in a new column\n",
    "df[\"Coordinates\"] = df[\"Location\"].apply(get_coordinates)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check Output\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google code worked OK - returned coords for 126 of 128\n",
    "\n",
    "### Below I am going to try to match up those coordinates to the michigan fields jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### set up paths and load data(copied from above)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "# import googlemaps\n",
    "\n",
    "## paths\n",
    "\n",
    "local_json = 'data\\michigan_fields.json'\n",
    "\n",
    "district_csv = 'district_tournaments.csv'\n",
    "\n",
    "regional_csv = 'data\\2023_regional_hosts.csv'\n",
    "\n",
    "# Replace this with your own API key\n",
    "api_key = \"AIzaSyA_BhlTupRdBPBhRptQuR6pYorMVYQnRMA\"\n",
    "\n",
    "# load Data\n",
    "df = pd.read_csv(district_csv)\n",
    "\n",
    "## Load MI fields data from json file\n",
    "\n",
    "\n",
    "# Read the JSON file\n",
    "with open(local_json) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a DataFrame from the JSON data\n",
    "mi_df = pd.DataFrame(data)\n",
    "\n",
    "mi_df.head()\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try a different approach to match the fields\n",
    "## Use the Host name to find 3 matches from the mi_df\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# Assuming you have the two dataframes df and mi_df\n",
    "\n",
    "def find_closest_park_names(host, n_closest=3):\n",
    "    closest_park_names = process.extract(host, mi_df[\"park_name\"], limit=n_closest, scorer=fuzz.token_sort_ratio)\n",
    "    return [name for name, score, index in closest_park_names]\n",
    "\n",
    "# Apply the function to the 'Host' column and store the results in new columns\n",
    "df[[\"closest_park_1\", \"closest_park_2\", \"closest_park_3\"]] = df[\"Host\"].apply(find_closest_park_names).apply(pd.Series)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTION TO FIND NEAREST FIELD TO DISTRICT TOURNAMENT LOCATION\n",
    "\n",
    "import math\n",
    "\n",
    "## Define a function to calculate the Haversine distance between two points\n",
    "def haversine_distance(coord1, coord2):\n",
    "    # Convert latitude and longitude to radians\n",
    "    lat1, lon1 = map(math.radians, coord1)\n",
    "    lat2, lon2 = map(math.radians, coord2)\n",
    "\n",
    "    # Calculate the differences between latitudes and longitudes\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # Calculate the Haversine distance\n",
    "    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    r = 6371  # Radius of the Earth in km\n",
    "\n",
    "    return c * r\n",
    "\n",
    "def find_closest_parks(coord, n_closest=3):\n",
    "    if coord is None:\n",
    "        return [\"Unknown\"] * n_closest\n",
    "\n",
    "    mi_df[\"distance\"] = mi_df[\"home_plate\"].apply(lambda x: haversine_distance(coord, (x[1], x[0])))\n",
    "    closest_park_indices = mi_df[\"distance\"].nsmallest(n_closest).index\n",
    "    return mi_df.loc[closest_park_indices, \"park_name\"].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Make sure 'home_plate' in mi_df has coordinates in the format (lat, lng)\n",
    "mi_df[\"home_plate\"] = mi_df[\"home_plate\"].apply(lambda x: (x[0], x[1]))\n",
    "\n",
    "# Create a new column 'closest_park' in df\n",
    "df[[\"closest_park_1\", \"closest_park_2\", \"closest_park_3\"]] = df[\"Coordinates\"].apply(find_closest_parks).apply(pd.Series)\n",
    "\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output this matching as a csv so I can manulauly check it\n",
    "\n",
    "df.to_csv('district_fields_text_match.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of 5-9-23 Work for now. output csv file with possible matches for the district fields\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with outlier fields \n",
    "\n",
    "## Start 59/23 Night\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependencies and Setup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "\n",
    "# Load files\n",
    "out_df = pd.read_csv('outlier_fields.csv')\n",
    "\n",
    "out_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_out = df_out.copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you already have the out_df DataFrame\n",
    "# Creating an empty HP2 column\n",
    "out_df['HP2'] = None\n",
    "\n",
    "# Loop through the DataFrame rows and populate the HP2 column with repeated points\n",
    "for idx, row in out_df.iterrows():\n",
    "    fop_list = row['fop']\n",
    "    repeated_points = [point for point in set(fop_list) if fop_list.count(point) > 1]\n",
    "    if len(repeated_points) > 0:\n",
    "        out_df.loc[idx, 'HP2'] = str(repeated_points[0])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "out_df.head(20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "url = 'https://my.mhsaa.com/Sports/Baseball/Districts'  # Replace this with the URL of the webpage you want to scrape\n",
    "page = requests.get(url)\n",
    "tree = html.fromstring(page.content)\n",
    "\n",
    "# Find the game location using the XPath\n",
    "location = tree.xpath('/html/body/form/div[5]/div[2]/div/div/div[2]/div[1]/div/div/div/div/div/div/div/div[2]/div[2]/div[1]/div[2]/div[1]/a')[0]\n",
    "\n",
    "# Extract the relevant information\n",
    "name = location.text.strip()\n",
    "link = location.get('href')\n",
    "address = link.split('=')[1].strip()\n",
    "\n",
    "# Create a dataframe to store the scraped data\n",
    "df = pd.DataFrame({'Field_name': [name], 'Location': [address], 'Link': [link]})\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Get latitude and longitude for each location\n",
    "df[\"Coordinates\"] = df[\"Location\"].apply(get_latitude_longitude)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_name_color.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Match the school name to School and merge the dataframes into a single object\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "def find_best_match(school_name, choices, score_cutoff=70):\n",
    "    best_match = process.extractOne(school_name, choices, scorer=fuzz.token_sort_ratio, score_cutoff=score_cutoff)\n",
    "    if best_match:\n",
    "        return best_match[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Get the list of school names from df_name_color\n",
    "school_names = df_name_color['School'].tolist()\n",
    "\n",
    "# Apply find_best_match function to create a new column 'best_match' in df_enrol\n",
    "df_enrol['best_match'] = df_enrol['school_name'].apply(find_best_match, choices=school_names, score_cutoff=80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'School' column in df_name_color to 'best_match'\n",
    "df_name_color = df_name_color.rename(columns={'School': 'best_match'})\n",
    "\n",
    "# Merge the dataframes on the 'best_match' column\n",
    "df_merged = df_enrol.merge(df_name_color, on='best_match', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_enrol.columns)\n",
    "\n",
    "print(df_name_color.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - get icons sorted out\n",
    "# -*** DONE*** get the level assigner sorted out in the etl\n",
    "# - add filter based on level to map\n",
    "# - implement the search box places from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
